---
title: Google Cloud Platform - GCP 덤프 정리
sidebarDepth: 1
---

## dump 1

Every employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do?

A. Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance.
B. Ask each member of the team to generate a new SSH key pair and to send you their public key. Use a configuration management tool to deploy those keys on each instance.
C. Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the ג€compute.osAdminLoginג€ role to the Google group corresponding to this team.
D. Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance.

:::details 풀이

문제 해석) 회사의 모든 직원이 Google 계정을 가지고 있습니다. 운영팀은 Compute Engine에서 많은 수의 인스턴스를 관리해야 합니다. 이 팀의 각 구성원은 서버에 대한 관리자 액세스 권한만 필요합니다. 보안팀은 자격 증명 배포가 운영상 효율적이어야 하며 누가 특정 인스턴스에 액세스했는지 확인할 수 있어야 한다고 요구합니다. 어떻게 해야 할까요?

서버에 대한 관리자 액세스 권한이라 함은, Compute Engine 서버 내에서 시스템 관리 작업에 대한 권한을 의미한다. 리소스 관리 권한이 아닌 것을 기억해야 한다.

오답 정리

-   A(오답): 새로운 SSH 키 쌍을 생성하고, 개인 키를 팀의 각 구성원에게 제공하며, 각 인스턴스의 메타데이터에 공개 키를 구성
    -   동일한 개인 키를 공유하면 누가 접근했는지 추적 불가
-   B(오답): 각 팀 구성원이 새로운 SSH 키 쌍을 생성하고 공개 키를 보내도록 하여, 구성 관리 도구로 각 인스턴스에 배포
    -   수동 배포 과정 비효율 / 확장성 저하
-   **C(정답)**: 각 팀 구성원이 새로운 SSH 키 쌍을 생성하고 Google 계정에 공개 키를 추가하도록 하고, 해당 팀에 대응하는 Google 그룹에 'compute.osAdminLogin' 역할을 부여
    -   OS Login을 통해 Google 계정과 SSH 키가 연동됨
    -   각 사용자를 개별적으로 추적 가능
    -   그룹 기반 권한 관리로 효율적
    -   Google의 감사 로그를 통해 접근 기록 추적 가능
-   D(오답): 새로운 SSH 키 쌍을 생성하고 개인 키를 각 구성원에게 제공하며, 프로젝트 전체 공개 SSH 키로 구성
    -   A와 동일하게 개별 사용자 추적 불가능

:::

## dump 2

```
You need to create a custom VPC with a single subnet. The subnet's range must be as large as possible. Which range should you use?

A. 0.0.0.0/0
B. 10.0.0.0/8
C. 172.16.0.0/12
D. 192.168.0.0/16
```

:::details 풀이

사용자 정의 VPC를 단일 서브넷으로 생성해야 합니다. 서브넷의 범위는 가능한 한 커야 합니다. 어떤 범위를 사용해야 할까요?

CIDR이란 IP 주소를 효율적으로 할당하고 라우팅하기 위한 방법이다. (`IP주소/프리픽스길이`)

A. 0.0.0.0/0

의미: 전체 IPv4 주소 공간 (모든 IP 주소)
범위: 0.0.0.0 ~ 255.255.255.255
주소 개수: 약 43억 개

B. 10.0.0.0/8 ⭐

의미: RFC 1918 사설 IP 대역 중 Class A
범위: 10.0.0.0 ~ 10.255.255.255
주소 개수: 약 1,677만 개

C. 172.16.0.0/12

의미: RFC 1918 사설 IP 대역 중 Class B
범위: 172.16.0.0 ~ 172.31.255.255
주소 개수: 약 104만 개

D. 192.168.0.0/16

의미: RFC 1918 사설 IP 대역 중 Class C
범위: 192.168.0.0 ~ 192.168.255.255
주소 개수: 약 6만 5천 개

CIDR 표기에서 192.168.1.100/24의 앞부분은 IP주소, 뒷부분은 서브넷마스크 길이이다. 192, 168과 같은 각 부분은 옥텟이라고 불린다. 각 옥텟마다 0~255까지 할당 가능한데, 이는 비트 기준 00000000 ~ 11111111로 값이 변할 수 있다는 것을 가리킨다.

서브넷 마스크 길이는 1의 갯수를 의미하고, /24면 비트 1이 24개가 고정이라는 것을 의미한다.

즉 192.168.1이 고정 주소이고, 나머지 100 주소값이 변할 수 있음을 의미한다.

:::

## dump 3

You want to select and configure a cost-effective solution for relational data on Google Cloud Platform. You are working with a small set of operational data in one geographic location. You need to support point-in-time recovery. What should you do?

-   A. Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected.
-   B. Select Cloud SQL (MySQL). Select the create failover replicas option.
-   C. Select Cloud Spanner. Set up your instance with 2 nodes.
-   D. Select Cloud Spanner. Set up your instance as multi-regional.

:::details 풀이

#### 영문 문제 해석

**문제**: "Google Cloud Platform에서 관계형 데이터를 위한 비용 효율적인 솔루션을 선택하고 구성하려고 합니다. 한 지리적 위치에서 소규모 운영 데이터로 작업하고 있습니다. 특정 시점 복구(point-in-time recovery)를 지원해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   관계형 데이터베이스
-   **비용 효율적인** 솔루션
-   **소규모** 운영 데이터
-   **단일 지리적 위치**
-   **Point-in-time Recovery (PITR)** 지원 필수

#### 선택지 해석

**A. Cloud SQL (MySQL)을 선택하고, enable binary logging 옵션이 선택되었는지 확인**

-   Binary logging: MySQL의 변경 사항을 로그로 기록
-   PITR을 위해 필수적인 기능

**B. Cloud SQL (MySQL)을 선택하고, create failover replicas 옵션을 선택**

-   Failover replicas: 고가용성을 위한 복제본 생성
-   주로 장애 대응용, PITR과는 다른 개념

**C. Cloud Spanner를 선택하고, 2개 노드로 인스턴스 설정**

-   Cloud Spanner: 글로벌 분산 데이터베이스
-   최소 2개 노드 필요 (비용이 높음)

**D. Cloud Spanner를 선택하고, 인스턴스를 멀티 리전으로 설정**

-   Multi-regional: 여러 지역에 분산
-   가장 비용이 높은 옵션

#### 문제 분석 및 정답 도출

#### 서비스 비교

**Cloud SQL vs Cloud Spanner**

-   **Cloud SQL**:
-   소규모~중간 규모 애플리케이션
-   비용 효율적
-   단일 리전에 적합
-   PITR 지원 (binary logging 필요)

-   **Cloud Spanner**:
-   대규모, 글로벌 애플리케이션
-   비용이 높음 (최소 2노드부터)
-   자동 PITR 지원하지만 과도한 스펙

#### Point-in-Time Recovery

**Cloud SQL에서 PITR 활성화 방법**:

1. **Binary logging 활성화** ← 핵심!
2. Automated backup 설정
3. 이 둘이 결합되어 PITR 기능 제공

**Failover replicas**는 고가용성을 위한 것으로 PITR과는 별개입니다.

#### 비용 고려사항

-   **소규모 데이터 + 단일 위치** → Cloud SQL이 적합
-   Cloud Spanner는 요구사항 대비 과도하게 비싼 솔루션

#### 정답: **A. Cloud SQL (MySQL)을 선택하고, enable binary logging 옵션이 선택되었는지 확인**

**선택 이유**:

1. **비용 효율성**: 소규모 데이터에는 Cloud SQL이 적합
2. **PITR 지원**: Binary logging이 PITR의 핵심 요구사항
3. **단일 위치**: 멀티 리전 불필요
4. **요구사항 충족**: 모든 조건을 가장 경제적으로 만족

:::

## dump 4

You want to configure autohealing for network load balancing for a group of Compute Engine instances that run in multiple zones, using the fewest possible steps. You need to configure re-creation of VMs if they are unresponsive after 3 attempts of 10 seconds each. What should you do?

-   A. Create an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy (HTTP)
-   B. Create an HTTP load balancer with a backend configuration that references an existing instance group. Define a balancing mode and set the maximum RPS to 10.
-   C. Create a managed instance group. Set the Autohealing health check to healthy (HTTP)
-   D. Create a managed instance group. Verify that the autoscaling setting is on.

:::details 풀이

#### 영문 문제 해석

**문제**: "여러 영역에서 실행되는 Compute Engine 인스턴스 그룹에 대해 네트워크 로드 밸런싱을 위한 자동 복구(autohealing)를 가능한 한 최소한의 단계로 구성하려고 합니다. 각각 10초씩 3회 시도 후 응답하지 않으면 VM을 다시 생성하도록 구성해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   **Autohealing** (자동 복구) 구성
-   **Network load balancing**
-   **Multiple zones** (여러 영역)
-   **최소 단계**로 구성
-   **3회 시도 × 10초 = 30초** 후 VM 재생성
-   응답하지 않는 VM 처리

#### 선택지 해석

**A. 기존 인스턴스 그룹을 참조하는 백엔드 구성으로 HTTP 로드 밸런서 생성. 헬스 체크를 healthy (HTTP)로 설정**

-   HTTP 로드 밸런서: L7 로드 밸런서 (문제에서 요구하는 것은 네트워크 로드 밸런싱)
    -   L7 - OSI 7계층을 의미, HTTP 로드밸런서는 7계층인데, 문제에서 요구하는건 네트워크 계층 (4계층) 로드밸런서
-   기존 인스턴스 그룹 사용 (autohealing 기능 없음)
    -   문제에서의 existing instance group이 unmanaged instance group을 의미한다.

**B. 기존 인스턴스 그룹을 참조하는 백엔드 구성으로 HTTP 로드 밸런서 생성. 밸런싱 모드 정의하고 최대 RPS를 10으로 설정**

-   HTTP 로드 밸런서 (네트워크 로드 밸런싱 아님)
-   RPS 설정은 autohealing과 무관

**C. 관리형 인스턴스 그룹(Managed Instance Group) 생성. Autohealing 헬스 체크를 healthy (HTTP)로 설정**

-   MIG: autohealing 기능 내장
-   헬스 체크로 자동 복구 가능

**D. 관리형 인스턴스 그룹 생성. 오토스케일링 설정이 켜져 있는지 확인**

-   MIG 생성은 맞지만
-   Autoscaling ≠ Autohealing (다른 기능)

#### 문제 분석 및 정답 도출

#### Autohealing vs Load Balancing 구분

**Autohealing (자동 복구)**:

-   비정상 인스턴스를 **자동으로 재생성**
-   Managed Instance Group의 기능
-   Health check로 인스턴스 상태 모니터링

**Load Balancing**:

-   트래픽을 여러 인스턴스에 **분산**
-   비정상 인스턴스로 트래픽 전송 중단
-   인스턴스를 재생성하지는 않음

#### Network Load Balancing

문제에서 "network load balancing"을 언급했지만, 핵심 요구사항은 **autohealing**입니다.

-   Network LB 자체는 autohealing 기능 없음
-   MIG + Health Check = autohealing 구현

##### Managed Instance Group (MIG)의 장점

1. **Autohealing 내장**: 헬스 체크 기반 자동 복구
2. **Multi-zone 지원**: 여러 영역에 인스턴스 분산
3. **최소 단계**: 한 번의 설정으로 모든 기능 활성화
4. **Health Check 커스터마이징**: 3회 × 10초 설정 가능

#### 헬스 체크 설정

MIG에서 autohealing 설정 시:

Check interval: 10초
Timeout: 10초
Unhealthy threshold: 3회
총 대기 시간: 30초 후 VM 재생성

#### 정답: **C. 관리형 인스턴스 그룹 생성. Autohealing 헬스 체크를 healthy (HTTP)로 설정**

**선택 이유**:

1. **Autohealing 직접 제공**: MIG는 autohealing 기능을 내장
2. **최소 단계**: 하나의 구성으로 모든 요구사항 충족
3. **Multi-zone 지원**: 자동으로 여러 영역에 분산
4. **헬스 체크 커스터마이징**: 3회 × 10초 설정 가능
5. **요구사항 완벽 일치**: VM 재생성 기능 제공

오토힐링 헬스체크 타입을 HTTP 지정한다는 것은 HTTP 요청을 보낸 뒤 응답 코드를 보고 상태를 판단한다는 것을 의미한다.

-   TCP: 포트 연결만 확인
-   HTTPS: SSL/TLS 포함한 HTTP 확인
-   HTTP/2: HTTP/2 프로토콜로 확인

:::

## dump 5

You are using multiple configurations for gcloud. You want to review the configured Kubernetes Engine cluster of an inactive configuration using the fewest possible steps. What should you do?

-   A. Use gcloud config configurations describe to review the output.
-   B. Use gcloud config configurations activate and gcloud config list to review the output.
-   C. Use kubectl config get-contexts to review the output.
-   D. Use kubectl config use-context and kubectl config view to review the output.

:::details 풀이

### 영문 문제 해석

**문제**: "gcloud에 대해 여러 구성을 사용하고 있습니다. 가장 적은 단계로 비활성 구성의 구성된 Kubernetes Engine 클러스터를 검토하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Multiple gcloud configurations (여러 gcloud 구성)
-   Inactive configuration (비활성 구성)
-   Kubernetes Engine cluster 검토
-   Fewest possible steps (최소 단계)

### 선택지 해석

**A. gcloud config configurations describe를 사용하여 출력 검토**

-   특정 구성의 모든 속성을 표시
-   구성 활성화 없이 바로 확인 가능

**B. gcloud config configurations activate와 gcloud config list를 사용하여 출력 검토**

-   구성 활성화 후 현재 활성 구성 확인
-   2단계 필요

**C. kubectl config get-contexts를 사용하여 출력 검토**

-   kubectl의 컨텍스트 목록 확인
-   gcloud 구성과는 별개의 kubectl 구성

**D. kubectl config use-context와 kubectl config view를 사용하여 출력 검토**

-   kubectl 컨텍스트 변경 후 클러스터 구성 정보 확인
-   2단계 필요

### 문제 분석 및 정답 도출

#### gcloud configuration vs kubectl context

**gcloud configuration**

-   Google Cloud CLI의 구성 프로필
-   프로젝트, 리전, 계정, 클러스터 이름 등 설정

**kubectl context**

-   Kubernetes CLI의 클러스터 접속 정보
-   클러스터 API 서버, 인증 정보, 네임스페이스 등

#### "configured Kubernetes Engine cluster를 review"의 의미

**실제 Kubernetes 클러스터 구성 확인**

-   클러스터 API 서버 주소
-   인증 정보 및 방식
-   네임스페이스 설정
-   사용자 권한 정보

#### 각 선택지 분석

**A. gcloud config configurations describe**

-   출력: 프로젝트, 리전, 클러스터 이름
-   한계: 실제 Kubernetes 구성 정보 없음

**B. gcloud config configurations activate + list**

-   출력: gcloud 레벨 정보
-   한계: Kubernetes 클러스터 구성 세부사항 없음

**C. kubectl config get-contexts**

-   출력: 컨텍스트 목록
-   한계: 실제 클러스터 구성 정보 없음

**D. kubectl config use-context + view**

-   출력: 클러스터 API 서버, 인증 정보, 사용자 설정 등
-   장점: 실제 Kubernetes 클러스터 구성 정보 제공

##### 정답: D. kubectl config use-context와 kubectl config view를 사용하여 출력 검토

**선택 이유**:

1. 실제 Kubernetes 클러스터 구성 정보 확인 가능
2. API 서버 주소, 인증 방식, 사용자 권한 등 상세 정보 제공
3. kubectl config view가 클러스터 구성 검토에 적합한 명령어
4. gcloud configuration은 메타 정보만 제공하여 부족

:::

## dump 6

Your company uses Cloud Storage to store application backup files for disaster recovery purposes. You want to follow Google's recommended practices. Which storage option should you use?

A. Multi-Regional Storage
B. Regional Storage
C. Nearline Storage
D. Coldline Storage

:::details 풀이

### 영문 문제 해석

**문제**: "회사에서 재해 복구 목적으로 애플리케이션 백업 파일을 저장하기 위해 Cloud Storage를 사용합니다. Google의 권장 사례를 따르고자 합니다. 어떤 스토리지 옵션을 사용해야 할까요?"

**핵심 요구사항**:

-   Application backup files (애플리케이션 백업 파일)
-   Disaster recovery purposes (재해 복구 목적)
-   Google's recommended practices (Google 권장 사례)

### 선택지 해석

**A. Multi-Regional Storage**

-   여러 지역에 데이터 복제
-   높은 가용성과 내구성
-   가장 높은 비용

**B. Regional Storage**

-   단일 리전 내 여러 존에 데이터 복제
-   높은 성능, 중간 비용
-   지역적 재해에 취약

**C. Nearline Storage**

-   월 1회 미만 접근하는 데이터용
-   중간 비용, 검색 비용 있음
-   30일 최소 보관 기간

**D. Coldline Storage**

-   분기 1회 미만 접근하는 데이터용
-   낮은 비용, 높은 검색 비용
-   90일 최소 보관 기간

### 문제 분석 및 정답 도출

#### 재해 복구용 백업의 실제 특성

**접근 패턴**

-   평상시: 거의 접근하지 않음 (분기 1회 미만)
-   재해 발생시에만 접근
-   장기간 보관이 주 목적

**백업 데이터의 생명주기**

-   생성 후 장기간 보관
-   법적 요구사항으로 수년간 보존
-   실제 사용 빈도는 매우 낮음

**비용 최적화**

-   저장 비용이 가장 중요한 요소
-   재해시 검색 비용은 일회성으로 허용 가능

#### Google의 백업 권장 사례

**Disaster Recovery 백업의 특징**

-   매우 낮은 접근 빈도 (분기 1회 미만)
-   장기 보관 (90일 이상)
-   비용 효율성 최우선
-   재해시에만 빠른 검색 필요

**Coldline Storage의 장점**

-   백업 데이터의 접근 패턴과 정확히 일치
-   가장 낮은 저장 비용
-   90일 최소 보관 기간이 백업 정책과 맞음
-   재해 복구시 허용 가능한 검색 시간

#### 다른 옵션들이 부적합한 이유

**Multi-Regional/Regional Storage**

-   백업 데이터에는 과도한 비용
-   자주 접근하는 데이터용

**Nearline Storage**

-   월 1회 접근을 가정하지만 백업은 그보다 훨씬 적음
-   Coldline보다 불필요하게 높은 비용

**Archive Storage**도 있는데, 이는 가격 측면에서는 이점이 있지만 재해복구시 데이터 접근 속도가 너무 느려서 부적절하다.

#### 정답: D. Coldline Storage

**선택 이유**:

1. 재해 복구 백업의 매우 낮은 접근 빈도와 일치 (분기 1회 미만)
2. 장기 보관에 최적화된 가장 낮은 저장 비용
3. 90일 최소 보관 기간이 백업 정책에 적합
4. Google이 재해 복구 백업에 권장하는 스토리지 클래스
5. 재해시에만 발생하는 검색 비용은 비용 대비 허용 가능

:::

## dump 7

Several employees at your company have been creating projects with Cloud Platform and paying for it with their personal credit cards, which the company reimburses. The company wants to centralize all these projects under a single, new billing account. What should you do?

A. Contact cloud-billing@google.com with your bank account details and request a corporate billing account for your company.
B. Create a ticket with Google Support and wait for their call to share your credit card details over the phone.
C. In the Google Platform Console, go to the Resource Manage and move all projects to the root Organizarion.
D. In the Google Cloud Platform Console, create a new billing account and set up a payment method.

:::details 풀이

### 영문 문제 해석

**문제**: "회사의 여러 직원들이 Cloud Platform으로 프로젝트를 만들고 개인 신용카드로 결제하여 회사에서 보상받고 있습니다. 회사는 모든 프로젝트를 단일한 새로운 청구 계정으로 중앙화하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Multiple projects with personal credit cards (개인 신용카드로 결제하는 여러 프로젝트)
-   Company reimbursement (회사 보상)
-   Centralize under single billing account (단일 청구 계정으로 중앙화)
-   New billing account needed (새로운 청구 계정 필요)

### 선택지 해석

**A. cloud-billing@google.com에 은행 계좌 세부정보를 제공하여 회사용 기업 청구 계정을 요청**

-   이메일로 은행 정보 전달
-   기업 청구 계정 요청 방식

**B. Google Support에 티켓을 생성하고 전화로 신용카드 세부정보를 공유하기 위해 연락을 기다림**

-   Support 티켓 생성
-   전화를 통한 신용카드 정보 공유

**C. Google Platform Console에서 Resource Manager로 이동하여 모든 프로젝트를 루트 Organization으로 이동**

-   조직 구조 변경
-   프로젝트 이동 (청구 계정 변경 아님)

**D. Google Cloud Platform Console에서 새로운 청구 계정을 생성하고 결제 방법을 설정**

-   콘솔에서 직접 청구 계정 생성
-   결제 방법 직접 설정

### 문제 분석 및 정답 도출

#### 청구 계정 중앙화 과정

**필요한 단계**

1. 새로운 기업 청구 계정 생성
2. 결제 방법 설정 (기업 카드/계좌)
3. 기존 프로젝트들을 새 청구 계정으로 이동

#### 각 선택지 분석

**A. 이메일로 기업 청구 계정 요청**

-   cloud-billing@google.com은 실제 Google 이메일이 아님
-   은행 정보를 이메일로 전달하는 것은 보안상 부적절
-   공식적인 프로세스가 아님

**B. Support 티켓으로 신용카드 정보 공유**

-   Google은 전화로 결제 정보를 요구하지 않음
-   피싱 시도와 유사한 위험한 방법
-   공식적인 청구 계정 생성 방법이 아님

**C. Resource Manager로 프로젝트 이동**

-   Organization 구조 변경은 청구와 별개
-   프로젝트를 Organization으로 이동해도 청구 계정은 변경되지 않음
-   청구 중앙화와 직접적 관련 없음

**D. Console에서 새 청구 계정 생성**

-   Google Cloud Console의 표준 프로세스
-   직접적이고 안전한 방법
-   기업 결제 방법 설정 가능
-   생성 후 프로젝트들을 새 청구 계정으로 연결 가능

#### Google Cloud 청구 계정 생성 프로세스

**표준 절차**

1. Google Cloud Console → Billing 섹션
2. Create billing account 선택
3. 계정 정보 및 결제 방법 설정
4. 기존 프로젝트들을 새 청구 계정으로 이동

**보안 및 공식성**

-   모든 과정이 Google Cloud Console 내에서 진행
-   안전한 결제 정보 입력
-   즉시 적용 가능

#### 정답: D. Google Cloud Platform Console에서 새로운 청구 계정을 생성하고 결제 방법을 설정

**선택 이유**:

1. Google Cloud의 표준적이고 공식적인 프로세스
2. 안전하고 직접적인 청구 계정 생성 방법
3. 기업 결제 방법을 즉시 설정 가능
4. 생성 후 기존 프로젝트들을 새 청구 계정으로 쉽게 이동 가능
5. 추가적인 외부 연락이나 대기 시간 불필요

:::

## dump 8

You have an application that looks for its licensing server on the IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server. What should you do?

A. Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.
B. Reserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.
C. Use the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.
D. Start the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address.

:::details 풀이

### 영문 문제 해석

**문제**: "라이선스 서버를 IP 10.0.3.21에서 찾는 애플리케이션이 있습니다. Compute Engine에 라이선스 서버를 배포해야 합니다. 애플리케이션의 구성을 변경하고 싶지 않으며 애플리케이션이 라이선스 서버에 도달할 수 있기를 원합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Application looks for licensing server on IP 10.0.3.21 (애플리케이션이 특정 IP에서 라이선스 서버 검색)
-   Deploy licensing server on Compute Engine (Compute Engine에 라이선스 서버 배포)
-   Do not change application configuration (애플리케이션 구성 변경 금지)
-   Application must reach licensing server (애플리케이션이 라이선스 서버에 접근 가능해야 함)

### 선택지 해석

**A. gcloud를 사용하여 IP 10.0.3.21을 정적 내부 IP 주소로 예약하고 라이선스 서버에 할당**

-   Static internal IP address 예약
-   내부 네트워크에서 사용
-   Private IP 범위 (10.x.x.x)

**B. gcloud를 사용하여 IP 10.0.3.21을 정적 공용 IP 주소로 예약하고 라이선스 서버에 할당**

-   Static public IP address 예약
-   인터넷에서 접근 가능한 공용 IP
-   10.x.x.x는 사설 IP 대역

**C. IP 10.0.3.21을 커스텀 임시 IP 주소로 사용하여 라이선스 서버에 할당**

-   Custom ephemeral IP 사용
-   임시 IP로 특정 IP 지정

**D. 자동 임시 IP 주소로 라이선스 서버를 시작한 후 정적 내부 IP 주소로 승격**

-   임시 IP로 시작
-   나중에 정적 IP로 변경
-   원하는 특정 IP (10.0.3.21) 보장 안됨

### 문제 분석 및 정답 도출

#### IP 주소 10.0.3.21 분석

**IP 주소 특성**

-   10.0.3.21은 RFC 1918 사설 IP 대역 (10.0.0.0/8)
-   내부 네트워크에서만 사용 가능
-   공용 인터넷에서 라우팅 불가

#### 각 선택지 분석

**A. Static internal IP 예약**

-   10.x.x.x는 사설 IP 대역으로 internal IP가 적합
-   정적 예약으로 IP 주소 고정 보장
-   애플리케이션이 동일한 VPC 내에서 접근 가능
-   구성 변경 없이 기존 IP 주소 사용

**B. Static public IP 예약**

-   10.x.x.x는 사설 IP 대역으로 public IP로 사용 불가
-   RFC 1918 사설 주소는 공용 IP로 할당 불가능
-   Google Cloud에서 허용하지 않음

**C. Custom ephemeral IP**

-   Ephemeral IP는 일시적이며 재시작 시 변경될 수 있음
-   특정 IP 주소 보장이 어려움
-   라이선스 서버의 지속적 접근성에 부적합

**D. 자동 ephemeral에서 static으로 승격**

-   처음에 할당받는 IP가 10.0.3.21일 보장 없음
-   다른 IP가 할당되면 애플리케이션 접근 불가
-   승격 과정에서도 원하는 IP 보장 안됨

#### Static Internal IP 예약 과정

**Google Cloud 명령어**

```bash
gcloud compute addresses create LICENSE-SERVER-IP --region=us-central1 --subnet=default --addresses=10.0.3.21
gcloud compute instances create licensing-server --private-network-ip=10.0.3.21
```

#### 네트워크 요구사항 충족

**애플리케이션과 라이선스 서버 연결**

-   동일한 VPC 네트워크 내 배치
-   방화벽 규칙으로 적절한 포트 허용
-   내부 DNS 또는 직접 IP 접근

#### 정답: A. gcloud를 사용하여 IP 10.0.3.21을 정적 내부 IP 주소로 예약하고 라이선스 서버에 할당

**선택 이유**:

1. 10.0.3.21은 사설 IP 대역으로 internal IP가 적합
2. 정적 예약으로 IP 주소 고정 보장
3. 애플리케이션 구성 변경 없이 기존 하드코딩된 IP 사용 가능
4. 동일한 VPC 내에서 애플리케이션이 라이선스 서버에 접근 가능
5. Google Cloud의 표준적인 정적 IP 관리 방식

RFC 1918 사설 IP 대역이란 인터넷에서 라우팅되지 않는 내부 네트워크 전용 IP 주소 범위를 말한다. 사설 IP 대역은 총 3개이다.

1. `10.0.0.0/8` - `10.0.0.0` ~ `10.255.255.255`까지 할당 가능
    - 대규모 기업 네트워크
2. `172.16.0.0/12` - `172.16.0.0` ~ `172.31.255.255`까지 할당 가능
    - 중간 규모 네트워크
3. `192.168.0.0/16` - `192.168.255.255`
    - 소규모 네트워크 / 가정용 라우터

위 IP주소로는 라우팅이 불가능하고, 공용 인터넷에서 해당 주소들로 패킷을 전송하지 않는다.

자동 ephemeral IP주소는 VM 인스턴스 생성 시 자동으로 부여되는 주소이고, VM 재시작시 해당 주소가 변경될 수 있다. 별도 IP 예약비용이 없는게 특징이다. 임시 작업 및 개발환경에서 사용 가능하다.

:::

## dump 9

You are deploying an application to App Engine. You want the number of instances to scale based on request rate. You need at least 3 unoccupied instances at all times. Which scaling type should you use?

A. Manual Scaling with 3 instances.
B. Basic Scaling with min_instances set to 3.
C. Basic Scaling with max_instances set to 3.
D. Automatic Scaling with min_idle_instances set to 3.

:::details 풀이

### 영문 문제 해석

**문제**: "App Engine에 애플리케이션을 배포하고 있습니다. 요청 비율에 따라 인스턴스 수가 확장되기를 원합니다. 항상 최소 3개의 사용되지 않는 인스턴스가 필요합니다. 어떤 스케일링 타입을 사용해야 할까요?"

**핵심 요구사항**:

-   Deploy to App Engine (App Engine에 배포)
-   Scale based on request rate (요청 비율 기반 스케일링)
-   At least 3 unoccupied instances at all times (항상 최소 3개의 비어있는 인스턴스)

### 선택지 해석

**A. 3개 인스턴스로 Manual Scaling 사용**

-   Manual Scaling: 수동으로 인스턴스 수 고정
-   요청량에 관계없이 항상 3개 인스턴스 유지
-   자동 스케일링 불가

**B. min_instances를 3으로 설정한 Basic Scaling 사용**

-   Basic Scaling: 요청이 있을 때만 인스턴스 시작
-   min_instances: 최소 인스턴스 수 설정
-   유휴(idle) 시간 후 인스턴스 종료

**C. max_instances를 3으로 설정한 Basic Scaling 사용**

-   Basic Scaling 사용
-   max_instances: 최대 인스턴스 수 제한
-   3개를 초과할 수 없음

**D. min_idle_instances를 3으로 설정한 Automatic Scaling 사용**

-   Automatic Scaling: 요청량에 따른 자동 스케일링
-   min_idle_instances: 항상 유지할 유휴 인스턴스 수
-   요청 처리 중인 인스턴스 외에 추가로 유휴 인스턴스 유지

### 문제 분석 및 정답 도출

#### App Engine 스케일링 타입별 특징

**Manual Scaling**

-   고정된 인스턴스 수
-   자동 스케일링 없음
-   항상 동일한 수의 인스턴스 실행
-   요청량 변화에 대응 불가

**Basic Scaling**

-   요청이 있을 때만 인스턴스 시작
-   idle_timeout 후 인스턴스 종료
-   min_instances: 최소 실행 인스턴스 수
-   max_instances: 최대 실행 인스턴스 수
-   단순한 on-demand 스케일링

**Automatic Scaling**

-   요청량에 따른 지능형 자동 스케일링
-   다양한 메트릭 기반 스케일링
-   min_idle_instances: 항상 대기 상태로 유지할 인스턴스 수
-   빠른 응답을 위한 유휴 인스턴스 pool 유지

#### 요구사항 분석

**"Scale based on request rate" (요청 비율 기반 스케일링)**

-   Manual Scaling: ❌ 고정된 인스턴스로 스케일링 불가
-   Basic Scaling: ✅ 요청에 따른 기본적 스케일링
-   Automatic Scaling: ✅ 고급 요청 기반 스케일링

**"At least 3 unoccupied instances" (최소 3개 비어있는 인스턴스)**

-   Manual Scaling: ❌ 모든 인스턴스가 요청 처리에 사용됨
-   Basic Scaling min_instances: ❌ 최소 실행 인스턴스이지 유휴 인스턴스가 아님
-   Basic Scaling max_instances: ❌ 최대 제한일 뿐, 유휴 인스턴스 보장 안함
-   Automatic Scaling min_idle_instances: ✅ 요청 처리와 별도로 유휴 인스턴스 보장

#### 각 선택지 상세 분석

**A. Manual Scaling with 3 instances**

-   항상 3개 인스턴스만 실행
-   요청량 증가 시 스케일링 불가
-   3개 모두 요청 처리에 사용되어 "unoccupied" 보장 안됨

**B. Basic Scaling with min_instances=3**

-   최소 3개 인스턴스 실행
-   요청이 많으면 추가 인스턴스 시작 가능
-   하지만 3개가 모두 요청 처리 중일 수 있어 "unoccupied" 보장 안됨

**C. Basic Scaling with max_instances=3**

-   최대 3개까지만 실행
-   요청 증가 시 스케일링 제한
-   유휴 인스턴스 보장 없음

**D. Automatic Scaling with min_idle_instances=3**

-   요청량에 따른 자동 스케일링
-   요청 처리 인스턴스와 별도로 항상 3개의 유휴 인스턴스 유지
-   두 요구사항 모두 충족

#### min_idle_instances의 동작 원리

**실제 시나리오**

```
현재 처리 중인 요청: 10개 → 10개 인스턴스 사용 중
min_idle_instances: 3 설정
결과: 총 13개 인스턈스 실행 (10개 사용 중 + 3개 대기 중)

새로운 요청 급증 시:
- 3개 유휴 인스턴스가 즉시 요청 처리 시작
- 필요 시 추가 인스턴스 자동 생성
- 항상 3개 유휴 인스턴스 pool 유지
```

#### 정답: D. min_idle_instances를 3으로 설정한 Automatic Scaling 사용

**선택 이유**:

1. 요청 비율에 따른 자동 스케일링 제공
2. min_idle_instances=3으로 항상 3개의 비어있는 인스턴스 보장
3. 트래픽 급증 시 즉시 대응 가능한 유휴 인스턴스 pool 유지
4. 두 가지 요구사항을 모두 완벽히 충족
5. App Engine의 고급 스케일링 기능 활용

:::

## dump 10

You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps. What should you do?

-   A. Use gcloud iam roles copy and specify the production project as the destination project.
-   B. Use gcloud iam roles copy and specify your organization as the destination organization.
-   C. In the Google Cloud Platform Console, use the 'create role from role' functionality.
-   D. In the Google Cloud Platform Console, use the 'create role' functionality and select all applicable permissions.

:::details 풀이

### 영문 문제 해석

**문제**: "적절한 IAM 역할이 정의된 개발 프로젝트가 있습니다. 프로덕션 프로젝트를 생성하고 있으며 가장 적은 단계를 사용하여 새 프로젝트에 동일한 IAM 역할을 적용하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Development project with IAM roles defined (IAM 역할이 정의된 개발 프로젝트)
-   Creating production project (프로덕션 프로젝트 생성)
-   Same IAM roles on new project (새 프로젝트에 동일한 IAM 역할)
-   Fewest possible steps (최소 단계)

### 선택지 해석

**A. gcloud iam roles copy를 사용하고 프로덕션 프로젝트를 대상 프로젝트로 지정**

-   명령줄을 통한 역할 복사
-   개발 프로젝트에서 프로덕션 프로젝트로 직접 복사
-   프로젝트 간 역할 복사

**B. gcloud iam roles copy를 사용하고 조직을 대상 조직으로 지정**

-   명령줄을 통한 역할 복사
-   조직 레벨로 역할 복사
-   조직 전체에서 사용 가능한 역할 생성

**C. Google Cloud Platform Console에서 'create role from role' 기능 사용**

-   웹 콘솔을 통한 역할 생성
-   기존 역할을 기반으로 새 역할 생성
-   GUI 기반 접근 방식

**D. Google Cloud Platform Console에서 'create role' 기능을 사용하고 모든 해당 권한 선택**

-   웹 콘솔에서 처음부터 역할 생성
-   모든 권한을 수동으로 선택
-   가장 많은 단계 필요

### 문제 분석 및 정답 도출

#### IAM 역할 복사 방법 비교

**gcloud CLI 방법**

-   명령어 한 줄로 즉시 실행 가능
-   스크립트 자동화 가능
-   빠르고 정확한 복사

**Console GUI 방법**

-   웹 인터페이스를 통한 수동 작업
-   클릭과 선택 작업 필요
-   시간이 더 소요됨

#### gcloud iam roles copy 명령어 분석

**기본 문법**

```bash
gcloud iam roles copy --source=SOURCE_ROLE_ID --destination=DEST_ROLE_ID --dest-project=DEST_PROJECT_ID
```

**프로젝트 간 복사 vs 조직 레벨 복사**

**A. 프로덕션 프로젝트로 복사**

```bash
gcloud iam roles copy --source=projects/dev-project/roles/customRole --destination=customRole --dest-project=prod-project
```

-   개발 프로젝트의 커스텀 역할을 프로덕션 프로젝트로 직접 복사
-   프로덕션 프로젝트에서만 사용 가능
-   정확히 요구사항에 맞음

**B. 조직으로 복사**

```bash
gcloud iam roles copy --source=projects/dev-project/roles/customRole --destination=customRole --dest-organization=123456789
```

-   조직 레벨로 역할 복사
-   조직 내 모든 프로젝트에서 사용 가능
-   과도한 범위 (프로덕션 프로젝트만 필요한데 조직 전체에 적용)

#### Console 방법 분석

**C. 'create role from role' 기능**

-   기존 역할을 선택하여 복사본 생성
-   GUI에서 몇 번의 클릭 필요
-   gcloud보다 더 많은 단계

**D. 'create role' 기능**

-   처음부터 새로운 역할 생성
-   모든 권한을 개별적으로 선택해야 함
-   가장 많은 시간과 단계 소요
-   실수 가능성 높음

#### 최소 단계 비교

**단계 수 비교**

1. **A (gcloud copy to project)**: 1단계 - 명령어 한 줄 실행
2. **B (gcloud copy to org)**: 1단계 - 하지만 불필요한 범위
3. **C (console from role)**: 3-4단계 - 웹 접속, 역할 선택, 복사, 설정
4. **D (console create)**: 5+ 단계 - 웹 접속, 역할 생성, 권한 개별 선택

#### 범위 적절성

**요구사항**: "새 프로젝트에 동일한 IAM 역할"

-   A: ✅ 프로덕션 프로젝트에만 적용 (정확한 범위)
-   B: ❌ 조직 전체에 적용 (과도한 범위)
-   C: ✅ 프로젝트별 적용 가능
-   D: ✅ 프로젝트별 적용 가능

#### 정답: A. gcloud iam roles copy를 사용하고 프로덕션 프로젝트를 대상 프로젝트로 지정

**선택 이유**:

1. 최소 단계: 명령어 한 줄로 완료
2. 정확한 범위: 프로덕션 프로젝트에만 역할 복사
3. 완전한 복사: 모든 권한이 정확히 복사됨
4. 자동화 가능: 스크립트로 반복 실행 가능
5. 실수 방지: 수동 권한 선택으로 인한 누락 방지

:::

## dump 11

You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use?

-   A. Deployment Manager
-   B. Cloud Composer
-   C. Managed Instance Group
-   D. Unmanaged Instance Group

:::details 풀이

### 영문 문제 해석

**문제**: "Compute Engine에서 VM을 동적으로 프로비저닝하는 방법이 필요합니다. 정확한 사양은 전용 구성 파일에 있을 것입니다. Google의 권장 사례를 따르고 싶습니다. 어떤 방법을 사용해야 할까요?"

**핵심 요구사항**:

-   Dynamic provisioning of VMs (VM의 동적 프로비저닝)
    -   VM 생성, VM 수 및 사양, 배포 및 관리 등의 과정이 자동으로 이루어지는 것을 의미한다.
    -   정적 프로비저닝은 관리자가 VM을 하나씩 직접 생성하고, 구성과 관리가 고정적으로 이루어진다.
-   Exact specifications in dedicated configuration file (전용 구성 파일에 정확한 사양)
-   Follow Google's recommended practices (Google 권장 사례 준수)
-   On Compute Engine (Compute Engine 사용)

### 선택지 해석

**A. Deployment Manager**

-   Google Cloud의 Infrastructure as Code (IaC) 서비스
-   YAML/Jinja2/Python 템플릿을 사용한 리소스 정의
-   구성 파일 기반 인프라 배포 및 관리
-   선언적 구성 관리

**B. Cloud Composer**

-   Apache Airflow 기반 워크플로우 오케스트레이션 서비스
-   복잡한 데이터 파이프라인 및 워크플로우 관리
-   DAG(Directed Acyclic Graph) 기반 작업 스케줄링
-   데이터 처리 워크플로우용

**C. Managed Instance Group (MIG)**

-   동일한 VM 인스턴스들의 관리형 그룹
-   Instance Template 기반 자동 스케일링
-   자동 복구 및 업데이트 기능
-   동적 스케일링 지원

**D. Unmanaged Instance Group**

-   기존 VM 인스턴스들의 단순한 그룹핑
-   수동 관리 필요
-   자동 스케일링 및 관리 기능 없음
-   정적인 구성

### 문제 분석 및 정답 도출

#### 요구사항 세부 분석

**"Dynamic provisioning" (동적 프로비저닝)**

-   필요에 따라 자동으로 리소스 생성/삭제
-   런타임에 인프라 변경 가능
-   선언적 구성을 통한 자동 배포

**"Dedicated configuration file" (전용 구성 파일)**

-   Infrastructure as Code 접근 방식
-   코드로 인프라 정의 및 버전 관리
-   재사용 가능한 템플릿 기반 배포

**"Google's recommended practices" (Google 권장 사례)**

-   IaC 사용 권장
-   자동화된 배포 및 관리
-   버전 관리 및 재현 가능한 배포

#### 각 선택지 상세 분석

**A. Deployment Manager**

-   ✅ 구성 파일 기반 (YAML/Jinja2/Python 템플릿)
-   ✅ 동적 프로비저닝 지원
-   ✅ Google의 공식 IaC 도구
-   ✅ 선언적 구성으로 VM 사양 정의 가능
-   ✅ 버전 관리 및 롤백 지원

**B. Cloud Composer**

-   ❌ 워크플로우 오케스트레이션 도구 (VM 프로비저닝용 아님)
-   ❌ 주로 데이터 파이프라인 관리용
-   ❌ Infrastructure as Code 도구가 아님
-   ❌ VM 사양 정의에 부적합

**C. Managed Instance Group**

-   ✅ 동적 스케일링 지원
-   ✅ Instance Template 기반 구성
-   ❌ 단일 템플릿 기반으로 제한적
-   ❌ 복잡한 인프라 구성에는 한계
-   ❌ 전체적인 IaC 솔루션이 아님

**D. Unmanaged Instance Group**

-   ❌ 수동 관리 필요
-   ❌ 동적 프로비저닝 지원 안함
-   ❌ 구성 파일 기반 관리 불가
-   ❌ 정적인 그룹핑만 제공

#### Deployment Manager의 장점

**Infrastructure as Code**

```yaml
# deployment-manager-template.yaml
resources:
    - name: web-server-template
      type: compute.v1.instanceTemplate
      properties:
          properties:
              machineType: n1-standard-1
              disks:
                  - boot: true
                    initializeParams:
                        sourceImage: projects/debian-cloud/global/images/family/debian-10
              networkInterfaces:
                  - network: global/networks/default
                    accessConfigs:
                        - type: ONE_TO_ONE_NAT

    - name: web-server-group
      type: compute.v1.instanceGroupManager
      properties:
          baseInstanceName: web-server
          instanceTemplate: $(ref.web-server-template.selfLink)
          targetSize: 3
```

**동적 관리**

-   구성 파일 변경 후 재배포로 인프라 업데이트
-   자동 롤백 및 버전 관리
-   의존성 관리 및 순서 보장

**Google 권장 사례**

-   공식 IaC 도구로 권장
-   다른 Google Cloud 서비스와 완전 통합
-   엔터프라이즈급 기능 제공

#### 다른 도구들과의 비교

**Terraform vs Deployment Manager**

-   Terraform: 멀티 클라우드 지원
-   Deployment Manager: Google Cloud 특화, 더 깊은 통합

**Instance Groups vs Deployment Manager**

-   Instance Groups: VM 그룹 관리에 특화
-   Deployment Manager: 전체 인프라 스택 관리

#### 정답: A. Deployment Manager

**선택 이유**:

1. 구성 파일 기반 Infrastructure as Code 지원
2. 동적 프로비저닝 및 리소스 관리 제공
3. Google Cloud의 공식 IaC 도구로 권장 사례 준수
4. YAML/Jinja2/Python 템플릿으로 복잡한 VM 사양 정의 가능
5. 버전 관리, 롤백, 의존성 관리 등 엔터프라이즈 기능 제공
6. Google Cloud 서비스와의 완전한 통합

IaC는 인프라를 코드로 관리하는 방법론을 말한다. 기존에는 GUI, 명령어 입력, 문서화 등으로 인프라를 관리했지만 IaC는 코드 작성을 통해 자동으로 배포 / 버전 관리 등이 이루어진다.

코드로 관리되기 때문에 변경사항이 코드로 추적 가능하고, 어떻게 구성하는 지에 대한 서술이 아닌 무엇을 구성하는지 작성함으로써 선언적인 코드 작성이 가능하다. 또한 같은 코드로 인프라 구성 시 동일한 결과를 나타낸다는 멱등성이 성립된다.

구글 클라우드에서는 Deployment Manager가 공식 IaC 도구이며, Cloud Foundation Toolkit이 모범 사례로 사용되는 템플릿이다.

:::

## dump 12

You have a Dockerfile that you need to deploy on Kubernetes Engine. What should you do?

A. Use kubectl app deploy `<dockerfilename>`.
B. Use gcloud app deploy `<dockerfilename>`.
C. Create a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.
D. Create a docker image from the Dockerfile and upload it to Cloud Storage. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.

:::details 풀이

### 영문 문제 해석

**문제**: "Kubernetes Engine에 배포해야 하는 Dockerfile이 있습니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Dockerfile exists (Dockerfile 존재)
-   Deploy on Kubernetes Engine (Kubernetes Engine에 배포)
-   Proper deployment process (적절한 배포 프로세스)

### 선택지 해석

**A. kubectl app deploy `<dockerfilename>` 사용**

-   kubectl 명령어 사용
-   app deploy 하위 명령어로 Dockerfile 직접 배포
-   직접적인 Dockerfile 배포 시도

**B. gcloud app deploy `<dockerfilename>` 사용**

-   gcloud 명령어 사용
-   App Engine 배포 명령어
-   Kubernetes Engine이 아닌 App Engine용 명령어

**C. Dockerfile에서 Docker 이미지 생성하여 Container Registry에 업로드. Deployment YAML 파일을 생성하여 해당 이미지를 가리키도록 설정. kubectl을 사용하여 파일로 배포 생성**

-   Docker 이미지 빌드 및 Container Registry 업로드
-   Kubernetes Deployment YAML 파일 생성
-   kubectl을 통한 배포

**D. Dockerfile에서 Docker 이미지 생성하여 Cloud Storage에 업로드. Deployment YAML 파일을 생성하여 해당 이미지를 가리키도록 설정. kubectl을 사용하여 파일로 배포 생성**

-   Docker 이미지 빌드 및 Cloud Storage 업로드
-   Kubernetes Deployment YAML 파일 생성
-   kubectl을 통한 배포

### 문제 분석 및 정답 도출

#### Kubernetes 배포 프로세스 이해

**Kubernetes의 기본 동작 원리**

-   Kubernetes는 컨테이너 이미지를 실행
-   Dockerfile 자체를 직접 실행하지 않음
-   컨테이너 레지스트리에서 이미지를 Pull하여 실행

**표준 배포 단계**

1. Dockerfile → Docker Image 빌드
2. Docker Image → Container Registry 업로드
3. Deployment YAML 작성 (이미지 참조)
4. kubectl로 Deployment 생성

#### 각 선택지 상세 분석

**A. kubectl app deploy `<dockerfilename>`**

-   kubectl에 'app deploy' 하위 명령어는 존재하지 않음
-   kubectl은 Kubernetes 리소스를 관리하는 도구
-   Dockerfile을 직접 처리하는 기능 없음
-   잘못된 명령어

**B. gcloud app deploy `<dockerfilename>`**

-   App Engine 배포 명령어
-   Kubernetes Engine이 아닌 App Engine용
-   서비스가 다름 (App Engine ≠ Kubernetes Engine)
-   요구사항과 불일치

**C. Container Registry 사용**

-   ✅ 올바른 Docker 빌드 프로세스
-   ✅ Container Registry는 Docker 이미지 저장소
-   ✅ Kubernetes가 Container Registry에서 이미지 Pull 가능
-   ✅ 표준 Kubernetes 배포 프로세스

**D. Cloud Storage 사용**

-   ✅ 올바른 Docker 빌드 프로세스
-   ❌ Cloud Storage는 파일 저장소 (Docker 이미지 레지스트리 아님)
-   ❌ Kubernetes가 Cloud Storage에서 직접 이미지 Pull 불가
-   ❌ 잘못된 저장소 선택

#### Container Registry vs Cloud Storage

**Container Registry**

-   Docker 이미지 전용 레지스트리 서비스
-   Docker pull/push 명령어 지원
-   Kubernetes와 완전 통합
-   이미지 버전 관리 및 보안 스캔
-   표준 Docker Registry API 지원

**Cloud Storage**

-   일반적인 파일 저장 서비스
-   Docker 이미지 형태로 저장 불가
-   Kubernetes가 직접 접근 불가
-   Docker Registry 프로토콜 미지원

#### 올바른 배포 프로세스 (선택지 C)

**1. Docker 이미지 빌드**

```bash
# Dockerfile에서 이미지 생성
docker build -t my-app:v1.0 .
```

**2. Container Registry에 업로드**

```bash
# 태그 지정
docker tag my-app:v1.0 gcr.io/my-project/my-app:v1.0

# 업로드
docker push gcr.io/my-project/my-app:v1.0
```

**3. Deployment YAML 작성**

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
    name: my-app-deployment
spec:
    replicas: 3
    selector:
        matchLabels:
            app: my-app
    template:
        metadata:
            labels:
                app: my-app
        spec:
            containers:
                - name: my-app
                  image: gcr.io/my-project/my-app:v1.0
                  ports:
                      - containerPort: 8080
```

**4. kubectl로 배포**

```bash
kubectl apply -f deployment.yaml
```

#### Google Cloud의 통합 서비스

**Container Registry와 GKE 통합**

-   같은 프로젝트 내에서 자동 인증
-   IAM 권한으로 접근 제어
-   네트워크 최적화된 이미지 전송
-   보안 및 취약점 스캔

#### 정답: C. Dockerfile에서 Docker 이미지를 생성하여 Container Registry에 업로드하고, Deployment YAML 파일을 생성하여 해당 이미지를 가리키도록 설정한 후, kubectl을 사용하여 해당 파일로 배포를 생성

**선택 이유**:

1. 표준 Kubernetes 배포 프로세스 준수
2. Container Registry는 Docker 이미지 전용 레지스트리 서비스
3. Kubernetes가 Container Registry에서 이미지를 정상적으로 Pull 가능
4. Google Cloud의 권장 사례 및 모범 사례
5. 보안, 버전 관리, 통합 측면에서 최적
6. 실제 운영 환경에서 사용하는 표준 방법

컨테이너 레지스트리는 Docker 이미지 저장소이다. 주소 형식은 `gcr.io/[PROJECT-ID]/[IMAGE-NAME]:[TAG]`이다.

```bash
# 1. 개발자가 코드 작성
vim app.py

# 2. Dockerfile 작성
vim Dockerfile

# 3. 이미지 빌드
docker build -t gcr.io/my-project/my-app:v1.0 .

# 4. 테스트
docker run gcr.io/my-project/my-app:v1.0

# 5. 업로드
docker push gcr.io/my-project/my-app:v1.0

# 6. Kubernetes에서 사용
kubectl run my-app --image=gcr.io/my-project/my-app:v1.0
```

:::

## dump 13

Your development team needs a new Jenkins server for their project. You need to deploy the server using the fewest steps possible. What should you do?

-   A. Download and deploy the Jenkins Java WAR to App Engine Standard.
-   B. Create a new Compute Engine instance and install Jenkins through the command line interface.
-   C. Create a Kubernetes cluster on Compute Engine and create a deployment with the Jenkins Docker image.
-   D. Use GCP Marketplace to launch the Jenkins solution.

:::details 풀이

### 영문 문제 해석

**문제**: "개발팀이 프로젝트를 위해 새로운 Jenkins 서버가 필요합니다. 가능한 한 최소한의 단계로 서버를 배포해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   New Jenkins server for development team (개발팀을 위한 새 Jenkins 서버)
-   Deploy using fewest steps possible (최소한의 단계로 배포)
-   Quick and simple deployment (빠르고 간단한 배포)

### 선택지 해석

**A. Jenkins Java WAR를 App Engine Standard에 다운로드하고 배포**

-   App Engine Standard 환경 사용
-   Jenkins WAR 파일을 직접 배포
-   서버리스 환경에서 Jenkins 실행

**B. 새로운 Compute Engine 인스턴스를 생성하고 명령줄 인터페이스를 통해 Jenkins 설치**

-   VM 인스턴스 수동 생성
-   SSH 접속 후 수동 설치
-   명령줄 기반 설치 과정

**C. Compute Engine에서 Kubernetes 클러스터를 생성하고 Jenkins Docker 이미지로 배포 생성**

-   GKE 클러스터 생성
-   Kubernetes 배포 설정
-   Jenkins Docker 컨테이너 실행

**D. GCP Marketplace를 사용하여 Jenkins 솔루션 실행**

-   Google Cloud Marketplace 활용
-   사전 구성된 Jenkins 솔루션 사용
-   클릭 몇 번으로 배포 완료

### 문제 분석 및 정답 도출

#### 각 선택지 단계 수 분석

**A. App Engine Standard + Jenkins WAR**
단계:

1. Jenkins WAR 파일 다운로드
    - 젠킨스 WAR는 자바 웹앱을 패키징한 압축파일이다.
2. app.yaml 구성 파일 작성
3. gcloud app deploy 실행
    - 문제점: App Engine Standard는 Jenkins 같은 지속적인 백그라운드 서비스에 부적합
    - 앱엔진 스탠다드 환경은 요청이 올때만 어플리케이션을 실행한다. (백그라운드 스레드를 제한한다.)
    - Git, 빌드 도구 등 외부 연결도 제한된다.
    - 앱엔진 스탠다드는 읽기 전용 파일시스템을 갖는다.

**B. Compute Engine + 수동 설치**
단계:

1. VM 인스턴스 생성
2. SSH 접속
3. Java 설치
4. Jenkins 다운로드
5. Jenkins 설치 및 구성
6. 방화벽 규칙 설정
7. 초기 설정 및 플러그인 설치
   총 7+단계 (가장 많음)

**C. GKE + Jenkins Docker**
단계:

1. GKE 클러스터 생성
2. kubectl 구성
3. Jenkins Deployment YAML 작성
4. Service YAML 작성
5. kubectl apply로 배포
6. Persistent Volume 설정
   총 6단계

**D. GCP Marketplace**
단계:

1. Google Cloud Console → Marketplace 접속
2. Jenkins 검색
3. 구성 옵션 선택 (VM 크기, 네트워크 등)
4. Deploy 버튼 클릭
   총 4단계 (가장 적음)

#### GCP Marketplace의 장점

**사전 구성된 솔루션**

-   Jenkins가 이미 설치 및 구성됨
-   최적화된 설정 적용
-   필요한 플러그인 포함
-   보안 설정 자동 적용

**자동 인프라 프로비저닝**

-   VM 인스턴스 자동 생성
-   방화벽 규칙 자동 설정
-   네트워크 구성 자동 적용
-   디스크 및 스토리지 자동 구성

**즉시 사용 가능**

-   배포 완료 후 바로 Jenkins 접속 가능
-   관리자 계정 자동 생성
-   SSL/TLS 인증서 자동 설정
-   모니터링 및 백업 옵션 제공

##### 다른 선택지들의 한계

**A. App Engine Standard 문제점**

-   Jenkins는 지속적으로 실행되어야 하는 서비스
-   App Engine Standard는 요청 기반 실행 모델
-   Jenkins의 빌드 작업, 스케줄링 기능과 호환성 문제
-   파일 시스템 접근 제한

**B. 수동 설치의 단점**

-   시간 소모적 (수 시간 소요 가능)
-   설정 실수 가능성
-   보안 설정 누락 위험
-   초기 구성의 복잡성

**C. Kubernetes의 복잡성**

-   클러스터 관리 오버헤드
-   YAML 구성 파일 작성 필요
-   Persistent Volume 설정 복잡
-   Jenkins에 특화된 설정 필요

#### GCP Marketplace Jenkins 솔루션 특징

**포함된 구성요소**

-   Jenkins LTS (Long Term Support) 버전
-   필수 플러그인 사전 설치
-   NGINX 리버스 프록시
-   Let's Encrypt SSL 인증서
-   자동 백업 스크립트

**배포 옵션**

-   VM 크기 선택 (n1-standard-1~8)
-   디스크 크기 설정
-   네트워크 및 서브넷 선택
-   방화벽 규칙 자동 생성

**사후 관리**

-   Google Cloud Console에서 상태 모니터링
-   자동 업데이트 옵션
-   백업 및 복원 기능
-   로그 및 메트릭 수집

#### 실제 배포 과정 (선택지 D)

**1단계: Marketplace 접속**

```
Google Cloud Console → Navigation Menu → Marketplace
```

**2단계: Jenkins 검색 및 선택**

```
검색창에 "Jenkins" 입력 → "Jenkins Certified by Bitnami" 선택
```

**3단계: 구성 설정**

```
- Deployment name: jenkins-server
- Zone: us-central1-a
- Machine type: n1-standard-2
- Boot disk size: 20GB
- Network: default
```

**4단계: 배포 실행**

```
"Deploy" 버튼 클릭 → 5-10분 후 배포 완료
```

#### 정답: D. GCP Marketplace를 사용하여 Jenkins 솔루션 실행

**선택 이유**:

1. 최소 단계: 단 4단계로 완료 (다른 방법 대비 가장 적음)
2. 사전 구성: Jenkins가 이미 최적화된 상태로 설치
3. 자동 인프라: VM, 네트워크, 보안 설정 자동 생성
4. 즉시 사용: 배포 완료 후 바로 Jenkins 접속 가능
5. 관리 편의성: Google Cloud 통합 모니터링 및 관리
6. 검증된 솔루션: Bitnami에서 제공하는 안정적인 이미지
7. 시간 절약: 수동 설치 대비 95% 시간 단축

젠킨스는 CI/CD를 자동화해주는 도구이다. CI 단계에서 통합된 코드를 자동으로 빌드하고 실행해주고, CD 단계에서 테스트 통과 코드를 자동으로 프로덕션에 배포해준다.

:::

## dump 14

You need to update a deployment in Deployment Manager without any resource downtime in the deployment. Which command should you use?

A. gcloud deployment-manager deployments create --config `<deployment-config-path>`
B. gcloud deployment-manager deployments update --config `<deployment-config-path>`
C. gcloud deployment-manager resources create --config `<deployment-config-path>`
D. gcloud deployment-manager resources update --config `<deployment-config-path>`

:::details 풀이

### 영문 문제 해석

**문제**: "배포에서 리소스 다운타임 없이 Deployment Manager의 배포를 업데이트해야 합니다. 어떤 명령어를 사용해야 할까요?"

**핵심 요구사항**:

-   Update a deployment in Deployment Manager (Deployment Manager에서 배포 업데이트)
-   Without any resource downtime (리소스 다운타임 없이)
-   Existing deployment modification (기존 배포 수정)

### 선택지 해석

**A. gcloud deployment-manager deployments create --config `<deployment-config-path>`**

-   새로운 배포를 생성하는 명령어
-   create 동작으로 기존 배포와 별개
-   신규 배포 생성용

**B. gcloud deployment-manager deployments update --config `<deployment-config-path>`**

-   기존 배포를 업데이트하는 명령어
-   update 동작으로 기존 배포 수정
-   배포 수준에서의 업데이트

**C. gcloud deployment-manager resources create --config `<deployment-config-path>`**

-   새로운 리소스를 생성하는 명령어
-   개별 리소스 생성용
-   배포 전체 업데이트와는 다른 개념

**D. gcloud deployment-manager resources update --config `<deployment-config-path>`**

-   개별 리소스를 업데이트하는 명령어
-   특정 리소스만 수정
-   배포 전체가 아닌 리소스 단위 업데이트

### 문제 분석 및 정답 도출

#### Deployment Manager 구조 이해

**Deployment (배포)**

-   여러 리소스들의 집합
-   YAML/Jinja2/Python 템플릿으로 정의
-   하나의 논리적 단위로 관리

**Resources (리소스)**

-   배포 내의 개별 구성 요소
-   VM, 네트워크, 로드밸런서 등
-   배포에 속하는 개별 클라우드 리소스

#### 명령어 체계 분석

**deployments vs resources**

```bash
# 배포 레벨 명령어 (전체 관리)
gcloud deployment-manager deployments [create|update|delete]

# 리소스 레벨 명령어 (개별 관리)
gcloud deployment-manager resources [create|update|delete]
```

**create vs update**

```bash
# 생성 (신규)
gcloud deployment-manager deployments create my-deployment --config config.yaml

# 업데이트 (기존 수정)
gcloud deployment-manager deployments update my-deployment --config config.yaml
```

#### 각 선택지 상세 분석

**A. deployments create**

-   새로운 배포 생성
-   기존 배포가 있으면 충돌 오류 발생
-   업데이트가 아닌 신규 생성
-   요구사항과 불일치

**B. deployments update**

-   기존 배포의 구성 수정
-   다운타임 최소화 전략 사용
-   Rolling update 또는 Blue-Green 배포 지원
-   전체 배포 단위에서 일관된 업데이트

**C. resources create**

-   개별 리소스 신규 생성
-   배포와 독립적으로 리소스만 추가
-   전체 배포 업데이트와는 다른 개념
-   요구사항과 불일치

**D. resources update**

-   개별 리소스만 수정
-   배포 전체의 일관성 보장 어려움
-   복잡한 의존성 관리 필요
-   전체적인 배포 업데이트에 부적합

#### Deployment Manager의 업데이트 메커니즘

**Smart Update 전략**

```
1. 구성 파일 변경사항 분석
2. 필요한 리소스만 선별적 업데이트
3. 의존성 순서에 따른 순차 업데이트
4. 롤백 지점 자동 생성
```

**다운타임 최소화 방법**

```
- Rolling Update: 점진적 교체
- Blue-Green Deployment: 새 버전 준비 후 전환
- 헬스체크 기반 업데이트
- 자동 롤백 기능
```

#### 실제 업데이트 과정

**기존 배포 확인**

```bash
gcloud deployment-manager deployments list
gcloud deployment-manager deployments describe my-deployment
```

**구성 파일 수정**

```yaml
# config.yaml 수정
resources:
    - name: web-server
      type: compute.v1.instance
      properties:
          machineType: n1-standard-2 # n1-standard-1에서 업그레이드
          zone: us-central1-a
```

**배포 업데이트 실행**

```bash
gcloud deployment-manager deployments update my-deployment --config config.yaml
```

**업데이트 모니터링**

```bash
gcloud deployment-manager operations list
gcloud deployment-manager operations describe OPERATION_ID
```

#### 다운타임 없는 업데이트의 원리

**Managed Instance Group 예시**

```yaml
# 기존: 3개 인스턴스
resources:
- name: web-servers
  type: compute.v1.instanceGroupManager
  properties:
    targetSize: 3
    instanceTemplate: old-template

# 업데이트: 새 템플릿으로 변경
resources:
- name: web-servers
  type: compute.v1.instanceGroupManager
  properties:
    targetSize: 3
    instanceTemplate: new-template  # 새 템플릿
```

**업데이트 과정**

```
1. 새 템플릿으로 1개 인스턴스 생성
2. 헬스체크 통과 확인
3. 기존 인스턴스 1개 삭제
4. 과정 반복 (Rolling Update)
5. 모든 인스턴스가 새 버전으로 교체 완료
```

#### deployments update의 장점

**일관된 상태 관리**

-   전체 배포의 일관성 보장
-   의존성 자동 해결
-   원자적 업데이트 (All or Nothing)

**자동 롤백**

-   업데이트 실패 시 자동 롤백
-   이전 상태로 안전하게 복원
-   데이터 무결성 보장

**모니터링 및 로깅**

-   업데이트 과정 전체 추적
-   상세한 로그 및 메트릭
-   문제 발생 시 신속한 진단

#### 정답: B. gcloud deployment-manager deployments update --config `<deployment-config-path>`

**선택 이유**:

1. 기존 배포를 업데이트하는 올바른 명령어
2. 다운타임 최소화를 위한 스마트 업데이트 전략 사용
3. 전체 배포의 일관성과 의존성을 자동으로 관리
4. Rolling Update 및 Blue-Green 배포 등 무중단 업데이트 지원
5. 자동 롤백 및 오류 복구 기능 제공
6. Google Cloud의 권장 사례 및 표준 방법

다운타임(Downtime)은 시스템이나 서비스가 정상적으로 작동하지 않는 시간을 의미한다. 다운타임 없이 배포하기 위해 롤링 업데이트를 지원하는 서비스가 필요하다.

:::

## dump 15

You need to run an important query in BigQuery but expect it to return a lot of records. You want to find out how much it will cost to run the query. You are using on-demand pricing. What should you do?

A. Arrange to switch to Flat-Rate pricing for this query, then move back to on-demand.
B. Use the command line to run a dry run query to estimate the number of bytes read. Then convert that bytes estimate to dollars using the Pricing Calculator.
C. Use the command line to run a dry run query to estimate the number of bytes returned. Then convert that bytes estimate to dollars using the Pricing Calculator.
D. Run a select count (\*) to get an idea of how many records your query will look through. Then convert that number of rows to dollars using the Pricing Calculator.

:::details 풀이

### 영문 문제 해석

**문제**: "BigQuery에서 중요한 쿼리를 실행해야 하지만 많은 레코드를 반환할 것으로 예상됩니다. 쿼리 실행 비용이 얼마나 될지 알고 싶습니다. On-demand 요금제를 사용하고 있습니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Important query in BigQuery (BigQuery에서 중요한 쿼리)
-   Expect to return a lot of records (많은 레코드 반환 예상)
-   Find out cost to run the query (쿼리 실행 비용 확인)
-   Using on-demand pricing (On-demand 요금제 사용)

### 선택지 해석

**A. 이 쿼리를 위해 Flat-Rate 요금제로 전환한 후 다시 On-demand로 복구**

-   요금제 변경을 통한 비용 관리
-   일시적으로 다른 요금제 사용
-   쿼리 완료 후 원래 요금제로 복귀

**B. 명령줄을 사용하여 dry run 쿼리로 읽을 바이트 수를 추정한 후 Pricing Calculator로 바이트 추정치를 달러로 변환**

-   Dry run으로 데이터 스캔량 미리 확인
-   읽을 바이트(bytes read) 기반 비용 계산
-   실제 쿼리 실행 전 비용 예측

**C. 명령줄을 사용하여 dry run 쿼리로 반환될 바이트 수를 추정한 후 Pricing Calculator로 바이트 추정치를 달러로 변환**

-   Dry run으로 결과 크기 미리 확인
-   반환될 바이트(bytes returned) 기반 비용 계산
-   결과 데이터 크기 기반 예측

**D. select count(\*)를 실행하여 쿼리가 살펴볼 레코드 수를 파악한 후 Pricing Calculator로 행 수를 달러로 변환**

-   레코드 수 기반 비용 예측
-   count(\*) 쿼리로 행 수 확인
-   행 개수를 비용으로 변환

### 문제 분석 및 정답 도출

#### BigQuery On-Demand 요금제 이해

**과금 기준**

-   BigQuery On-demand 요금제는 **스캔한 데이터의 바이트 수**를 기준으로 과금
-   쿼리가 읽은 데이터량 (bytes read/processed)이 비용 결정 요소
-   반환되는 결과의 크기나 행 개수는 과금과 무관

**요금 구조 (2024년 기준)**

```
$6.25 per TB (테라바이트당 $6.25)
첫 1TB/월은 무료
```

#### 각 선택지 상세 분석

**A. Flat-Rate로 전환**

-   Flat-Rate는 고정 비용 요금제 ($2,000-40,000/월)
-   일회성 쿼리를 위한 요금제 변경은 비효율적
-   변경/복구 과정이 복잡하고 시간 소요
-   단일 쿼리 비용 예측 목적에 부적합

**B. Dry run으로 읽을 바이트 수 추정** ✅

-   BigQuery의 dry run 기능 활용
-   실제 데이터를 스캔하지 않고 스캔할 데이터량만 계산
-   On-demand 과금 기준과 정확히 일치
-   실제 비용 예측에 가장 적합

**C. Dry run으로 반환될 바이트 수 추정**

-   반환되는 결과 크기는 BigQuery 과금과 무관
-   On-demand는 스캔한 데이터량으로 과금하므로 잘못된 접근
-   결과 크기와 실제 비용 간의 연관성 없음
-   사용자에게 반환될 쿼리 결과의 크기를 의미한다.

**D. count(\*)로 레코드 수 확인**

-   행 개수는 BigQuery On-demand 과금 기준이 아님
-   스캔한 데이터의 바이트 수가 중요
-   행 수가 많아도 컬럼이 적으면 스캔 데이터량은 적을 수 있음
-   정확한 비용 예측 불가

#### BigQuery Dry Run 사용법

**명령줄에서 dry run 실행**

```bash
# dry run으로 스캔할 데이터량 확인
bq query --use_legacy_sql=false --dry_run \
'SELECT customer_id, order_total, order_date
 FROM `project.dataset.orders`
 WHERE order_date >= "2023-01-01"'

# 출력 예시:
Query successfully validated. Assuming the tables are not modified,
running this query will process 2.5 GB of data.
```

**웹 UI에서 dry run**

```
1. BigQuery Console 접속
2. 쿼리 입력
3. "More" 버튼 → "Query validator" 클릭
4. 스캔할 데이터량 표시: "This query will process 2.5 GB"
```

#### 비용 계산 과정

**1단계: Dry run으로 스캔 데이터량 확인**

```bash
Query will process 2.5 GB of data
```

**2단계: 비용 계산**

```
2.5 GB = 0.0025 TB
비용 = 0.0025 TB × $6.25/TB = $0.015625 (약 1.6센트)
```

**3단계: Pricing Calculator 활용**

```
Google Cloud Pricing Calculator에서:
- BigQuery 선택
- On-demand queries
- Query data: 2.5 GB 입력
- 결과: $0.02 (반올림된 비용)
```

#### Dry Run의 장점

**정확성**

-   실제 쿼리 실행 시 스캔할 데이터량과 동일
-   파티션 제거, 컬럼 프루닝 등 최적화 반영
-   가장 정확한 비용 예측

**안전성**

-   실제 데이터를 스캔하지 않음
-   비용 발생 없이 예측 가능
-   쿼리 문법 검증도 동시에 수행

**빠른 실행**

-   메타데이터만 확인하므로 빠름
-   대용량 테이블도 몇 초 내 결과 확인
-   반복적인 비용 예측 가능

#### 스캔 데이터량에 영향을 주는 요소

**테이블 구조**

```sql
-- 모든 컬럼 스캔 (많은 데이터)
SELECT * FROM large_table

-- 특정 컬럼만 스캔 (적은 데이터)
SELECT id, name FROM large_table
```

**파티션 활용**

```sql
-- 전체 파티션 스캔
SELECT * FROM partitioned_table

-- 특정 파티션만 스캔 (데이터량 대폭 감소)
SELECT * FROM partitioned_table
WHERE _PARTITIONTIME >= '2023-01-01'
```

**클러스터링 활용**

```sql
-- 클러스터 컬럼으로 필터링 시 스캔량 감소
SELECT * FROM clustered_table
WHERE cluster_column = 'specific_value'
```

#### 정답: B. 명령줄을 사용하여 dry run 쿼리로 읽을 바이트 수를 추정한 후 Pricing Calculator로 바이트 추정치를 달러로 변환

**선택 이유**:

1. BigQuery On-demand 과금 기준과 정확히 일치 (스캔한 데이터량 기준)
2. Dry run 기능으로 실제 비용 발생 없이 정확한 데이터량 예측 가능
3. 쿼리 최적화 효과가 반영된 실제 스캔량 확인
4. Pricing Calculator로 정확한 달러 금액 변환 가능
5. Google Cloud 공식 권장 방법
6. 빠르고 안전한 비용 예측 방법

:::

## dump 16

You have a single binary application that you want to run on Google Cloud Platform. You decided to automatically scale the application based on underlying infrastructure CPU usage. Your organizational policies require you to use virtual machines directly. You need to ensure that the application scaling is operationally efficient and completed as quickly as possible. What should you do?

A. Create a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.
B. Create an instance template, and use the template in a managed instance group with autoscaling configured.
C. Create an instance template, and use the template in a managed instance group that scales up and down based on the time of day.
D. Use a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring.

:::details 풀이

### 영문 문제 해석

**문제**: "Google Cloud Platform에서 실행하려는 단일 바이너리 애플리케이션이 있습니다. 기본 인프라의 CPU 사용률을 기반으로 애플리케이션을 자동으로 확장하기로 결정했습니다. 조직 정책에 따라 가상머신을 직접 사용해야 합니다. 애플리케이션 확장이 운영 효율적이고 가능한 한 빠르게 완료되도록 해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Single binary application (단일 바이너리 애플리케이션)
-   Automatically scale based on CPU usage (CPU 사용률 기반 자동 스케일링)
-   Organizational policy requires virtual machines directly (조직 정책상 가상머신 직접 사용 필수)
-   Operationally efficient (운영 효율적)
-   Completed as quickly as possible (가능한 한 빠른 완료)

### 선택지 해석

**A. Google Kubernetes Engine 클러스터를 생성하고 horizontal pod autoscaling을 사용하여 애플리케이션 확장**

-   GKE 클러스터 사용
-   Pod 기반 수평 확장
-   Kubernetes 환경에서 컨테이너 실행
-   VM을 직접 사용하지 않음 (GKE가 VM 관리)

**B. 인스턴스 템플릿을 생성하고 자동 스케일링이 구성된 관리형 인스턴스 그룹에서 템플릿 사용**

-   Instance Template 생성
-   Managed Instance Group (MIG) 사용
-   CPU 기반 자동 스케일링 구성
-   VM 직접 사용

**C. 인스턴스 템플릿을 생성하고 시간대에 따라 확장/축소하는 관리형 인스턴스 그룹에서 템플릿 사용**

-   Instance Template 및 MIG 사용
-   시간 기반 스케일링 (CPU 기반 아님)
-   예측 가능한 패턴 기반 확장
-   실제 CPU 사용률과 무관한 스케일링

**D. 써드파티 도구를 사용하여 Stackdriver CPU 사용량 모니터링을 기반으로 애플리케이션 확장/축소 자동화 구축**

-   외부 도구 사용
-   사용자 정의 자동화 스크립트
-   Stackdriver(현재 Cloud Monitoring) 메트릭 기반
-   수동 구현 필요

### 문제 분석 및 정답 도출

#### 조직 정책 요구사항 분석

**"가상머신 직접 사용" 요구사항**

-   VM 인스턴스를 직접 관리해야 함
-   Kubernetes 같은 추상화 계층 사용 불가
-   컨테이너가 아닌 VM에서 바이너리 직접 실행

#### 각 선택지 상세 분석

**A. GKE + Horizontal Pod Autoscaling**

-   ❌ 조직 정책 위반: VM 직접 사용하지 않음
-   ❌ GKE가 VM을 추상화하여 관리
-   ✅ CPU 기반 자동 스케일링 지원
-   ✅ 빠른 스케일링 속도
-   ✅ 운영 효율적

**B. Instance Template + MIG + Autoscaling**

-   ✅ VM 직접 사용 (조직 정책 준수)
-   ✅ CPU 기반 자동 스케일링 지원
-   ✅ Google Cloud 네이티브 솔루션 (운영 효율적)
-   ✅ 빠른 스케일링 (사전 구성된 템플릿 사용)
-   ✅ 모든 요구사항 충족

**C. MIG + 시간 기반 스케일링**

-   ✅ VM 직접 사용
-   ❌ CPU 기반이 아닌 시간 기반 스케일링
-   ❌ 실제 워크로드와 무관한 스케일링
-   ❌ 요구사항 불충족

**D. 써드파티 도구 + 사용자 정의 자동화**

-   ✅ VM 직접 사용 가능
-   ✅ CPU 기반 스케일링 가능
-   ❌ 운영 비효율적 (사용자 정의 구현 필요)
-   ❌ 개발 및 유지보수 부담
-   ❌ 느린 구현 속도

#### Managed Instance Group (MIG)의 장점

**자동 스케일링 기능**

```yaml
# 자동 스케일링 정책 예시
autoscaling:
    minNumReplicas: 1
    maxNumReplicas: 10
    cpuUtilization:
        utilizationTarget: 0.6 # CPU 60% 기준
    coolDownPeriodSec: 60 # 60초 쿨다운
```

**빠른 인스턴스 프로비저닝**

-   Instance Template에 모든 구성 사전 정의
-   부팅 디스크, 네트워크, 메타데이터 등 미리 설정
-   스케일 아웃 시 템플릿 기반으로 빠른 VM 생성

**운영 효율성**

-   Google Cloud 네이티브 솔루션
-   자동화된 헬스 체크 및 복구
-   로드 밸런싱과의 통합
-   모니터링 및 로깅 자동 구성

#### Instance Template 구성 예시

```yaml
# instance-template.yaml
name: app-template
properties:
    machineType: n1-standard-2
    disks:
        - boot: true
          initializeParams:
              sourceImage: projects/debian-cloud/global/images/family/debian-11
    networkInterfaces:
        - network: global/networks/default
    metadata:
        items:
            - key: startup-script
              value: |
                  #!/bin/bash
                  # 바이너리 애플리케이션 다운로드 및 실행
                  gsutil cp gs://my-bucket/my-app ./my-app
                  chmod +x ./my-app
                  ./my-app
    tags:
        items:
            - http-server
```

#### MIG 자동 스케일링 구성

```bash
# MIG 생성
gcloud compute instance-groups managed create app-group \
  --template=app-template \
  --size=2 \
  --zone=us-central1-a

# 자동 스케일링 설정
gcloud compute instance-groups managed set-autoscaling app-group \
  --max-num-replicas=10 \
  --min-num-replicas=1 \
  --target-cpu-utilization=0.6 \
  --cool-down-period=60 \
  --zone=us-central1-a
```

#### 스케일링 속도 비교

**MIG 자동 스케일링**

```
1. CPU 사용률 60% 초과 감지
2. 스케일링 결정 (10-30초)
3. 새 인스턴스 생성 (1-2분)
4. 애플리케이션 시작 (30초-2분)
총 소요 시간: 2-5분
```

**써드파티 도구 방식**

```
1. 모니터링 데이터 수집 (1-5분)
2. 사용자 정의 로직 실행 (30초-2분)
3. API 호출로 VM 생성 (1-2분)
4. 애플리케이션 배포 및 시작 (2-5분)
총 소요 시간: 5-15분
```

#### 운영 효율성 비교

**MIG 방식 (선택지 B)**

```
✅ 설정 한 번으로 자동화 완료
✅ Google Cloud 통합 모니터링
✅ 자동 헬스 체크 및 복구
✅ 롤링 업데이트 지원
✅ 부하 분산 자동 연동
```

**사용자 정의 방식 (선택지 D)**

```
❌ 스크립트 개발 및 테스트 필요
❌ 모니터링 시스템 별도 구축
❌ 오류 처리 로직 구현 필요
❌ 유지보수 부담
❌ 보안 및 권한 관리 복잡
```

#### 조직 정책 준수

**VM 직접 사용 요구사항**

```
MIG 방식:
- 실제 Compute Engine VM 인스턴스 사용
- VM에서 바이너리 애플리케이션 직접 실행
- VM의 CPU, 메모리, 디스크 직접 관리
→ 정책 완벽 준수

GKE 방식:
- VM 위에 Kubernetes 추상화 계층 존재
- 컨테이너로 애플리케이션 실행
- VM 직접 관리하지 않음
→ 정책 위반
```

#### 정답: B. 인스턴스 템플릿을 생성하고 자동 스케일링이 구성된 관리형 인스턴스 그룹에서 템플릿 사용

**선택 이유**:

1. 조직 정책 완벽 준수: VM 직접 사용
2. CPU 기반 자동 스케일링 지원
3. 운영 효율성: Google Cloud 네이티브 솔루션
4. 빠른 스케일링: 사전 구성된 템플릿 기반 신속한 프로비저닝
5. 통합 관리: 모니터링, 로깅, 헬스체크 자동화
6. 확장성: 다중 존 및 리전 지원
7. 비용 효율성: 불필요한 써드파티 도구 비용 없음

MIG는 인스턴스 템플릿 기반으로만 VM을 생성하기 때문에 기존 VM들을 가져올 수 없다. 기존 인스턴스를 그룹화하려면 Unmanaged Instance Group (UIG) 방식을 사용해야 한다.

:::

## dump 17

You are analyzing Google Cloud Platform service costs from three separate projects. You want to use this information to create service cost estimates by service type, daily and monthly, for the next six months using standard query syntax. What should you do?

A. Export your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.
B. Export your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.
C. Export your transactions to a local file, and perform analysis with a desktop tool.
D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.

:::details 풀이

### 영문 문제 해석

**문제**: "3개의 별도 프로젝트에서 Google Cloud Platform 서비스 비용을 분석하고 있습니다. 이 정보를 사용하여 향후 6개월 동안 서비스 타입별로 일별 및 월별 서비스 비용 추정치를 표준 쿼리 구문으로 생성하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Analyzing GCP service costs from 3 projects (3개 프로젝트의 GCP 서비스 비용 분석)
-   Create service cost estimates by service type (서비스 타입별 비용 추정)
-   Daily and monthly analysis (일별 및 월별 분석)
-   Next six months forecast (향후 6개월 예측)
-   Using standard query syntax (표준 쿼리 구문 사용)

### 선택지 해석

**A. 청구서를 Cloud Storage 버킷으로 내보낸 후 분석을 위해 Cloud Bigtable로 가져오기**

-   Cloud Storage 중간 저장소 사용
-   Cloud Bigtable을 분석 도구로 활용
-   NoSQL 데이터베이스 방식

**B. 청구서를 Cloud Storage 버킷으로 내보낸 후 분석을 위해 Google Sheets로 가져오기**

-   Cloud Storage 중간 저장소 사용
-   Google Sheets 스프레드시트로 분석
-   GUI 기반 분석 도구

**C. 거래 내역을 로컬 파일로 내보내고 데스크톱 도구로 분석 수행**

-   로컬 환경으로 데이터 다운로드
-   데스크톱 분석 도구 사용 (Excel, 기타 도구)
-   오프라인 분석 방식

**D. 청구서를 BigQuery 데이터셋으로 내보낸 후 시간 윈도우 기반 SQL 쿼리로 분석 작성**

-   BigQuery 데이터 웨어하우스 활용
-   SQL 쿼리를 통한 분석
-   시계열 데이터 분석 최적화

### 문제 분석 및 정답 도출

#### 요구사항 세부 분석

**"표준 쿼리 구문 사용" 요구사항**

-   SQL과 같은 표준화된 쿼리 언어 필요
-   복잡한 데이터 조작 및 집계 기능 필요
-   재사용 가능한 쿼리 스크립트 작성

**"서비스 타입별, 일별/월별 분석" 요구사항**

-   다차원 데이터 분석 (서비스별, 시간별)
-   시계열 데이터 처리
-   집계 및 그룹화 기능

**"향후 6개월 예측" 요구사항**

-   대용량 데이터 처리
-   복잡한 통계 분석
-   시계열 예측 모델링

#### 각 선택지 상세 분석

**A. Cloud Bigtable**

-   ❌ NoSQL 데이터베이스로 복잡한 집계 쿼리에 부적합
-   ❌ 표준 SQL 쿼리 지원하지 않음
-   ❌ 비용 분석보다는 실시간 애플리케이션에 특화
-   ❌ 시계열 분석 도구로서 제한적

**B. Google Sheets**

-   ❌ 대용량 데이터 처리 한계 (100만 행 제한)
-   ❌ 복잡한 SQL 쿼리 지원 안함
-   ❌ 3개 프로젝트 데이터 통합 분석 어려움
-   ❌ 자동화 및 스케일링 제한

**C. 로컬 데스크톱 도구**

-   ❌ 클라우드 환경과 분리되어 실시간 업데이트 어려움
-   ❌ 대용량 데이터 다운로드 시간 및 저장 공간 문제
-   ❌ 협업 및 공유 어려움
-   ❌ 자동화 제한적

**D. BigQuery + SQL**

-   ✅ 표준 SQL 쿼리 완벽 지원
-   ✅ 페타바이트 규모 데이터 처리 가능
-   ✅ 시계열 데이터 분석에 최적화
-   ✅ 복잡한 집계 및 분석 함수 제공
-   ✅ 자동화 및 스케줄링 지원

#### BigQuery를 통한 청구 데이터 분석

**청구 데이터 내보내기 설정**

```bash
# BigQuery로 청구 데이터 자동 내보내기 설정
gcloud beta billing accounts get-iam-policy ACCOUNT_ID
gcloud beta billing export create \
  --billing-account=ACCOUNT_ID \
  --dataset=PROJECT_ID:billing_dataset
```

**표준 SQL을 통한 분석 쿼리 예시**

**서비스별 일별 비용 분석**

```sql
-- 서비스별 일별 비용 추이
SELECT
  service.description AS service_name,
  usage_start_time AS usage_date,
  SUM(cost) AS daily_cost,
  project.id AS project_id
FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
GROUP BY service_name, usage_date, project_id
ORDER BY usage_date DESC, daily_cost DESC
```

**월별 비용 집계 및 예측**

```sql
-- 월별 서비스 비용 집계 및 트렌드 분석
WITH monthly_costs AS (
  SELECT
    service.description AS service_name,
    EXTRACT(YEAR FROM usage_start_time) AS year,
    EXTRACT(MONTH FROM usage_start_time) AS month,
    SUM(cost) AS monthly_cost
  FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
  WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)
  GROUP BY service_name, year, month
),
trend_analysis AS (
  SELECT
    service_name,
    AVG(monthly_cost) AS avg_monthly_cost,
    STDDEV(monthly_cost) AS cost_stddev,
    -- 선형 회귀를 통한 트렌드 계산
    CORR(month, monthly_cost) AS cost_trend
  FROM monthly_costs
  GROUP BY service_name
)
SELECT
  service_name,
  avg_monthly_cost,
  -- 향후 6개월 예상 비용
  avg_monthly_cost * (1 + cost_trend * 0.1) AS projected_cost_6m
FROM trend_analysis
ORDER BY avg_monthly_cost DESC
```

**3개 프로젝트 통합 분석**

```sql
-- 프로젝트별 서비스 비용 비교
SELECT
  project.id AS project_name,
  service.description AS service_name,
  DATE_TRUNC(usage_start_time, MONTH) AS month,
  SUM(cost) AS monthly_cost,
  -- 프로젝트 간 비용 비율
  SUM(cost) / SUM(SUM(cost)) OVER (PARTITION BY DATE_TRUNC(usage_start_time, MONTH)) AS cost_ratio
FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
GROUP BY project_name, service_name, month
ORDER BY month DESC, monthly_cost DESC
```

#### BigQuery의 고급 분석 기능

**시계열 예측 함수**

```sql
-- BigQuery ML을 활용한 비용 예측
CREATE MODEL `project.billing_dataset.cost_forecast_model`
OPTIONS (
  model_type='ARIMA_PLUS',
  time_series_timestamp_col='usage_date',
  time_series_data_col='daily_cost',
  time_series_id_col='service_name'
) AS
SELECT
  DATE(usage_start_time) AS usage_date,
  service.description AS service_name,
  SUM(cost) AS daily_cost
FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)
GROUP BY usage_date, service_name
```

**예측 결과 조회**

```sql
-- 향후 6개월 비용 예측
SELECT *
FROM ML.FORECAST(
  MODEL `project.billing_dataset.cost_forecast_model`,
  STRUCT(180 AS horizon, 0.8 AS confidence_level)
)
ORDER BY service_name, forecast_timestamp
```

##### 다른 선택지들의 한계

**Cloud Bigtable 한계**

```
- SQL 지원 안함 → 표준 쿼리 구문 요구사항 불충족
- 스키마리스 NoSQL → 구조화된 청구 데이터 분석에 부적합
- 집계 함수 제한 → 복잡한 비용 분석 어려움
```

**Google Sheets 한계**

```
- 행 수 제한 → 대용량 청구 데이터 처리 불가
- 고급 SQL 함수 없음 → 시계열 분석 제한
- 자동화 어려움 → 정기적 분석 및 예측 어려움
```

**로컬 도구 한계**

```
- 클라우드 통합 부족 → 실시간 데이터 업데이트 어려움
- 협업 제한 → 팀 단위 분석 어려움
- 스케일링 한계 → 대용량 데이터 처리 제약
```

#### BigQuery 사용의 추가 이점

**비용 효율성**

-   쿼리당 과금으로 분석 비용 최적화
-   자동 스케일링으로 리소스 효율성

**통합 및 확장성**

-   다른 Google Cloud 서비스와 완벽 통합
-   실시간 데이터 스트리밍 지원
-   BigQuery ML을 통한 고급 예측 분석

**표준화 및 재사용성**

-   표준 SQL 99 지원
-   쿼리 스케줄링 및 자동화
-   대시보드 및 리포팅 도구 연동

#### 정답: D. 청구서를 BigQuery 데이터셋으로 내보낸 후 시간 윈도우 기반 SQL 쿼리로 분석 작성

**선택 이유**:

1. 표준 SQL 쿼리 구문 완벽 지원으로 요구사항 충족
2. 대용량 시계열 데이터 분석에 최적화된 아키텍처
3. 서비스별, 일별/월별 다차원 분석 기능 제공
4. 복잡한 집계 및 예측 분석 함수 지원
5. 3개 프로젝트 데이터 통합 분석 용이
6. 자동화 및 스케줄링을 통한 정기적 분석 가능
7. BigQuery ML을 활용한 향후 6개월 비용 예측 지원
8. Google Cloud 네이티브 통합으로 실시간 데이터 업데이트

:::

## dump 18

You need to set up a policy so that videos stored in a specific Cloud Storage Regional bucket are moved to Coldline after 90 days, and then deleted after one year from their creation. How should you set up the policy?

A. Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 275 days (365 ג€" 90)
B. Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 365 days.
C. Use gsutil rewrite and set the Delete action to 275 days (365-90).
D. Use gsutil rewrite and set the Delete action to 365 days.

:::details 풀이

### 영문 문제 해석

**문제**: "특정 Cloud Storage Regional 버킷에 저장된 비디오가 90일 후 Coldline으로 이동되고, 생성 후 1년 후에 삭제되도록 정책을 설정해야 합니다. 정책을 어떻게 설정해야 할까요?"

**핵심 요구사항**:

-   Videos in Cloud Storage Regional bucket (Regional 버킷의 비디오 파일)
-   Move to Coldline after 90 days (90일 후 Coldline으로 이동)
-   Delete after one year from creation (생성 후 1년 후 삭제)
-   Set up policy (정책 설정)

### 선택지 해석

**A. Age 조건으로 Cloud Storage Object Lifecycle Management를 사용하여 SetStorageClass와 Delete 액션 설정. SetStorageClass 액션을 90일로, Delete 액션을 275일(365-90)로 설정**

-   Object Lifecycle Management 사용
-   90일: Regional → Coldline 이동
-   275일: 삭제 (총 365일 - 90일 = 275일)
-   상대적 날짜 계산 방식

**B. Age 조건으로 Cloud Storage Object Lifecycle Management를 사용하여 SetStorageClass와 Delete 액션 설정. SetStorageClass 액션을 90일로, Delete 액션을 365일로 설정**

-   Object Lifecycle Management 사용
-   90일: Regional → Coldline 이동
-   365일: 삭제 (생성일 기준 절대 날짜)
-   절대적 날짜 계산 방식

**C. gsutil rewrite를 사용하고 Delete 액션을 275일(365-90)로 설정**

-   gsutil rewrite 명령어 사용 (수동 도구)
-   275일 후 삭제 설정
-   라이프사이클 정책이 아닌 일회성 작업

**D. gsutil rewrite를 사용하고 Delete 액션을 365일로 설정**

-   gsutil rewrite 명령어 사용 (수동 도구)
-   365일 후 삭제 설정
-   라이프사이클 정책이 아닌 일회성 작업

### 문제 분석 및 정답 도출

#### Cloud Storage Object Lifecycle Management 이해

**라이프사이클 정책의 작동 방식**

-   모든 조건과 액션은 **객체 생성일(Creation Date) 기준**으로 계산
-   Age 조건: 객체가 생성된 후 경과된 일수
-   각 액션은 독립적으로 생성일부터 계산

**Age 조건의 정확한 의미**

```json
{
    "lifecycle": {
        "rule": [
            {
                "condition": { "age": 90 },
                "action": {
                    "type": "SetStorageClass",
                    "storageClass": "COLDLINE"
                }
            },
            {
                "condition": { "age": 365 },
                "action": { "type": "Delete" }
            }
        ]
    }
}
```

#### 각 선택지 상세 분석

**A. SetStorageClass: 90일, Delete: 275일**

```
타임라인:
Day 0: 객체 생성 (Regional)
Day 90: Coldline으로 이동 ✓
Day 275: 삭제 실행 ❌ (너무 이름)

문제점:
- 생성 후 275일에 삭제 = 약 9개월 후 삭제
- 요구사항인 "1년 후 삭제"와 불일치
- 잘못된 계산 방식 적용
```

**B. SetStorageClass: 90일, Delete: 365일**

```
타임라인:
Day 0: 객체 생성 (Regional)
Day 90: Coldline으로 이동 ✓
Day 365: 삭제 실행 ✓ (정확히 1년 후)

결과:
- 90일 후 Coldline 이동: 요구사항 충족
- 365일(1년) 후 삭제: 요구사항 충족
- 올바른 절대 날짜 계산 방식
```

**C. gsutil rewrite + Delete: 275일**

```
문제점:
- gsutil rewrite는 일회성 수동 명령어
- 자동 라이프사이클 정책 아님
- 새로 업로드되는 객체에 자동 적용 안됨
- 275일 계산도 잘못됨
```

**D. gsutil rewrite + Delete: 365일**

```
문제점:
- gsutil rewrite는 라이프사이클 정책이 아님
- Storage Class 변경 기능 누락
- 수동 개입 필요
- 자동화 불가능
```

#### Object Lifecycle Management vs gsutil rewrite

**Object Lifecycle Management (올바른 접근)**

```json
{
    "lifecycle": {
        "rule": [
            {
                "condition": { "age": 90 },
                "action": {
                    "type": "SetStorageClass",
                    "storageClass": "COLDLINE"
                }
            },
            {
                "condition": { "age": 365 },
                "action": { "type": "Delete" }
            }
        ]
    }
}
```

**장점:**

-   자동화된 정책 실행
-   새로운 객체에 자동 적용
-   지속적인 관리 불필요
-   다양한 조건과 액션 지원

**gsutil rewrite (부적절한 도구)**

```bash
# 일회성 명령어 (자동화 안됨)
gsutil rewrite -s COLDLINE gs://bucket-name/*
gsutil lifecycle set lifecycle.json gs://bucket-name
```

**단점:**

-   수동 실행 필요
-   기존 객체에만 적용
-   스토리지 클래스 변경과 삭제를 동시에 처리 어려움
-   지속적인 정책 적용 불가능

#### Age 조건 계산 방식의 오해

**잘못된 이해 (선택지 A의 오류)**

```
생각: "90일 후 Coldline 이동, 그 후 275일 더 지나서 삭제"
Day 0: 생성
Day 90: Coldline 이동
Day 90+275=365: 삭제

실제 Cloud Storage 동작:
❌ 이런 방식으로 작동하지 않음
```

**올바른 이해 (선택지 B)**

```
모든 조건은 생성일 기준:
Day 0: 객체 생성
Day 90: age=90 조건 만족 → Coldline 이동
Day 365: age=365 조건 만족 → 삭제

각 액션은 독립적으로 생성일부터 계산됨
```

#### 실제 라이프사이클 정책 설정

**JSON 구성 파일**

```json
{
    "lifecycle": {
        "rule": [
            {
                "condition": {
                    "age": 90
                },
                "action": {
                    "type": "SetStorageClass",
                    "storageClass": "COLDLINE"
                }
            },
            {
                "condition": {
                    "age": 365
                },
                "action": {
                    "type": "Delete"
                }
            }
        ]
    }
}
```

**정책 적용 명령어**

```bash
# 라이프사이클 정책 적용
gsutil lifecycle set lifecycle.json gs://my-video-bucket

# 정책 확인
gsutil lifecycle get gs://my-video-bucket
```

#### 라이프사이클 정책의 실행 과정

**Cloud Storage의 자동 실행**

```
매일 자동 스캔:
1. 버킷 내 모든 객체의 age 계산
2. 조건 만족하는 객체 식별
3. 해당 액션 자동 실행
4. 로그 기록 및 모니터링

사용자 개입 불필요!
```

#### 비용 최적화 효과

**Regional → Coldline → Delete**

```
비용 변화:
- Regional Storage: $0.020/GB/month
- Coldline Storage: $0.004/GB/month
- 90일 후: 80% 스토리지 비용 절약
- 365일 후: 100% 비용 절약 (삭제)

대용량 비디오 파일의 경우 상당한 비용 절약
```

#### 다른 Storage Class 옵션

**왜 Coldline인가?**

```
Nearline: 월 1회 미만 접근 (30일 최소 보관)
Coldline: 분기 1회 미만 접근 (90일 최소 보관) ✓
Archive: 연 1회 미만 접근 (365일 최소 보관)

90일 후 이동이므로 Coldline이 적절
```

#### 정답: B. Age 조건으로 Cloud Storage Object Lifecycle Management를 사용하여 SetStorageClass와 Delete 액션 설정. SetStorageClass 액션을 90일로, Delete 액션을 365일로 설정

**선택 이유**:

1. Object Lifecycle Management는 자동화된 정책 관리의 표준 방법
2. Age 조건은 모든 액션이 생성일 기준으로 독립적으로 계산됨
3. 90일: Regional → Coldline 이동 요구사항 정확히 충족
4. 365일: 생성 후 정확히 1년 후 삭제 요구사항 충족
5. 새로운 객체에 자동으로 적용되는 지속적 정책
6. 수동 개입 없이 완전 자동화된 라이프사이클 관리
7. 비용 최적화와 스토리지 관리 효율성 제공

`gsutil rewrite` 명령어는 기존 클라우드 스토리지 객체 메타데이터나 암호화 설정을 변경하는 명령어이다.

:::

## dump 19

You have a Linux VM that must connect to Cloud SQL. You created a service account with the appropriate access rights. You want to make sure that the VM uses this service account instead of the default Compute Engine service account. What should you do?

A. When creating the VM via the web console, specify the service account under the 'Identity and API Access' section.
B. Download a JSON Private Key for the service account. On the Project Metadata, add that JSON as the value for the key compute-engine-service- account.
C. Download a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-engine- service-account.
D. Download a JSON Private Key for the service account. After creating the VM, ssh into the VM and save the JSON under ~/.gcloud/compute-engine-service- account.json.

:::details 풀이

### 영문 문제 해석

**문제**: "Cloud SQL에 연결해야 하는 Linux VM이 있습니다. 적절한 액세스 권한을 가진 서비스 계정을 생성했습니다. VM이 기본 Compute Engine 서비스 계정 대신 이 서비스 계정을 사용하도록 하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Linux VM that must connect to Cloud SQL (Cloud SQL에 연결할 Linux VM)
-   Service account with appropriate access rights (적절한 권한을 가진 서비스 계정)
-   VM uses this service account instead of default (기본 대신 특정 서비스 계정 사용)
-   Proper configuration method (올바른 설정 방법)

### 선택지 해석

**A. 웹 콘솔을 통해 VM 생성 시 'Identity and API Access' 섹션에서 서비스 계정 지정**

-   VM 생성 시점에서 서비스 계정 설정
-   Google Cloud Console UI를 통한 설정
-   공식적인 VM 구성 방법

**B. 서비스 계정의 JSON Private Key를 다운로드하여 Project Metadata에 compute-engine-service-account 키 값으로 추가**

-   JSON 키 파일 다운로드 방식
-   프로젝트 수준 메타데이터 설정
-   전체 프로젝트에 영향

**C. 서비스 계정의 JSON Private Key를 다운로드하여 VM의 Custom Metadata에 compute-engine-service-account 키 값으로 추가**

-   JSON 키 파일 다운로드 방식
-   VM별 커스텀 메타데이터 설정
-   특정 VM에만 영향

**D. 서비스 계정의 JSON Private Key를 다운로드하여 VM 생성 후 ssh로 접속하여 ~/.gcloud/compute-engine-service-account.json에 저장**

-   JSON 키 파일 다운로드 방식
-   수동으로 VM 내부에 파일 저장
-   파일 시스템 기반 설정

### 문제 분석 및 정답 도출

#### Google Cloud VM 서비스 계정 설정 방식

**공식적인 방법: VM 생성 시 서비스 계정 할당**

```
VM 생성 과정:
1. Compute Engine → VM instances → Create instance
2. Identity and API Access 섹션
3. Service account 드롭다운에서 원하는 서비스 계정 선택
4. Access scopes 설정
```

**Google Cloud의 권장 아키텍처**

-   VM과 서비스 계정의 바인딩은 VM 메타데이터 수준에서 관리
-   JSON 키 파일 다운로드는 보안상 권장하지 않음
-   네이티브 IAM 통합 방식 사용

#### 각 선택지 상세 분석

**A. VM 생성 시 Identity and API Access에서 설정**

-   ✅ Google Cloud의 공식 권장 방법
-   ✅ 보안이 가장 우수 (키 파일 노출 없음)
-   ✅ VM 메타데이터 서버를 통한 자동 인증
-   ✅ 키 로테이션 자동 처리
-   ✅ IAM과 완전 통합

**B. Project Metadata에 JSON 키 저장**

-   ❌ 비공식적인 방법
-   ❌ JSON 키 파일 노출 위험
-   ❌ 프로젝트 전체에 영향 (과도한 범위)
-   ❌ 키 관리 복잡성
-   ❌ compute-engine-service-account는 실제 메타데이터 키가 아님

**C. VM Custom Metadata에 JSON 키 저장**

-   ❌ 비공식적인 방법
-   ❌ JSON 키 파일 보안 위험
-   ❌ 메타데이터를 통한 키 노출 가능성
-   ❌ 수동 키 관리 필요
-   ❌ compute-engine-service-account는 유효한 메타데이터 키가 아님

**D. VM 내부 파일 시스템에 JSON 키 저장**

-   ❌ 매우 비보안적인 방법
-   ❌ 파일 시스템 접근으로 키 노출 위험
-   ❌ 키 로테이션 수동 처리 필요
-   ❌ ~/.gcloud/ 경로는 올바른 위치가 아님
-   ❌ VM 재시작 시 설정 유지 문제

#### Google Cloud 서비스 계정 인증 메커니즘

**메타데이터 서버를 통한 인증 (권장 방법)**

```bash
# VM 내에서 자동으로 동작하는 방식
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# 애플리케이션에서 자동 인증
gcloud auth application-default print-access-token
```

**JSON 키 파일 방식 (권장하지 않음)**

```bash
# 수동으로 설정해야 하는 방식
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"
gcloud auth activate-service-account --key-file=/path/to/key.json
```

#### VM 생성 시 서비스 계정 설정 과정

**Google Cloud Console에서**

```
1. Compute Engine → VM instances
2. CREATE INSTANCE 클릭
3. Identity and API access 섹션 확장
4. Service account 드롭다운:
   - Compute Engine default service account
   - [사용자 정의 서비스 계정] ← 여기서 선택
5. Access scopes:
   - Allow default access
   - Allow full access to all Cloud APIs
   - Set access for each API (권장)
```

**gcloud 명령어로**

```bash
gcloud compute instances create my-vm \
  --service-account=my-service-account@project.iam.gserviceaccount.com \
  --scopes=https://www.googleapis.com/auth/sqlservice.admin
```

#### Cloud SQL 연결을 위한 권한 설정

**필요한 IAM 역할**

```
서비스 계정에 부여해야 할 권한:
- Cloud SQL Client (roles/cloudsql.client)
- 또는 Cloud SQL Editor (roles/cloudsql.editor)
```

**액세스 스코프 설정**

```
VM의 액세스 스코프:
- https://www.googleapis.com/auth/sqlservice.admin
- 또는 "Allow full access to all Cloud APIs"
```

#### 보안 모범 사례

**권장 사항 (선택지 A)**

```
✅ VM 생성 시 서비스 계정 할당
✅ 최소 권한 원칙 적용
✅ 액세스 스코프 세밀하게 설정
✅ JSON 키 파일 사용 피하기
✅ IAM 조건부 액세스 활용
```

**지양해야 할 방식 (선택지 B, C, D)**

```
❌ JSON 키 파일 다운로드
❌ 키 파일을 메타데이터나 파일시스템에 저장
❌ 수동 키 관리
❌ 과도한 권한 부여
❌ 키 하드코딩
```

#### 실제 구현 예시

**올바른 방법 (선택지 A)**

```bash
# VM 생성 시 서비스 계정 지정
gcloud compute instances create sql-client-vm \
  --service-account=cloudsql-client@my-project.iam.gserviceaccount.com \
  --scopes=https://www.googleapis.com/auth/sqlservice.admin \
  --zone=us-central1-a

# VM 내에서 자동 인증 확인
gcloud auth list
# * cloudsql-client@my-project.iam.gserviceaccount.com

# Cloud SQL 연결 테스트
gcloud sql instances list
```

#### VM 생성 후 서비스 계정 변경

**기존 VM의 서비스 계정 변경**

```bash
# VM 중지 필요
gcloud compute instances stop my-vm --zone=us-central1-a

# 서비스 계정 변경
gcloud compute instances set-service-account my-vm \
  --service-account=new-service-account@project.iam.gserviceaccount.com \
  --scopes=https://www.googleapis.com/auth/sqlservice.admin \
  --zone=us-central1-a

# VM 시작
gcloud compute instances start my-vm --zone=us-central1-a
```

#### Cloud SQL 연결 확인

**연결 테스트**

```bash
# VM 내에서 Cloud SQL 연결 확인
gcloud sql connect my-instance --user=root

# 또는 직접 연결
mysql -h [INSTANCE_IP] -u root -p
```

#### 정답: A. 웹 콘솔을 통해 VM 생성 시 'Identity and API Access' 섹션에서 서비스 계정 지정

**선택 이유**:

1. Google Cloud의 공식 권장 방법으로 가장 안전하고 표준적
2. JSON 키 파일 다운로드 없이 네이티브 IAM 통합 활용
3. 메타데이터 서버를 통한 자동 인증으로 보안성 극대화
4. 키 로테이션 및 관리가 Google에 의해 자동 처리
5. VM과 서비스 계정 간의 바인딩이 플랫폼 수준에서 관리
6. Cloud SQL 연결에 필요한 권한을 안전하게 제공
7. 확장성과 유지보수성이 뛰어난 솔루션

:::

## dump 21

Your company has a 3-tier solution running on Compute Engine. The configuration of the current infrastructure is shown below. Each tier has a service account that is associated with all instances within it. You need to enable communication on TCP port 8080 between tiers as follows: Instances in tier #1 must communicate with tier #2. Instances in tier #2 must communicate with tier #3. What should you do?

![gcp](../.vuepress/assets/gcp/250911-1.png)

1.  Create an ingress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.2.0/24). Protocols: allow all. 2. Create an ingress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.1.0/24). Protocols: allow all.
1.  Create an ingress firewall rule with the following settings: Targets: all instances with tier #2 service account. Source filter: all instances with tier #1 service account. Protocols: allow TCP: 8080. 2. Create an ingress firewall rule with the following settings: Targets: all instances with tier #3 service account. Source filter: all instances with tier #2 service account. Protocols: allow TCP: 8080.
1.  Create an ingress firewall rule with the following settings: Targets: all instances with tier #2 service account. Source filter: all instances with tier #1 service account. Protocols: allow all. 2. Create an ingress firewall rule with the following settings: Targets: all instances with tier #3 service account. Source filter: all instances with tier #2 service account. Protocols: allow all.
1.  Create an egress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.2.0/24). Protocols: allow TCP: 8080. 2. Create an egress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.1.0/24). Protocols: allow TCP: 8080.

:::details 풀이

### 영문 문제 해석

**문제**: "회사에 Compute Engine에서 실행되는 3-tier 솔루션이 있습니다. 현재 인프라의 구성은 아래와 같습니다. 각 tier에는 해당 tier 내의 모든 인스턴스와 연관된 서비스 계정이 있습니다. 다음과 같이 tier 간에 TCP 포트 8080에서 통신을 활성화해야 합니다: Tier #1의 인스턴스는 tier #2와 통신해야 합니다. Tier #2의 인스턴스는 tier #3과 통신해야 합니다. 어떻게 해야 할까요?"

**네트워크 구성**:

-   VPC 내 3개 서브넷
-   Subnet Tier #1: 10.0.1.0/24
-   Subnet Tier #2: 10.0.2.0/24
-   Subnet Tier #3: 10.0.3.0/24
-   각 tier별 전용 서비스 계정

**통신 요구사항**:

-   Tier #1 → Tier #2 (TCP 8080)
-   Tier #2 → Tier #3 (TCP 8080)

### 선택지 해석

**선택지 1**:

1. 모든 인스턴스를 대상으로 하고 소스를 10.0.2.0/24로 하는 ingress 규칙 (모든 프로토콜 허용)
2. 모든 인스턴스를 대상으로 하고 소스를 10.0.1.0/24로 하는 ingress 규칙 (모든 프로토콜 허용)

**선택지 2**:

1. Tier #2 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #1 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)
2. Tier #3 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #2 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)

**선택지 3**:

1. Tier #2 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #1 서비스 계정으로 하는 ingress 규칙 (모든 프로토콜 허용)
2. Tier #3 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #2 서비스 계정으로 하는 ingress 규칙 (모든 프로토콜 허용)

**선택지 4**:

1. 모든 인스턴스를 대상으로 하고 소스를 10.0.2.0/24로 하는 egress 규칙 (TCP 8080 허용)
2. 모든 인스턴스를 대상으로 하고 소스를 10.0.1.0/24로 하는 egress 규칙 (TCP 8080 허용)

### 문제 분석 및 정답 도출

#### 통신 흐름 분석

**요구되는 통신**

```
Tier #1 (10.0.1.0/24) → Tier #2 (10.0.2.0/24) : TCP 8080
Tier #2 (10.0.2.0/24) → Tier #3 (10.0.3.0/24) : TCP 8080
```

**방화벽 규칙 관점**

```
Tier #2가 Tier #1으로부터 ingress 트래픽 수신
Tier #3이 Tier #2로부터 ingress 트래픽 수신
```

#### Google Cloud 방화벽 규칙 이해

**Ingress vs Egress**

-   **Ingress**: 외부→내부로 들어오는 트래픽
-   **Egress**: 내부→외부로 나가는 트래픽

**서비스 계정 기반 타겟팅**

-   서비스 계정으로 인스턴스 그룹 식별 가능
-   IP 범위보다 더 세밀하고 안전한 제어
-   인스턴스 추가/제거 시 자동으로 적용

#### 각 선택지 상세 분석

**선택지 1 분석**

```
문제점:
❌ 너무 광범위한 타겟 (모든 인스턴스)
❌ 모든 프로토콜 허용 (과도한 권한)
❌ 잘못된 IP 범위 매핑
❌ 보안 원칙 위배 (최소 권한 원칙)

규칙 1: 모든 인스턴스가 10.0.2.0/24에서 모든 트래픽 수신
규칙 2: 모든 인스턴스가 10.0.1.0/24에서 모든 트래픽 수신
→ 의도한 방향과 다름
```

**선택지 2 분석** ✅

```
장점:
✅ 정확한 서비스 계정 기반 타겟팅
✅ 특정 포트만 허용 (TCP 8080)
✅ 최소 권한 원칙 준수
✅ 통신 방향 정확

규칙 1: Tier #2 ← Tier #1 (TCP 8080)
규칙 2: Tier #3 ← Tier #2 (TCP 8080)
→ 요구사항과 정확히 일치
```

**선택지 3 분석**

```
문제점:
✅ 올바른 서비스 계정 타겟팅
❌ 모든 프로토콜 허용 (과도한 권한)
❌ 보안 위험 증가
❌ 요구사항은 TCP 8080만 필요

규칙은 올바르지만 불필요한 프로토콜까지 허용
```

**선택지 4 분석**

```
문제점:
❌ Egress 규칙 사용 (잘못된 방향)
❌ Source filter를 IP 범위로 설정 (egress에서는 destination)
❌ 잘못된 개념 적용
❌ egress 규칙의 source filter는 의미 없음
```

#### 올바른 방화벽 규칙 설계

**규칙 1: Tier #1 → Tier #2 통신 허용**

```bash
gcloud compute firewall-rules create tier1-to-tier2 \
  --direction=INGRESS \
  --action=ALLOW \
  --target-service-accounts=tier2-service-account@project.iam.gserviceaccount.com \
  --source-service-accounts=tier1-service-account@project.iam.gserviceaccount.com \
  --rules=tcp:8080
```

**규칙 2: Tier #2 → Tier #3 통신 허용**

```bash
gcloud compute firewall-rules create tier2-to-tier3 \
  --direction=INGRESS \
  --action=ALLOW \
  --target-service-accounts=tier3-service-account@project.iam.gserviceaccount.com \
  --source-service-accounts=tier2-service-account@project.iam.gserviceaccount.com \
  --rules=tcp:8080
```

#### 서비스 계정 vs IP 범위 비교

**서비스 계정 기반 (선택지 2)**

```
장점:
✅ 동적 인스턴스 관리 (인스턴스 추가/제거 자동 적용)
✅ 세밀한 접근 제어
✅ IP 변경에 영향받지 않음
✅ 보안 모범 사례

예시: 인스턴스가 다른 서브넷으로 이동해도 서비스 계정은 유지
```

**IP 범위 기반 (선택지 1)**

```
단점:
❌ 정적 구성 (IP 변경 시 규칙 수정 필요)
❌ 서브넷 전체에 적용 (불필요한 권한)
❌ 관리 복잡성 증가
❌ 스케일링 시 유연성 부족
```

#### 보안 원칙 적용

**최소 권한 원칙**

```
✅ 필요한 포트만 허용 (TCP 8080)
✅ 특정 서비스 계정 간에만 허용
✅ 명시적 허용 정책
❌ 모든 프로토콜 허용 금지
```

**심층 방어**

```
네트워크 레벨: 방화벽 규칙
애플리케이션 레벨: 인증/권한 부여
인스턴스 레벨: OS 방화벽, 서비스 구성
```

#### 실제 적용 시나리오

**3-Tier 애플리케이션 예시**

```
Tier #1: Web Server (Apache/Nginx)
├── 사용자 요청 수신
├── Tier #2로 API 호출 (TCP 8080)
└── 결과를 사용자에게 반환

Tier #2: Application Server (Tomcat/Node.js)
├── Tier #1에서 요청 수신 (TCP 8080)
├── 비즈니스 로직 처리
├── Tier #3으로 데이터 요청 (TCP 8080)
└── 결과를 Tier #1으로 반환

Tier #3: Database Server (MySQL/PostgreSQL)
├── Tier #2에서 요청 수신 (TCP 8080)
├── 데이터 처리 및 응답
└── 결과를 Tier #2로 반환
```

#### 트러블슈팅 관점

**방화벽 규칙 테스트**

```bash
# 연결성 테스트
gcloud compute instances list --filter="service-account:tier1-sa"
gcloud compute instances list --filter="service-account:tier2-sa"

# 포트 연결 테스트
telnet `<tier2-internal-ip>` 8080
nc -zv `<tier3-internal-ip>` 8080
```

**로그 모니터링**

```bash
# 방화벽 로그 확인
gcloud logging read "resource.type=gce_subnetwork AND jsonPayload.rule_details.action=ALLOW"
```

#### 정답: 선택지 2

**규칙 1**: Tier #2 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #1 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)
**규칙 2**: Tier #3 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #2 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)

**선택 이유**:

1. 정확한 통신 방향과 일치 (ingress 규칙)
2. 서비스 계정 기반의 세밀한 접근 제어
3. 최소 권한 원칙 준수 (TCP 8080만 허용)
4. 확장성과 유지보수성 우수
5. 보안 모범 사례 적용
6. 동적 인스턴스 관리 지원
7. 요구사항과 정확히 일치하는 구성

ingress는 트래픽을 받는 규칙, egress는 트래픽을 내보내는 규칙이다.

:::

## dump 22

You significantly changed a complex Deployment Manager template and want to confirm that the dependencies of all defined resources are properly met before committing it to the project. You want the most rapid feedback on your changes. What should you do?

A. Use granular logging statements within a Deployment Manager template authored in Python.
B. Monitor activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console.
C. Execute the Deployment Manager template against a separate project with the same configuration, and monitor for failures.
D. Execute the Deployment Manager template using the C-preview option in the same project, and observe the state of interdependent resources.

:::details 풀이

### 영문 문제 해석

**문제**: "복잡한 Deployment Manager 템플릿을 크게 변경했으며, 프로젝트에 커밋하기 전에 정의된 모든 리소스의 종속성이 제대로 충족되는지 확인하고 싶습니다. 변경 사항에 대한 가장 빠른 피드백을 원합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Complex Deployment Manager template with significant changes (크게 변경된 복잡한 템플릿)
-   Confirm dependencies are properly met (종속성이 제대로 충족되는지 확인)
-   Before committing to project (프로젝트에 커밋하기 전)
-   Most rapid feedback (가장 빠른 피드백)

### 선택지 해석

**A. Python으로 작성된 Deployment Manager 템플릿 내에서 세분화된 로깅 구문 사용**

-   Python 템플릿에 로깅 코드 추가
-   실행 중 상세한 로그 출력
-   디버깅 목적의 로깅 추가

**B. GCP Console의 Stackdriver Logging 페이지에서 Deployment Manager 실행 활동 모니터링**

-   실제 배포 실행 후 로그 모니터링
-   Stackdriver를 통한 사후 분석
-   실시간 실행 로그 확인

**C. 동일한 구성을 가진 별도 프로젝트에 대해 Deployment Manager 템플릿을 실행하고 실패 모니터링**

-   테스트 프로젝트에서 실제 배포 실행
-   실제 리소스 생성을 통한 검증
-   실패 발생 시 분석

**D. 같은 프로젝트에서 --preview 옵션을 사용하여 Deployment Manager 템플릿을 실행하고 상호 종속적인 리소스의 상태 관찰**

-   Preview 모드로 dry run 실행
-   실제 리소스 생성 없이 검증
-   종속성 및 구성 오류 사전 확인

### 문제 분석 및 정답 도출

#### Deployment Manager Preview Mode 이해

**--preview 옵션의 특징**

```bash
gcloud deployment-manager deployments create my-deployment \
  --config=template.yaml \
  --preview

# 또는 기존 배포 업데이트 시
gcloud deployment-manager deployments update my-deployment \
  --config=new-template.yaml \
  --preview
```

**Preview Mode 동작 방식**

-   실제 리소스를 생성하지 않음
-   템플릿 구문 검증
-   종속성 그래프 생성 및 검증
-   API 호출 시뮬레이션 (실제 생성 없이)
-   리소스 간 종속성 확인

#### 각 선택지 상세 분석

**A. Python 템플릿에 로깅 추가**

```python
# 예시: 로깅 추가
def GenerateConfig(context):
    print("Generating resources...")  # 로깅 추가
    resources = []
    # 템플릿 로직
    return {'resources': resources}
```

**문제점:**

-   ❌ 실제 실행해야 로그 확인 가능
-   ❌ 종속성 검증과 직접적 관련 없음
-   ❌ 빠른 피드백 제공 어려움
-   ❌ 로깅만으로는 종속성 오류 사전 발견 불가

**B. Stackdriver Logging 모니터링**

```
장점: 상세한 실행 로그 확인 가능
단점:
❌ 실제 배포 실행 후에만 확인 가능
❌ 실패 시 리소스 정리 필요
❌ 사후 분석 방식 (사전 검증 아님)
❌ 빠른 피드백 제공 어려움
```

**C. 별도 프로젝트에서 실제 실행**

```
장점: 완전한 실제 환경 테스트
단점:
❌ 별도 프로젝트 설정 필요 (시간 소요)
❌ 실제 리소스 생성으로 인한 비용 발생
❌ 실행 시간이 오래 걸림
❌ 실패 시 리소스 정리 복잡
❌ "가장 빠른 피드백"과 상반됨
```

**D. Preview 옵션 사용** ✅

```
장점:
✅ 실제 리소스 생성 없이 검증
✅ 종속성 그래프 사전 확인
✅ 템플릿 구문 오류 즉시 발견
✅ API 호출 시뮬레이션
✅ 몇 초 내 빠른 피드백
✅ 비용 발생 없음
✅ 안전한 검증 방법
```

#### Preview Mode의 구체적인 검증 내용

**템플릿 구문 검증**

```yaml
# 잘못된 구문 예시
resources:
- name: vm-instance
  type: compute.v1.instance
  properties:
    zone: us-central1-a
    machineType: invalid-machine-type  # 오류

# Preview 실행 시 즉시 오류 발견
ERROR: Invalid machine type: invalid-machine-type
```

**종속성 검증**

```yaml
# 종속성 오류 예시
resources:
- name: vm-instance
  type: compute.v1.instance
  properties:
    networkInterfaces:
    - network: $(ref.my-network.selfLink)  # my-network가 정의되지 않음

# Preview 실행 시 종속성 오류 발견
ERROR: Reference to undefined resource: my-network
```

**리소스 간 관계 확인**

```yaml
# 올바른 종속성 예시
resources:
    - name: my-network
      type: compute.v1.network

    - name: my-subnet
      type: compute.v1.subnetwork
      properties:
          network: $(ref.my-network.selfLink) # 올바른 참조

    - name: vm-instance
      type: compute.v1.instance
      properties:
          networkInterfaces:
              - subnetwork: $(ref.my-subnet.selfLink) # 올바른 참조
```

#### Preview 실행 결과 예시

**성공적인 Preview**

```bash
$ gcloud deployment-manager deployments create test-deployment \
  --config=template.yaml --preview

Create operation operation-1234567890 completed successfully.
NAME                TYPE                     STATE
my-network          compute.v1.network       IN_PREVIEW
my-subnet           compute.v1.subnetwork    IN_PREVIEW
vm-instance         compute.v1.instance      IN_PREVIEW

Dependencies validated successfully.
```

**종속성 오류가 있는 Preview**

```bash
$ gcloud deployment-manager deployments create test-deployment \
  --config=template.yaml --preview

ERROR: Error in Operation [operation-1234567890]:
errors:
- code: INVALID_RESOURCE_REFERENCE
  message: Reference to undefined resource 'missing-network'
```

#### 실제 워크플로우에서의 활용

**개발 프로세스**

```
1. 템플릿 수정
2. Preview 실행 (--preview)
3. 오류 발견 시 템플릿 수정 후 2단계 반복
4. Preview 성공 시 실제 배포 실행
5. 프로덕션에 커밋
```

**빠른 피드백 루프**

```
템플릿 수정 → Preview (10-30초) → 오류 확인 → 수정 → 반복
vs
템플릿 수정 → 실제 배포 (5-30분) → 오류 확인 → 정리 → 수정 → 반복
```

#### Preview vs 실제 배포 비교

**Preview Mode**

```
✅ 실행 시간: 10-30초
✅ 비용: $0
✅ 위험도: 없음
✅ 종속성 검증: 가능
✅ 리소스 정리: 불필요
❌ 실제 환경 테스트: 불가능
```

**실제 배포**

```
❌ 실행 시간: 5-30분
❌ 비용: 리소스 사용료 발생
❌ 위험도: 높음 (실제 리소스 영향)
✅ 종속성 검증: 가능
❌ 리소스 정리: 필요
✅ 실제 환경 테스트: 가능
```

#### 복잡한 템플릿에서 Preview의 중요성

**복잡한 종속성 예시**

```yaml
# 20개 이상의 리소스가 상호 의존하는 경우
resources:
    - name: vpc-network
    - name: subnet-web
    - name: subnet-app
    - name: subnet-db
    - name: firewall-web
    - name: firewall-app
    - name: load-balancer
    - name: instance-template-web
    - name: instance-group-web
    - name: autoscaler-web
# ... 등등

# 이런 복잡한 템플릿일수록 Preview가 중요
```

#### 정답: D. 같은 프로젝트에서 --preview 옵션을 사용하여 Deployment Manager 템플릿을 실행하고 상호 종속적인 리소스의 상태 관찰

**선택 이유**:

1. 가장 빠른 피드백 (10-30초 내 결과 확인)
2. 실제 리소스 생성 없이 안전한 검증
3. 종속성 그래프 및 참조 무결성 완전 검증
4. 템플릿 구문 오류 즉시 발견
5. 비용 발생 없음
6. API 호출 시뮬레이션을 통한 권한 및 할당량 검증
7. 반복적인 테스트에 적합한 빠른 실행 속도
8. 프로덕션 환경에 영향 없는 안전한 테스트 방법

:::

## dump 23

You are analyzing Google Cloud Platform service costs from three separate projects. You want to use this information to create service cost estimates by service type, daily and monthly, for the next six months using standard query syntax. What should you do?

A. Export your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.

B. Export your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.

C. Export your transactions to a local file, and perform analysis with a desktop tool.

D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.

:::details 풀이

**영문 문제 해석**

Google Cloud Platform의 세 개의 별도 프로젝트에서 서비스 비용을 분석하고 있습니다. 이 정보를 사용하여 표준 쿼리 구문을 통해 서비스 유형별로 일별 및 월별 서비스 비용 추정치를 향후 6개월 동안 생성하고자 합니다. 어떻게 해야 할까요?

이 문제는 GCP의 여러 프로젝트에서 발생한 비용 데이터를 수집하고 분석하여 미래의 비용을 예측하는 시나리오입니다. 핵심은 "표준 쿼리 구문(standard query syntax)"을 사용한다는 점입니다.

### 선택지 해석

**A. Export your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.**
빌링 데이터를 Cloud Storage 버킷으로 내보낸 다음 Cloud Bigtable로 가져와서 분석하는 방법입니다. Cloud Bigtable은 NoSQL 데이터베이스로 대용량 데이터 처리에 특화되어 있지만, 표준 SQL 쿼리를 지원하지 않습니다.

**B. Export your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.**
빌링 데이터를 Cloud Storage 버킷으로 내보낸 다음 Google Sheets로 가져와서 분석하는 방법입니다. Google Sheets는 간단한 분석에는 유용하지만 대용량 데이터 처리와 복잡한 시간 기반 쿼리에는 제한적입니다.

**C. Export your transactions to a local file, and perform analysis with a desktop tool.**
거래 내역을 로컬 파일로 내보내서 데스크톱 도구로 분석하는 방법입니다. 이는 확장성이 떨어지고 클라우드 환경의 이점을 활용하지 못합니다.

**D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.**
빌링 데이터를 BigQuery 데이터셋으로 내보낸 다음 시간 창 기반 SQL 쿼리를 작성하여 분석하는 방법입니다. BigQuery는 표준 SQL을 지원하며 대용량 데이터 분석에 최적화되어 있습니다.

### 문제 분석 및 정답 도출

이 문제의 핵심 요구사항을 분석해보면:

1. **표준 쿼리 구문 사용**: 문제에서 명시적으로 "standard query syntax"를 요구했습니다.
2. **시간 기반 분석**: 일별, 월별 분석과 6개월 예측이 필요합니다.
3. **서비스 유형별 분석**: 다양한 GCP 서비스별로 비용을 분류해야 합니다.
4. **다중 프로젝트 데이터**: 세 개의 프로젝트 데이터를 통합 분석해야 합니다.

각 선택지를 평가하면:

-   **A번 (Cloud Bigtable)**: NoSQL 데이터베이스로 표준 SQL을 지원하지 않습니다.
-   **B번 (Google Sheets)**: 간단한 분석에는 적합하지만 대용량 데이터와 복잡한 시간 기반 쿼리에는 한계가 있습니다.
-   **C번 (로컬 파일)**: 확장성이 부족하고 클라우드의 이점을 활용하지 못합니다.
-   **D번 (BigQuery)**: 표준 SQL을 완전히 지원하며, 시간 창 기반 쿼리(`DATE_TRUNC`, `EXTRACT` 등)와 대용량 데이터 분석에 최적화되어 있습니다.

BigQuery는 GCP의 빌링 데이터 분석을 위한 표준 솔루션입니다. BigQuery로 빌링 데이터를 내보내면 다음과 같은 이점이 있습니다:

-   표준 SQL 문법으로 복잡한 시간 기반 쿼리 작성 가능
-   `GROUP BY`, `PARTITION BY`, 윈도우 함수 등을 활용한 고급 분석
-   대용량 데이터 처리 성능
-   여러 프로젝트 데이터의 통합 분석 지원

**정답: D**

윈도우 기반 SQL 쿼리 (Window-based SQL Queries)란 시간 윈도우나 데이터 그룹을 기반으로 집계 및 분석을 수행하는 SQL 쿼리를 의미합니다.

:::

## dump 24

You want to send and consume Cloud Pub/Sub messages from your App Engine application. The Cloud Pub/Sub API is currently disabled. You will use a service account to authenticate your application to the API. You want to make sure your application can use Cloud Pub/Sub. What should you do?

A. Enable the Cloud Pub/Sub API in the API Library on the GCP Console.
B. Rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it.
C. Use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed.
D. Grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/Sub.

:::details 풀이

**영문 문제 해석**

App Engine 애플리케이션에서 Cloud Pub/Sub 메시지를 전송하고 소비하고자 합니다. 현재 Cloud Pub/Sub API가 비활성화되어 있습니다. 서비스 계정을 사용하여 API에 대한 애플리케이션 인증을 할 예정입니다. 애플리케이션이 Cloud Pub/Sub를 사용할 수 있도록 하려면 어떻게 해야 할까요?

이 문제는 GCP에서 API 활성화와 서비스 계정 권한 설정에 관한 기본적인 개념을 다루고 있습니다. 핵심은 비활성화된 API를 사용하기 위해 필요한 단계를 이해하는 것입니다.

### 선택지 해석

**A. Enable the Cloud Pub/Sub API in the API Library on the GCP Console.**
GCP Console의 API 라이브러리에서 Cloud Pub/Sub API를 활성화하는 방법입니다. 이는 GCP에서 API를 사용하기 위한 표준적이고 직접적인 방법입니다.

**B. Rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it.**
서비스 계정이 API에 액세스할 때 Cloud Pub/Sub API가 자동으로 활성화되는 것에 의존하는 방법입니다. 하지만 GCP에서는 API가 자동으로 활성화되지 않습니다.

**C. Use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed.**
Deployment Manager를 사용하여 애플리케이션을 배포하고, 배포되는 애플리케이션이 사용하는 모든 API의 자동 활성화에 의존하는 방법입니다. Deployment Manager는 API 자동 활성화 기능을 제공하지 않습니다.

**D. Grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/Sub.**
App Engine 기본 서비스 계정에 Cloud Pub/Sub Admin 역할을 부여하고, 애플리케이션이 Cloud Pub/Sub에 처음 연결할 때 API를 활성화하도록 하는 방법입니다. 하지만 애플리케이션 코드에서 직접 API를 활성화하는 것은 일반적인 접근 방식이 아닙니다.

### 문제 분석 및 정답 도출

이 문제의 핵심 요구사항을 분석해보면:

1. **API 활성화 필요성**: Cloud Pub/Sub API가 현재 비활성화되어 있어 사용할 수 없는 상태입니다.
2. **애플리케이션 접근**: App Engine 애플리케이션에서 Cloud Pub/Sub를 사용해야 합니다.
3. **서비스 계정 인증**: 서비스 계정을 통한 인증을 사용할 예정입니다.

GCP에서 API 사용을 위한 기본 원칙:

-   **API 활성화가 선행되어야 함**: 모든 GCP API는 사용하기 전에 반드시 활성화되어야 합니다.
-   **수동 활성화 필요**: GCP는 보안과 비용 관리를 위해 API를 자동으로 활성화하지 않습니다.
-   **Console을 통한 활성화**: API Library에서 API를 활성화하는 것이 표준 방법입니다.

각 선택지를 평가하면:

-   **A번**: 가장 직접적이고 표준적인 방법입니다. GCP Console의 API Library에서 Cloud Pub/Sub API를 활성화하면 즉시 사용 가능해집니다.
-   **B번**: GCP는 자동 API 활성화를 지원하지 않습니다. 명시적 활성화가 필요합니다.
-   **C번**: Deployment Manager는 리소스 배포를 위한 도구이며, API 자동 활성화 기능은 제공하지 않습니다.
-   **D번**: IAM 권한 부여는 중요하지만, API가 활성화되지 않으면 권한이 있어도 API를 사용할 수 없습니다. 또한 애플리케이션에서 직접 API를 활성화하는 것은 일반적이지 않습니다.

API 사용을 위한 올바른 순서:

1. API Library에서 API 활성화
2. 필요한 IAM 권한 설정
3. 서비스 계정 구성
4. 애플리케이션에서 API 사용

**정답: A**

GCP Pub/Sub은 Publisher-Subscriber 패턴의 완전 관리형 메시징 서비스이다. 애플리케이션 간 비동기적으로 메시지를 주고받을 수 있다. 자동 확장을 지원하고, 초당 수백만 개 메시지 처리가 가능하다.

클라우드 환경에서 메시지는 애플리케이션 간 정보 송수신을 위해 사용된다.

-   주문 처리 시스템에서 결제 - 재고 - 배송 각 단계별 메시지 전달
-   이메일 / SMS 발송 요청을 큐에 저장 후 순차 처리
-   로그 데이터 수집 및 분석
-   실시간 알림 서비스

:::

## dump 25

You have a website hosted on App Engine standard environment. You want 1% of your users to see a new test version of the website. You want to minimize complexity.

A. Deploy the new version in the same application and use the –-migrate option.

B. Deploy the new version in the same application and use the –-splits option to give a weight of 99 to the current version and a weight of 1 to the new version.

C. Create a new App Engine application in the same project. Deploy the new version in that application. Use the App Engine library to proxy 1% of the requests to the new version.

D. Create a new App Engine application in the same project. Deploy the new version in that application. Configure your network load balancer to send 1% of the traffic to that new application.

:::details 풀이

### **영문 문제 해석**

App Engine standard environment에서 웹사이트를 호스팅하고 있습니다. 사용자의 1%만 새로운 테스트 버전의 웹사이트를 보게 하고 싶습니다. 복잡성을 최소화하려고 합니다.

이는 A/B 테스트나 카나리 배포를 위한 트래픽 분할 시나리오입니다. App Engine에서 새 버전을 배포할 때 일부 사용자만 새 버전에 노출시키는 방법을 묻는 문제입니다.

### **선택지 해석**

**A. Deploy the new version in the same application and use the –-migrate option.**

-   같은 애플리케이션에 새 버전을 배포하고 `--migrate` 옵션을 사용
-   `--migrate` 옵션은 모든 트래픽을 새 버전으로 즉시 이동시킴 (100% 마이그레이션)
-   1% 분할이 아닌 전체 트래픽 이동이므로 요구사항에 맞지 않음

**B. Deploy the new version in the same application and use the –-splits option to give a weight of 99 to the current version and a weight of 1 to the new version.**

-   같은 애플리케이션에 새 버전을 배포하고 `--splits` 옵션 사용
-   현재 버전에 99의 가중치, 새 버전에 1의 가중치를 부여
-   99:1 비율로 트래픽을 분할하여 정확히 1%의 사용자가 새 버전을 보게 됨
-   App Engine의 내장 기능을 사용하여 복잡성이 낮음

**C. Create a new App Engine application in the same project. Deploy the new version in that application. Use the App Engine library to proxy 1% of the requests to the new version.**

-   같은 프로젝트에 새로운 App Engine 애플리케이션 생성
-   새 애플리케이션에 새 버전 배포
-   App Engine 라이브러리를 사용하여 1% 요청을 프록시
-   별도 애플리케이션 생성과 프록시 로직 구현으로 복잡성이 높음

**D. Create a new App Engine application in the same project. Deploy the new version in that application. Configure your network load balancer to send 1% of the traffic to that new application.**

-   같은 프로젝트에 새로운 App Engine 애플리케이션 생성
-   네트워크 로드 밸런서를 구성하여 1% 트래픽 전송
-   별도 애플리케이션과 로드 밸런서 설정으로 매우 복잡함

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 1% 사용자만 새 버전 노출
2. 복잡성 최소화

**App Engine의 트래픽 분할 기능:**

-   App Engine은 같은 애플리케이션 내에서 여러 버전 간 트래픽 분할을 지원
-   `gcloud app deploy --splits` 명령어로 가중치 기반 트래픽 분할 가능
-   별도의 인프라나 프록시 없이 간단하게 구현 가능

**각 선택지 분석:**

-   **A**: 전체 마이그레이션으로 요구사항 미충족
-   **B**: App Engine 내장 기능으로 정확한 1% 분할, 최소 복잡성
-   **C**: 별도 애플리케이션과 프록시 로직으로 복잡성 증가
-   **D**: 로드 밸런서 추가 구성으로 가장 복잡

**정답: B**

App Engine의 `--splits` 옵션을 사용하면 같은 애플리케이션 내에서 버전 간 트래픽을 가중치로 분할할 수 있습니다. 현재 버전 99, 새 버전 1의 가중치로 설정하면 정확히 1%의 사용자가 새 버전을 보게 되며, 별도의 인프라나 복잡한 설정 없이 가장 간단하게 구현할 수 있습니다.

D - 클라이언트와 서버 사이에서 요청을 가로채 트래픽 분할을 처리하는 로드밸런서를 구성한다는 것을 의미한다. 상당한 복잡성

:::

## dump 26

Your organization is a financial company that needs to store audit log files for 3 years. Your organization has hundreds of Google Cloud projects. You need to implement a cost-effective approach for log file retention.
A. Create an export to the sink that saves logs from Cloud Audit to BigQuery.

B. Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.

C. Write a custom script that uses logging API to copy the logs from Stackdriver logs to BigQuery.

D. Export these logs to Cloud Pub/Sub and write a Cloud Dataflow pipeline to store logs to Cloud SQL.

:::details 풀이

### **영문 문제 해석**

금융회사에서 감사 로그 파일을 3년간 보관해야 합니다. 조직에는 수백 개의 Google Cloud 프로젝트가 있습니다. 로그 파일 보존을 위한 비용 효과적인 접근 방식을 구현해야 합니다.

이는 대규모 조직에서 장기간 로그 보관을 위한 최적의 저장소 선택 문제입니다. 3년이라는 긴 보관 기간과 비용 효과성이 핵심 요구사항입니다.

### **선택지 해석**

**A. Create an export to the sink that saves logs from Cloud Audit to BigQuery.**

-   Cloud Audit 로그를 BigQuery로 내보내는 싱크 생성
-   BigQuery는 데이터 분석에는 적합하지만 장기 보관 시 비용이 높음
-   3년간 대량 로그 보관 시 스토리지 비용이 상당함
-   Audit 로그는 GCP에서 자동으로 생성되는 감사(감사원장 할때 그 감사) 로그이다.
    -   감사 로그: 누가, 언제, 무엇을, 어떻게 했는지 기록하는 로그
    -   GCP 리소스에 대한 관리 활동 및 데이터 접근 추적

**B. Create an export to the sink that saves logs from Cloud Audit to a Coldline Storage bucket.**

-   Cloud Audit 로그를 Coldline Storage 버킷으로 내보내는 싱크 생성
-   Coldline은 장기 보관용 저비용 스토리지 클래스
-   접근 빈도가 낮은 데이터에 최적화되어 3년 보관에 적합
-   가장 비용 효과적인 옵션

**C. Write a custom script that uses logging API to copy the logs from Stackdriver logs to BigQuery.**

-   Logging API를 사용하여 Stackdriver 로그를 BigQuery로 복사하는 커스텀 스크립트 작성
-   커스텀 스크립트 개발 및 유지보수 비용 발생
-   BigQuery의 높은 스토리지 비용 + 개발 복잡성
-   Stackdriver 로그는 현재 Cloud logging으로 명칭이 변경되었음. 모든 GCP 서비스와 애플리케이션 로그를 수집, 저장, 분석하는 서비스이다.
    -   감사 로그보다 규모가 훨씬 크기 때문에 비효율적
-   Logging API는 Google Cloud Logging과 프로그래밍 방식으로 상호작용 할 수 있게 해주는 RESTful API이다.

**D. Export these logs to Cloud Pub/Sub and write a Cloud Dataflow pipeline to store logs to Cloud SQL.**

-   로그를 Cloud Pub/Sub로 내보내고 Cloud Dataflow 파이프라인으로 Cloud SQL에 저장
-   가장 복잡한 아키텍처
-   Cloud SQL은 관계형 데이터베이스로 로그 저장에 부적합
-   높은 운영 비용과 복잡성

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 3년간 감사 로그 보관
2. 비용 효과적 접근 방식
3. 수백 개 프로젝트 대상

**Cloud Storage 클래스별 특성:**

-   **Standard**: 자주 접근하는 데이터용, 비용 높음
-   **Nearline**: 월 1회 미만 접근, 30일 최소 보관
-   **Coldline**: 분기 1회 미만 접근, 90일 최소 보관
-   **Archive**: 연 1회 미만 접근, 365일 최소 보관

**감사 로그 특성:**

-   규제 준수를 위한 장기 보관 필요
-   일반적으로 접근 빈도가 낮음
-   필요 시에만 조회하는 아카이브 성격

**각 선택지 분석:**

-   **A**: BigQuery는 분석용으로 장기 보관 시 비용 부담
-   **B**: Coldline은 3년 보관에 최적화된 저비용 솔루션
-   **C**: 커스텀 개발 + BigQuery 비용으로 비효율적
-   **D**: 과도하게 복잡한 아키텍처, Cloud SQL 부적합

**정답: B**

감사 로그는 규제 준수를 위해 보관하지만 자주 접근하지 않는 데이터입니다. Coldline Storage는 분기별 1회 미만 접근하는 데이터를 위한 저비용 스토리지 클래스로, 3년간 장기 보관에 가장 비용 효과적입니다. Cloud Logging의 export sink 기능을 사용하면 자동으로 로그를 Coldline 버킷으로 전송할 수 있어 운영 복잡성도 최소화됩니다.

:::

## dump 27

You built an application on Google Cloud that uses Cloud Spanner. Your support team needs to monitor the environment but should not have access to table data. You need a streamlined solution to grant the correct permissions to your support team, and you want to follow Google-recommended practices. What should you do?

A. Add the support team group to the roles/monitoring.viewer role.

B. Add the support team group to the roles/spanner.databaseUser role.

C. Add the support team group to the roles/spanner.databaseReader role.

D. Add the support team group to the roles/stackdriver.accounts.viewer role.

:::details 풀이

### **영문 문제 해석**

Google Cloud에서 Cloud Spanner를 사용하는 애플리케이션을 구축했습니다. 지원팀이 환경을 모니터링해야 하지만 테이블 데이터에는 액세스할 수 없어야 합니다. 지원팀에 올바른 권한을 부여하는 간소화된 솔루션이 필요하며, Google 권장 사례를 따르고 싶습니다.

이는 Cloud Spanner 환경에서 모니터링은 가능하지만 실제 데이터 접근은 차단해야 하는 권한 분리 문제입니다. 최소 권한 원칙과 역할 기반 액세스 제어가 핵심입니다.

### **선택지 해석**

**A. Add the support team group to the roles/monitoring.viewer role.**

-   Cloud Monitoring의 뷰어 역할 부여 (Cloud Monitoring이라는 서비스가 존재함.)
    -   리소스와 애플리케이션 성능, 가용성, 상태를 모니터링하는 서비스.
    -   메트릭 수집, 대시보드, 알림 등을 제공하는 완전 관리형 서비스.
-   모든 Google Cloud 서비스의 모니터링 메트릭과 대시보드를 읽기 전용으로 접근
-   Cloud Spanner 성능 메트릭, 쿼리 통계, 인스턴스 상태 등 모니터링 정보 확인 가능
-   실제 테이블 데이터에는 액세스 불가
-   Google 권장 사례에 부합하는 간소화된 솔루션

**B. Add the support team group to the roles/spanner.databaseUser role.**

-   Cloud Spanner 데이터베이스 사용자 역할
-   데이터베이스에 연결하고 DML 작업(SELECT, INSERT, UPDATE, DELETE) 수행 가능
-   테이블 데이터에 직접 액세스할 수 있어 요구사항에 위배

**C. Add the support team group to the roles/spanner.databaseReader role.**

-   Cloud Spanner 데이터베이스 리더 역할
-   데이터베이스의 모든 테이블과 뷰에서 데이터를 읽을 수 있음
-   SELECT 쿼리를 통해 실제 테이블 데이터에 액세스 가능
-   요구사항에 명확히 위배됨

**D. Add the support team group to the roles/stackdriver.accounts.viewer role.**

-   더 이상 사용되지 않는 레거시 Stackdriver 역할
-   현재는 Cloud Operations 제품군으로 통합됨
-   Google 권장 사례에 부합하지 않음

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 환경 모니터링 가능
2. 테이블 데이터 접근 불가
3. 간소화된 솔루션
4. Google 권장 사례 준수

**Cloud Spanner 모니터링 vs 데이터 접근:**

-   **모니터링**: 성능 메트릭, CPU 사용률, 쿼리 통계, 인스턴스 상태
-   **데이터 접근**: 실제 테이블의 레코드 내용 조회

**역할별 권한 분석:**

-   **monitoring.viewer**: 모니터링 데이터만 접근, 실제 데이터 접근 불가
-   **spanner.databaseUser**: 데이터 읽기/쓰기 가능 (요구사항 위배)
-   **spanner.databaseReader**: 데이터 읽기 가능 (요구사항 위배)
-   **stackdriver.accounts.viewer**: 레거시 역할 (권장 사례 위배)

**Google 권장 사례:**

-   최소 권한 원칙: 필요한 최소한의 권한만 부여
-   목적별 역할 사용: 모니터링 목적이면 모니터링 역할 사용
-   최신 역할 사용: 레거시 Stackdriver 대신 현재 Cloud Operations 역할

**모니터링 정보 포함 범위:**
`roles/monitoring.viewer`로 확인 가능한 Cloud Spanner 정보:

-   인스턴스 CPU 사용률
-   스토리지 사용량
-   쿼리 성능 통계
-   API 요청 수
-   대기 시간 메트릭
-   오류율

**정답: A**

`roles/monitoring.viewer` 역할은 Cloud Spanner를 포함한 모든 Google Cloud 서비스의 모니터링 메트릭에 읽기 전용 액세스를 제공합니다. 이를 통해 지원팀은 성능, 가용성, 오류율 등 환경 상태를 모니터링할 수 있지만, 실제 테이블 데이터에는 접근할 수 없습니다. 이는 최소 권한 원칙을 따르는 Google 권장 사례에 부합하는 간소화된 솔루션입니다.

:::

## dump 28

You want to run a single caching HTTP reverse proxy on GCP for a latency-sensitive website. This specific reverse proxy consumes almost no CPU. You want to have a 30-GB in-memory cache, and need an additional 2 GB of memory for the rest of the processes. You want to minimize cost. How should you run this reverse proxy?

A. Create a Cloud Memorystore for Redis instance with 32-GB capacity.

B. Run it on Compute Engine, and choose a custom instance type with 6 vCPUs and 32 GB of memory.

C. Package it in a container image, and run it on Kubernetes Engine, using n1-standard-32 instances as nodes.

D. Run it on Compute Engine, choose the instance type n1-standard-1, and add an SSD persistent disk of 32 GB.

:::details 풀이

### **영문 문제 해석**

지연 시간에 민감한 웹사이트를 위해 GCP에서 단일 캐싱 HTTP 리버스 프록시를 실행하려고 합니다. 이 특정 리버스 프록시는 CPU를 거의 사용하지 않습니다. 30GB의 인메모리 캐시가 필요하고, 나머지 프로세스를 위해 추가로 2GB의 메모리가 필요합니다. 비용을 최소화하려고 합니다.

이는 CPU 사용량은 낮지만 대용량 캐시가 필요한 리버스 프록시 구성에서 최적의 비용 효율적 솔루션을 찾는 문제입니다. 총 32GB 인메모리 캐시와 비용 최소화가 핵심 요구사항입니다.

### **선택지 해석**

**A. Create a Cloud Memorystore for Redis instance with 32-GB capacity.**

-   Google Cloud의 관리형 Redis 서비스 사용
-   32GB 용량의 인메모리 캐시 제공
-   HTTP 리버스 프록시는 별도 경량 인스턴스에서 실행하고 Redis를 캐시 백엔드로 사용
-   관리형 서비스로 운영 부담 최소화
-   캐시에 특화된 고성능과 고가용성 제공

**B. Run it on Compute Engine, and choose a custom instance type with 6 vCPUs and 32 GB of memory.**

-   Compute Engine 커스텀 인스턴스 사용
-   6 vCPUs + 32GB 메모리 구성
-   CPU 사용량이 거의 없다고 했으므로 6 vCPUs는 과도한 리소스
-   VM 관리 부담과 불필요한 CPU 비용 발생

**C. Package it in a container image, and run it on Kubernetes Engine, using n1-standard-32 instances as nodes.**

-   GKE에서 컨테이너로 실행
-   n1-standard-32 인스턴스 (32 vCPUs, 120GB 메모리) 사용
-   단일 리버스 프록시를 위해 과도한 리소스 할당
-   Kubernetes 관리 오버헤드와 불필요한 CPU/메모리로 매우 비효율적

**D. Run it on Compute Engine, choose the instance type n1-standard-1, and add an SSD persistent disk of 32 GB.**

-   n1-standard-1 인스턴스 (1 vCPU, 3.75GB 메모리)
-   32GB SSD persistent disk 추가
-   인메모리 캐시가 필요한데 디스크 스토리지 제안
-   디스크는 메모리보다 훨씬 느려서 지연 시간 요구사항 미충족

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 30GB 인메모리 캐시 + 2GB 프로세스 메모리
2. CPU 사용량 거의 없음
3. 지연 시간에 민감 (고성능 인메모리 필수)
4. 비용 최소화

**Cloud Memorystore의 장점:**

-   **관리형 서비스**: 백업, 모니터링, 패치 자동화
-   **고가용성**: 자동 장애 조치 및 복제
-   **최적화된 성능**: 캐싱에 특화된 Redis 엔진
-   **스케일링**: 필요 시 쉽게 용량 조정
-   **운영 비용 절약**: 인프라 관리 부담 없음

**아키텍처 구성:**
[클라이언트] → [경량 HTTP 프록시 (소형 VM)] → [Cloud Memorystore Redis 32GB] → [백엔드 서버]

**각 선택지 총 비용 분석:**

-   **A**: Cloud Memorystore 32GB + 소형 프록시 VM = 최적화된 비용
-   **B**: 커스텀 VM 32GB + VM 관리 비용 = 더 높은 총 비용
-   **C**: 과도한 GKE 리소스 = 매우 높은 비용
-   **D**: 성능 요구사항 미충족으로 부적합

**정답: A**

대용량 인메모리 캐시가 필요한 리버스 프록시의 경우, Cloud Memorystore for Redis 32GB 인스턴스가 가장 비용 효율적입니다. HTTP 프록시는 CPU를 거의 사용하지 않으므로 소형 VM에서 실행하고, 실제 캐싱은 관리형 Redis 서비스에서 처리하는 구조가 관리 부담과 총 운영 비용을 최소화합니다. 또한 캐시에 특화된 고성능과 고가용성을 자동으로 제공받을 수 있습니다.

:::

## dump 29

You have a single binary application that you want to run on Google Cloud Platform. You decided to automatically scale the application based on underlying infrastructure CPU usage. Your organizational policies require you to use Virtual Machines directly. You need to ensure that the application scaling is operationally efficient and completed as quickly as possible. What should you do?

A. Create a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.

B. Create an instance template, and use the template in a Managed Instance Group with autoscaling configured.

C. Create an instance template, and use the template in a Managed Instance Group that scales up and down based on the time of day.

D. Use a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring.

:::details

### **영문 문제 해석**

Google Cloud Platform에서 실행하려는 단일 바이너리 애플리케이션이 있습니다. 기본 인프라의 CPU 사용률을 기반으로 애플리케이션을 자동 확장하기로 결정했습니다. 조직 정책상 가상 머신을 직접 사용해야 합니다. 애플리케이션 확장이 운영적으로 효율적이고 가능한 한 빠르게 완료되도록 해야 합니다.

이는 VM을 사용해야 하는 제약 조건 하에서 CPU 기반 자동 확장을 구현하는 문제입니다. 운영 효율성과 확장 속도가 핵심 요구사항입니다.

### **선택지 해석**

**A. Create a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.**

-   GKE 클러스터 생성 및 Horizontal Pod Autoscaler 사용
-   Kubernetes는 컨테이너 기반으로 VM을 직접 사용하는 조건에 부합하지 않음
-   조직 정책 위반으로 부적합한 선택지

**B. Create an instance template, and use the template in a Managed Instance Group with autoscaling configured.**

-   인스턴스 템플릿 생성 후 관리형 인스턴스 그룹에서 자동 확장 설정
-   VM 기반으로 조직 정책 준수
-   CPU 사용률 기반 자동 확장 지원
-   Google Cloud의 네이티브 자동 확장 기능으로 운영 효율성 높음
-   사전 정의된 템플릿으로 빠른 인스턴스 생성

**C. Create an instance template, and use the template in a Managed Instance Group that scales up and down based on the time of day.**

-   시간대 기반 확장 설정
-   CPU 사용률이 아닌 시간 기반 확장으로 요구사항 미충족
-   실제 부하와 관계없는 확장으로 비효율적

**D. Use a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring.**

-   써드파티 도구를 사용한 커스텀 자동화 구축
-   Stackdriver(Cloud Monitoring) CPU 모니터링 기반
-   커스텀 솔루션 개발로 운영 복잡성 증가
-   구현 및 유지보수 부담으로 운영 효율성 저하

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. CPU 사용률 기반 자동 확장
2. 가상 머신 직접 사용 (조직 정책)
3. 운영 효율성 최대화
4. 빠른 확장 완료

**Managed Instance Group (MIG)의 장점:**

-   **자동 확장**: CPU, 메모리, HTTP 로드 등 다양한 메트릭 기반
-   **인스턴스 템플릿**: 표준화된 VM 구성으로 일관성 보장
-   **빠른 확장**: 사전 구성된 템플릿으로 신속한 인스턴스 생성
-   **자동 치유**: 비정상 인스턴스 자동 교체
-   **로드 밸런싱**: 자동 로드 밸런서 연동
-   **관리형 서비스**: Google Cloud 네이티브 기능으로 운영 부담 최소화

**각 선택지 분석:**

-   **A**: 조직 정책 위반 (VM 직접 사용 조건 위배)
-   **B**: 모든 요구사항 충족, 네이티브 기능으로 최적 효율성
-   **C**: CPU 기반이 아닌 시간 기반 확장으로 요구사항 미충족
-   **D**: 커스텀 구현으로 운영 복잡성 증가, 효율성 저하

**MIG 자동 확장 설정 예:**

```bash
gcloud compute instance-groups managed set-autoscaling my-mig \
    --max-num-replicas 10 \
    --min-num-replicas 1 \
    --target-cpu-utilization 0.75 \
    --cool-down-period 90s
```

운영 효율성 측면:

네이티브 기능: Google Cloud가 제공하는 완전 관리형 서비스
모니터링 통합: Cloud Monitoring과 자동 연동
확장 속도: 템플릿 기반으로 빠른 인스턴스 프로비저닝
유지보수: 써드파티 도구나 커스텀 스크립트 불필요

정답: B

조직 정책에 따라 VM을 직접 사용해야 하는 환경에서 CPU 기반 자동 확장을 구현하려면 Managed Instance Group이 가장 적합합니다. 인스턴스 템플릿으로 표준화된 VM 구성을 정의하고, MIG의 자동 확장 기능을 통해 CPU 사용률에 따른 확장/축소가 가능합니다. 이는 Google Cloud의 네이티브 기능으로 운영 효율성이 높고, 사전 구성된 템플릿을 통해 빠른 확장을 보장합니다.

:::

## dump 30

You need to set up permissions for a set of Compute Engine instances to enable them to write data into a particular Cloud Storage bucket. You want to follow Google-recommended practices. What should you do?

A. Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/devstorage.write_only'.

B. Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/cloud-platform'.

C. Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket.

D. Create a service account and add it to the IAM role 'storage.objectAdmin' for that bucket.

:::details 풀이

### **영문 문제 해석**

Compute Engine 인스턴스들이 특정 Cloud Storage 버킷에 데이터를 쓸 수 있도록 권한을 설정해야 합니다. Google 권장 사례를 따르고 싶습니다.

이는 VM에서 Cloud Storage에 쓰기 권한을 부여하는 최적의 방법을 찾는 문제입니다. 최소 권한 원칙과 Google 권장 사례가 핵심입니다.

### **선택지 해석**

**A. Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/devstorage.write_only'.**

-   서비스 계정 생성 후 액세스 스코프 사용
-   `devstorage.write_only` 스코프는 Cloud Storage 쓰기 전용 권한
-   액세스 스코프는 레거시 방식으로 Google에서 권장하지 않음
-   IAM 역할 대신 스코프를 사용하는 구식 접근법
-   액세스 스코프는 GCP VM 인스턴스가 구글 클라우드 API에 접근할 수 있는 범위를 제한하는 레거시 방식이다. VM 런타임에 권한 조정이 어렵다.

**B. Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/cloud-platform'.**

-   `cloud-platform` 스코프 사용
-   모든 Google Cloud 서비스에 대한 권한 부여
-   과도한 권한으로 최소 권한 원칙 위배
-   여전히 레거시 스코프 방식 사용

**C. Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket.**

-   서비스 계정 생성 후 IAM 역할 할당
-   `storage.objectCreator` 역할: 객체 생성 권한만 제공
-   특정 버킷에 대해서만 권한 부여
-   최소 권한 원칙 준수 및 현대적 IAM 방식

**D. Create a service account and add it to the IAM role 'storage.objectAdmin' for that bucket.**

-   `storage.objectAdmin` 역할 사용
-   객체 생성, 읽기, 수정, 삭제 모든 권한 포함
-   쓰기만 필요한 상황에서 과도한 권한 부여
-   최소 권한 원칙 위배

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. Compute Engine에서 특정 Cloud Storage 버킷에 데이터 쓰기
2. Google 권장 사례 준수
3. 최소 권한 원칙

**Google 권장 사례:**

**1. IAM 역할 vs 액세스 스코프**

-   **현재 권장**: IAM 역할 사용
-   **레거시**: 액세스 스코프 (더 이상 권장하지 않음)
-   IAM 역할이 더 세밀하고 유연한 권한 제어 제공

**2. 최소 권한 원칙**

-   필요한 최소한의 권한만 부여
-   쓰기만 필요하면 쓰기 권한만 부여
-   과도한 권한은 보안 위험 증가

**Storage IAM 역할 비교:**

**`storage.objectCreator`**

-   객체 생성 및 업로드만 가능
-   기존 객체 읽기, 수정, 삭제 불가
-   쓰기 전용 시나리오에 최적

**`storage.objectAdmin`**

-   모든 객체 작업 가능 (CRUD)
-   버킷 내 모든 권한 포함
-   필요 이상의 권한 제공

**액세스 스코프의 문제점:**

-   VM 레벨에서 설정되어 세밀한 제어 어려움
-   IAM 정책과 혼재 시 복잡성 증가
-   Google에서 IAM 역할 사용 권장

**구현 예시:**

```bash
# 1. 서비스 계정 생성
gcloud iam service-accounts create vm-storage-writer

# 2. IAM 역할 바인딩 (특정 버킷에만)
gcloud projects add-iam-policy-binding `<project-id>` \
    --member="serviceAccount:vm-storage-writer@`<project-id>`.iam.gserviceaccount.com" \
    --role="roles/storage.objectCreator" \
    --condition='expression=resource.name.startsWith("projects/_/buckets/my-bucket/")'

# 3. VM에 서비스 계정 할당
gcloud compute instances create my-vm \
    --service-account=vm-storage-writer@`<project-id>`.iam.gserviceaccount.com
```

각 선택지 분석:

A, B: 레거시 스코프 방식으로 권장되지 않음
C: 최소 권한 + 현대적 IAM 방식으로 최적
D: 과도한 권한으로 보안 위험

정답: C
Google 권장 사례에 따르면 액세스 스코프 대신 IAM 역할을 사용해야 하며, 최소 권한 원칙에 따라 쓰기 작업만 필요한 경우 storage.objectCreator 역할을 사용해야 합니다. 이 역할은 특정 버킷에 객체를 생성하고 업로드할 수 있는 권한만 제공하여 보안을 유지하면서 필요한 기능을 수행할 수 있습니다.

:::

## dump 31

You have an object in a Cloud Storage bucket that you want to share with an external company. The object contains sensitive data. You want access to the content to be removed after four hours. The external company does not have a Google account to which you can grant specific user-based access privileges. You want to use the most secure method that requires the fewest steps. What should you do?
A. Create a signed URL with a four-hour expiration and share the URL with the company.

B. Set object access to 'public' and use object lifecycle management to remove the object after four hours.

C. Configure the storage bucket as a static website and furnish the object's URL to the company. Delete the object from the storage bucket after four hours.

D. Create a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours have passed.

:::details 풀이

### **영문 문제 해석**

Cloud Storage 버킷에 있는 객체를 외부 회사와 공유하려고 합니다. 이 객체에는 민감한 데이터가 포함되어 있습니다. 4시간 후에 콘텐츠에 대한 접근을 제거하려고 합니다. 외부 회사는 특정 사용자 기반 접근 권한을 부여할 수 있는 Google 계정이 없습니다. 가장 안전하면서 최소한의 단계가 필요한 방법을 사용하려고 합니다.

이는 Google 계정이 없는 외부 사용자에게 민감한 데이터를 시간 제한과 함께 안전하게 공유하는 문제입니다. 보안성과 단순성이 핵심 요구사항입니다.

### **선택지 해석**

**A. Create a signed URL with a four-hour expiration and share the URL with the company.**

-   4시간 만료 시간이 설정된 서명된 URL 생성 후 공유
-   서명된 URL은 특정 시간 후 자동 만료되는 보안 메커니즘
-   Google 계정 없이도 접근 가능
-   가장 간단하고 안전한 방법
-   URL 생성 한 번으로 완료되는 최소 단계
-   구글 클라우드 스토리지에서 signed URL을 사용하면 만료시간을 설정할 수 있다. 구글에서 제공하는 보안 서명까지 제공되므로 퍼블릭하지 않게 데이터 제공이 가능하다.

**B. Set object access to 'public' and use object lifecycle management to remove the object after four hours.**

-   객체를 퍼블릭으로 설정하고 생명주기 관리로 4시간 후 삭제
-   퍼블릭 접근으로 인터넷의 누구든지 접근 가능
-   민감한 데이터에 대해 매우 위험한 방법
-   보안 요구사항에 심각하게 위배

**C. Configure the storage bucket as a static website and furnish the object's URL to the company. Delete the object from the storage bucket after four hours.**

-   버킷을 정적 웹사이트로 구성하고 4시간 후 수동 삭제
-   정적 웹사이트 설정은 퍼블릭 접근을 의미
-   수동 삭제 필요로 자동화되지 않음
-   민감한 데이터에 부적합하고 복잡함

**D. Create a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours have passed.**

-   외부 회사 전용 새 버킷 생성 및 객체 복사
-   4시간 후 전체 버킷 삭제
-   여전히 접근 권한 문제 해결되지 않음
-   불필요한 복잡성과 추가 단계 필요

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 민감한 데이터 안전한 공유
2. 4시간 후 자동 접근 제거
3. Google 계정 없는 외부 사용자 대상
4. 최소 단계로 최대 보안

**서명된 URL (Signed URL)의 장점:**

**1. 시간 제한 보안**

-   지정된 시간 후 자동 만료
-   만료 후 URL로 접근 불가능
-   수동 삭제나 추가 작업 불필요

**2. 인증 없는 접근**

-   Google 계정 불필요
-   URL 소유자만 접근 가능
-   외부 사용자에게 이상적

**3. 세밀한 권한 제어**

-   특정 객체에만 접근 허용
-   읽기 전용 또는 특정 작업만 허용
-   최소 권한 원칙 준수

**4. 암호화된 보안**

-   URL 자체에 서명과 만료 정보 포함
-   위조나 변조 불가능
-   HTTPS 프로토콜로 전송 중 암호화

**서명된 URL 생성 예시:**

    # gsutil 사용
    gsutil signurl -d 4h service-account-key.json gs://bucket/sensitive-file.pdf

    # gcloud 사용
    gcloud storage sign-url gs://bucket/sensitive-file.pdf --duration=4h

**생성된 URL 형태:**

    https://storage.googleapis.com/bucket/sensitive-file.pdf?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=...&X-Goog-Date=20231215T120000Z&X-Goog-Expires=14400&X-Goog-SignedHeaders=host&X-Goog-Signature=...

**각 선택지 보안 분석:**

-   **A**: 암호화된 서명 + 자동 만료 = 최고 보안
-   **B**: 퍼블릭 접근 = 보안 위험 매우 높음
-   **C**: 퍼블릭 웹사이트 = 보안 위험 높음
-   **D**: 접근 권한 미해결 = 보안 문제 지속

**단계 수 비교:**

-   **A**: 1단계 (서명된 URL 생성)
-   **B**: 2단계 (퍼블릭 설정 + 생명주기 설정)
-   **C**: 3단계 (웹사이트 구성 + URL 제공 + 수동 삭제)
-   **D**: 4단계 (버킷 생성 + 복사 + 권한 설정 + 삭제)

**정답: A**

서명된 URL은 Google 계정이 없는 외부 사용자에게 민감한 데이터를 시간 제한과 함께 안전하게 공유할 수 있는 최적의 방법입니다. 4시간 만료 시간이 URL에 암호화되어 포함되므로 자동으로 접근이 차단되며, 단일 단계로 생성 가능하여 가장 간단하면서도 보안적인 솔루션입니다. 다른 방법들은 모두 퍼블릭 접근을 허용하여 민감한 데이터에 부적합합니다.

:::

## dump 32

You need to create an autoscaling Managed Instance Group for an HTTPS web application. You want to make sure that unhealthy VMs are recreated. What should you do?

A. Create a health check on port 443 and use that when creating the Managed Instance Group.

B. Select Multi-Zone instead of Single-Zone when creating the Managed Instance Group.

C. In the Instance Template, add the label 'health-check'.

D. In the Instance Template, add a startup script that sends a heartbeat to the metadata server.

:::details 풀이

### **영문 문제 해석**

HTTPS 웹 애플리케이션을 위한 자동 확장 관리형 인스턴스 그룹을 생성해야 합니다. 비정상 VM이 재생성되도록 하려고 합니다.

이는 Managed Instance Group에서 VM의 상태를 모니터링하고 비정상 인스턴스를 자동으로 교체하는 메커니즘을 구현하는 문제입니다. HTTPS 웹 애플리케이션의 건강성 확인이 핵심입니다.

### **선택지 해석**

**A. Create a health check on port 443 and use that when creating the Managed Instance Group.**

-   443 포트(HTTPS)에서 헬스 체크 생성
-   관리형 인스턴스 그룹 생성 시 헬스 체크 설정
-   HTTPS 애플리케이션의 실제 가용성을 확인
-   응답하지 않는 인스턴스를 자동으로 비정상으로 판단하여 재생성

**B. Select Multi-Zone instead of Single-Zone when creating the Managed Instance Group.**

-   멀티 존 배포 선택
-   고가용성과 장애 격리를 위한 설정
-   비정상 VM 감지 및 재생성과는 직접적 관련 없음
-   존 장애에 대한 복원력은 제공하지만 개별 VM 상태 모니터링은 별개

**C. In the Instance Template, add the label 'health-check'.**

-   인스턴스 템플릿에 `health-check` 라벨 추가
-   라벨은 단순한 메타데이터로 실제 헬스 체크 기능 없음
-   비정상 VM 감지나 재생성 메커니즘과 무관

**D. In the Instance Template, add a startup script that sends a heartbeat to the metadata server.**

-   스타트업 스크립트로 메타데이터 서버에 하트비트 전송
-   메타데이터 서버는 헬스 체크 기능을 제공하지 않음
-   커스텀 모니터링 로직이지만 MIG의 자동 복구와 연동되지 않음

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. HTTPS 웹 애플리케이션 대상
2. 비정상 VM 자동 재생성
3. 자동 확장 관리형 인스턴스 그룹

**Managed Instance Group의 자동 복구 메커니즘:**

-   **헬스 체크**: VM의 실제 애플리케이션 상태 확인
-   **자동 복구**: 헬스 체크 실패 시 인스턴스 자동 삭제 및 재생성
-   **로드 밸런서 연동**: 비정상 인스턴스로 트래픽 전송 중단

**HTTPS 헬스 체크의 중요성:**

-   **포트 443**: HTTPS 프로토콜의 표준 포트
-   **애플리케이션 레벨 확인**: 단순히 VM이 실행 중인지가 아니라 웹 애플리케이션이 정상 응답하는지 확인
-   **실제 사용자 경험 반영**: 사용자가 접근하는 것과 동일한 방식으로 상태 확인

**헬스 체크 작동 원리:**

    # 헬스 체크 생성 예시
    gcloud compute health-checks create https my-https-health-check \
        --port 443 \
        --request-path /health \
        --check-interval 30s \
        --timeout 10s \
        --healthy-threshold 2 \
        --unhealthy-threshold 3

    # MIG에 헬스 체크 적용
    gcloud compute instance-groups managed create my-mig \
        --template my-template \
        --health-check my-https-health-check \
        --initial-delay 300s

**각 선택지 분석:**

-   **A**: HTTPS 애플리케이션의 실제 상태를 확인하는 정확한 방법
-   **B**: 고가용성 제공하지만 개별 VM 헬스 체크와는 별개
-   **C**: 메타데이터 라벨은 기능적 역할 없음
-   **D**: 커스텀 하트비트는 MIG 자동 복구와 연동되지 않음

**헬스 체크 vs 다른 모니터링:**

-   **헬스 체크**: MIG가 인식하고 자동 조치 가능
-   **커스텀 모니터링**: 별도 로직 구현 필요, MIG와 연동 복잡

**자동 복구 프로세스:**

1. 헬스 체크가 연속으로 실패
2. MIG가 해당 인스턴스를 비정상으로 판단
3. 비정상 인스턴스 삭제
4. 인스턴스 템플릿을 기반으로 새 인스턴스 생성
5. 새 인스턴스가 헬스 체크 통과 후 서비스 투입

**정답: A**

HTTPS 웹 애플리케이션의 비정상 VM을 자동으로 재생성하려면 443 포트에서 헬스 체크를 생성하고 이를 Managed Instance Group에 연결해야 합니다. 헬스 체크는 실제 애플리케이션의 가용성을 확인하여 응답하지 않는 인스턴스를 자동으로 감지하고, MIG는 이러한 비정상 인스턴스를 삭제한 후 새로운 인스턴스로 교체합니다. 이는 Google Cloud의 네이티브 자동 복구 메커니즘으로 가장 효율적이고 안정적인 솔루션입니다.

포트(Port)의 개념:
포트는 컴퓨터에서 네트워크 통신의 출입구 역할을 합니다. 하나의 컴퓨터에서 여러 서비스가 동시에 실행될 때, 각 서비스를 구분하기 위한 번호입니다.

```text
클라이언트 → IP주소:포트번호 → 서버
예: 192.168.1.100:443 (HTTPS 서비스)
    192.168.1.100:80  (HTTP 서비스)
    192.168.1.100:22  (SSH 서비스)
```

포트 번호 체계:

0-1023: 잘 알려진 포트 (Well-known ports)
1024-49151: 등록된 포트 (Registered ports)
49152-65535: 동적/사설 포트 (Dynamic/Private ports)

주요 표준 포트들:
웹 서비스:

80번: HTTP (HyperText Transfer Protocol)
443번: HTTPS (HTTP Secure)

이메일:

25번: SMTP (메일 전송)
110번: POP3 (메일 수신)
143번: IMAP (메일 수신)

파일 전송:

21번: FTP (File Transfer Protocol)
22번: SSH/SFTP (Secure Shell)

데이터베이스:

3306번: MySQL
3389: RDP
5432번: PostgreSQL
1433번: SQL Server

:::

## dump 33

You are deploying an application to a Compute Engine VM in a Managed Instance Group. The application must be running at all times, but only a single instance of the VM should run per GCP project. How should you configure the instance group?

A. Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 1.

B. Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 1.

C. Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 2.

D. Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 2.

:::details 풀이

### **영문 문제 해석**

Managed Instance Group에서 HTTPS 웹 애플리케이션을 위한 자동 확장을 생성해야 합니다. 애플리케이션이 항상 실행되고 있어야 하지만, GCP 프로젝트당 VM의 단일 인스턴스만 실행되어야 합니다.

이는 고가용성을 유지하면서도 정확히 하나의 인스턴스만 실행되도록 보장하는 특수한 요구사항을 가진 문제입니다. 단일성과 가용성을 동시에 만족해야 합니다.

### **선택지 해석**

**A. Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 1.**

-   자동 확장 활성화
-   최소 인스턴스: 1개
-   최대 인스턴스: 1개
-   항상 정확히 1개 인스턴스 유지하면서 자동 복구 기능 제공

**B. Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 1.**

-   자동 확장 비활성화
-   인스턴스가 실패해도 자동으로 재생성되지 않음
-   수동 개입 필요

**C. Set autoscaling to On, set the minimum number of instances to 1, and then set the maximum number of instances to 2.**

-   최대 2개 인스턴스 허용
-   "단일 인스턴스만 실행" 요구사항 위배

**D. Set autoscaling to Off, set the minimum number of instances to 1, and then set the maximum number of instances to 2.**

-   자동 확장 비활성화 + 자동 복구 없음
-   요구사항 미충족

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 애플리케이션이 항상 실행 상태 유지
2. 프로젝트당 정확히 1개 인스턴스만 실행
3. 자동 관리를 통한 운영 효율성

**자동 확장의 역할:**

-   **자동 확장 ON**: 헬스 체크 실패 시 자동으로 인스턴스 재생성
-   **자동 확장 OFF**: 인스턴스 실패 시 수동 개입 필요

**min=1, max=1 설정의 의미:**

-   항상 정확히 1개 인스턴스 유지
-   인스턴스 실패 시 즉시 새 인스턴스로 교체
-   부하와 관계없이 인스턴스 수 고정

**각 선택지 분석:**

-   **A**: 자동 복구 + 단일 인스턴스 보장 = 모든 요구사항 충족
-   **B**: 자동 복구 없음으로 가용성 위험
-   **C**: 최대 2개로 단일성 요구사항 위배
-   **D**: 자동 복구 없음 + 설정 불일치

**실제 동작 시나리오:** # 정상 상태: 1개 인스턴스 실행 # 인스턴스 실패 시: MIG가 자동으로 감지 # 자동 복구: 즉시 새 인스턴스 생성 # 결과: 다운타임 최소화하며 1개 인스턴스 유지

**정답: A**

"애플리케이션이 항상 실행되어야 한다"는 고가용성 요구사항과 "단일 인스턴스만 실행"이라는 제약사항을 동시에 만족하려면 자동 확장을 활성화하되 min=1, max=1로 설정해야 합니다. 이렇게 하면 인스턴스 실패 시 MIG가 자동으로 새 인스턴스를 생성하여 서비스 연속성을 보장하면서도 정확히 하나의 인스턴스만 실행되도록 할 수 있습니다.

:::

## dump 34

You have production and test workloads that you want to deploy on Compute Engine. Production VMs need to be in a different subnet than the test VMs. All the VMs must be able to reach each other over internal IP without creating additional routes. You need to set up VPC and the 2 subnets. Which configuration meets these requirements?

A. Create a single custom VPC with 2 subnets. Create each subnet in a different region and with a different CIDR range.

B. Create a single custom VPC with 2 subnets. Create each subnet in the same region and with the same CIDR range.

C. Create 2 custom VPCs, each with a single subnet. Create each subnet is a different region and with a different CIDR range.

D. Create 2 custom VPCs, each with a single subnet. Create each subnet in the same region and with the same CIDR range.

:::details 풀이

### **영문 문제 해석**

프로덕션과 테스트 워크로드를 Compute Engine에 배포하려고 합니다. 프로덕션 VM은 테스트 VM과 다른 서브넷에 있어야 합니다. 모든 VM은 추가 라우트를 생성하지 않고도 내부 IP로 서로 접근할 수 있어야 합니다. VPC와 2개의 서브넷을 설정해야 합니다.

이는 네트워크 분리와 내부 통신을 동시에 만족하는 VPC 설계 문제입니다. 서브넷 분리와 자동 라우팅이 핵심 요구사항입니다.

### **선택지 해석**

**A. Create a single custom VPC with 2 subnets. Create each subnet in a different region and with a different CIDR range.**

-   단일 커스텀 VPC에 2개 서브넷 생성
-   각 서브넷을 다른 리전에 생성하고 다른 CIDR 범위 사용
-   단일 VPC 내의 서브넷들은 자동으로 라우팅됨
-   리전 분리로 네트워크 격리와 내부 통신 모두 만족

**B. Create a single custom VPC with 2 subnets. Create each subnet in the same region and with the same CIDR range.**

-   같은 리전에 같은 CIDR 범위로 서브넷 생성
-   같은 CIDR 범위는 IP 충돌을 일으키므로 불가능
-   기술적으로 구현 불가능한 선택지

**C. Create 2 custom VPCs, each with a single subnet. Create each subnet is a different region and with a different CIDR range.**

-   2개의 별도 VPC 생성
-   서로 다른 VPC 간에는 기본적으로 통신 불가능
-   추가 라우트나 VPC 피어링 설정 필요하므로 요구사항 위배

**D. Create 2 custom VPCs, each with a single subnet. Create each subnet in the same region and with the same CIDR range.**

-   2개의 별도 VPC + 같은 CIDR 범위
-   VPC 간 통신 불가능하고 CIDR 충돌까지 발생
-   요구사항 완전 미충족

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 프로덕션과 테스트 VM을 다른 서브넷에 분리
2. 모든 VM이 내부 IP로 상호 접근 가능
3. 추가 라우트 생성 없이 자동 라우팅

**Google Cloud VPC 네트워킹 원리:**

-   **단일 VPC**: 모든 서브넷이 자동으로 라우팅됨
-   **다중 VPC**: 기본적으로 격리되어 추가 설정 필요
-   **서브넷 간 통신**: 같은 VPC 내에서는 자동으로 가능

**CIDR 범위 고려사항:**

-   서브넷마다 고유한 CIDR 범위 필요
-   같은 CIDR 범위로는 서브넷 생성 불가능
-   다른 CIDR 범위로 네트워크 분리 달성

**리전 vs 존 고려:**

-   리전이 다르면 물리적 분리와 고가용성 제공
-   같은 VPC 내에서는 리전 간에도 자동 라우팅

**각 선택지 분석:**

-   **A**: 단일 VPC로 자동 라우팅 + 다른 CIDR로 분리 = 완벽
-   **B**: CIDR 충돌로 구현 불가능
-   **C**: 다중 VPC로 추가 설정 필요
-   **D**: 다중 VPC + CIDR 충돌로 완전 부적합

**네트워크 설계 예시:**

    VPC: production-test-vpc
    ├── Subnet 1 (us-central1): 10.0.1.0/24 (Production)
    └── Subnet 2 (us-west1): 10.0.2.0/24 (Test)

    자동 라우팅: 10.0.1.x ↔ 10.0.2.x

**정답: A**

단일 커스텀 VPC에 2개의 서브넷을 생성하되, 각각 다른 리전과 다른 CIDR 범위를 사용하는 것이 최적입니다. 이렇게 하면 프로덕션과 테스트 환경을 네트워크적으로 분리하면서도, 같은 VPC 내에서 자동 라우팅을 통해 모든 VM이 추가 설정 없이 내부 IP로 통신할 수 있습니다. Google Cloud VPC의 기본 라우팅 메커니즘을 활용한 가장 효율적인 솔루션입니다.

### VPC 기본 개념정의

-   클라우드 상에서 논리적으로 격리된 가상 네트워크
-   사용자가 정의하고 제어할 수 있는 네트워크 환경
-   온프레미스 네트워크와 유사한 구조를 클라우드에서 구현

### Google Cloud VPC 특징

-   전역(Global) 리소스
    -   하나의 VPC는 모든 Google Cloud 리전에 걸쳐 존재
    -   리전 간 자동 연결 및 라우팅 제공
-   서브넷 구조
    VPC (전역)
    ├── Subnet A (us-central1): 10.0.1.0/24
    ├── Subnet B (europe-west1): 10.0.2.0/24
    └── Subnet C (asia-east1): 10.0.3.0/24

VPC 구성 요소

1. 서브넷 (Subnets)
    - 역할: VPC 내의 IP 주소 범위 구분
    - 특징: 리전별로 생성, CIDR 블록으로 정의
    - 예시: 10.0.1.0/24 (256개 IP 주소)
2. 라우팅 테이블
    - 기능: 네트워크 트래픽 경로 결정
    - 자동 라우팅: 같은 VPC 내 서브넷 간 자동 연결
    - 커스텀 라우팅: 특정 트래픽 경로 수동 설정
3. 방화벽 규칙

```python
# 예시 방화벽 규칙
allow-ssh:
  direction: ingress
  ports: 22
  source: 0.0.0.0/0
  target: ssh-servers

allow-internal:
  direction: ingress
  source: 10.0.0.0/8
  target: all-instances
```

4. NAT 및 인터넷 게이트웨이

-   인터넷 액세스: 외부 IP 또는 Cloud NAT
-   내부 통신: VPC 내부 IP 주소 사용

### VPC 네트워킹 모델

```text
VM Instance ←→ Subnet ←→ VPC ←→ Internet/Other VPCs
```

### 통신 경로

```text
같은 서브넷: 직접 통신
다른 서브넷 (같은 VPC): 자동 라우팅
다른 VPC: VPC 피어링 또는 VPN 필요
온프레미스: Cloud VPN 또는 Interconnect
```

온프레미스(On-premises)는 기업이 자체적으로 소유하고 운영하는 물리적 데이터센터나 서버실에서 IT 인프라를 구축하고 관리하는 방식입니다.

### VPC 간 연결 방법

1. VPC 피어링
    - 용도: 같은 또는 다른 프로젝트의 VPC 연결
    - 특징: 비전이적(non-transitive), 1:1 연결
    - 제한: IP 주소 중복 불가
2. Cloud VPN
    - 용도: 온프레미스와 VPC 연결
    - 특징: IPSec VPN 터널 사용
    - 대역폭: 최대 3Gbps per tunnel
3. Cloud Interconnect
    - 용도: 고속 전용 연결
    - 특징: 물리적 전용 회선
    - 대역폭: 10Gbps ~ 100Gbps

### 보안 모델 - 계층별 보안

1. VPC 레벨: 네트워크 격리
2. 서브넷 레벨: IP 범위 분리
3. 인스턴스 레벨: 방화벽 태그
4. 애플리케이션 레벨: IAM 및 서비스 계정

### 네트워크 보안 기능

-   방화벽 규칙 (Stateful)
-   Cloud Armor (DDoS 보호)
-   Private Google Access
-   VPC Flow Logs
-   Packet Mirroring

### 네트워크 설계

1. 환경별 VPC 분리 (prod/dev/test)
2. 계층별 서브넷 구성 (web/app/db)
3. 적절한 CIDR 블록 계획
4. 방화벽 규칙 최소 권한 원칙

### 운영 관리

1. 네트워크 모니터링 설정
2. 로그 및 감사 기능 활용
3. 자동화 도구 사용 (Terraform 등)
4. 재해 복구 계획 수립

### 주요 비용 요소

-   NAT Gateway 사용량
-   외부 IP 주소 개수
-   리전 간 트래픽 전송
-   VPN 연결 시간

### 비용 절약 방법

1. 내부 IP 우선 사용
2. Cloud NAT 효율적 활용
3. 리전 간 트래픽 최소화
4. 미사용 리소스 정리

방화벽 태그(Firewall Tags)는 Google Cloud에서 VM 인스턴스를 논리적으로 그룹핑하여 방화벽 규칙을 적용하는 메커니즘입니다.

### 정의

-   VM 인스턴스에 할당하는 텍스트 라벨
-   방화벽 규칙에서 대상을 지정할 때 사용
-   IP 주소 대신 논리적 그룹으로 보안 정책 관리

### 작동 방식

1. VM에 태그 할당 (예: "web-server", "database")
2. 방화벽 규칙에서 태그를 대상으로 지정
3. 해당 태그가 있는 모든 VM에 규칙 적용

### 태그 할당

```bash
# VM 생성 시 태그 지정
gcloud compute instances create web-server-1 \
    --tags=web-servers,frontend

# 기존 VM에 태그 추가
gcloud compute instances add-tags web-server-1 \
    --tags=https-servers
```

### 방화벽 규칙 생성

```bash
# web-servers 태그가 있는 VM에 HTTP 허용
gcloud compute firewall-rules create allow-http-web \
    --allow tcp:80 \
    --source-ranges 0.0.0.0/0 \
    --target-tags web-servers

# database 태그가 있는 VM에 MySQL 허용 (web-servers에서만)
gcloud compute firewall-rules create allow-mysql-from-web \
    --allow tcp:3306 \
    --source-tags web-servers \
    --target-tags database
```

### Source Tags (소스 태그)

```bash
# web-servers 태그가 있는 VM에서 오는 트래픽만 허용
--source-tags web-servers
```

### Target Tags (대상 태그)

`--taget-tags` 태그를 갖는 모든 VM에게 해당 방화벽 규칙을 적용하겠다는 의미이다.

```bash
# database 태그가 있는 VM에만 규칙 적용
--target-tags database
```

:::

## dump 35

You have an instance group that you want to load balance. You want the load balancer to terminate the client SSL session. The instance group is used to serve a public web application over HTTPS. You want to follow Google-recommended practices. What should you do?

A. Configure an HTTP(S) load balancer.

B. Configure an internal TCP load balancer.

C. Configure an external SSL proxy load balancer.

D. Configure an external TCP proxy load balancer.

:::details 풀이

### **영문 문제 해석**

로드 밸런싱할 인스턴스 그룹이 있습니다. 로드 밸런서가 클라이언트 SSL 세션을 종료하기를 원합니다. 인스턴스 그룹은 HTTPS를 통해 공개 웹 애플리케이션을 제공하는 데 사용됩니다. Google 권장 사례를 따르고 싶습니다.

이는 공개 웹 애플리케이션에서 SSL 종료를 로드 밸런서에서 처리하는 최적의 방법을 찾는 문제입니다. SSL 종료와 HTTP/HTTPS 트래픽 처리가 핵심 요구사항입니다.

### **선택지 해석**

**A. Configure an HTTP(S) load balancer.**

-   HTTP(S) 로드 밸런서 구성
-   Layer 7 (애플리케이션 레이어) 로드 밸런서
-   HTTPS 트래픽을 기본적으로 지원하며 SSL 종료 기능 제공
-   웹 애플리케이션에 최적화된 기능들 포함 (URL 기반 라우팅, 헤더 조작 등)
-   Google Cloud의 권장 솔루션

**B. Configure an internal TCP load balancer.**

-   내부 TCP 로드 밸런서
-   내부 네트워크 전용으로 공개 웹 애플리케이션에 부적합
-   외부 인터넷에서 접근할 수 없음
-   요구사항에 맞지 않음

**C. Configure an external SSL proxy load balancer.**

-   외부 SSL 프록시 로드 밸런서
-   SSL/TLS 연결에 특화된 Layer 4 로드 밸런서
-   SSL 종료는 지원하지만 HTTP 특화 기능 부족
-   웹 애플리케이션에는 HTTP(S) 로드 밸런서가 더 적합

**D. Configure an external TCP proxy load balancer.**

-   외부 TCP 프록시 로드 밸런서
-   일반적인 TCP 연결용 Layer 4 로드 밸런서
-   SSL 종료 기능이 제한적이고 HTTPS에 최적화되지 않음
-   웹 애플리케이션에 부적합

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 공개 웹 애플리케이션 제공
2. HTTPS 프로토콜 사용
3. 로드 밸런서에서 SSL 종료
4. Google 권장 사례 준수

**SSL 종료 (SSL Termination) 개념:**

-   클라이언트와 로드 밸런서 간: HTTPS 암호화 연결
-   로드 밸런서와 백엔드 간: HTTP 평문 또는 새로운 SSL 연결
-   로드 밸런서가 SSL 인증서를 관리하고 암호화/복호화 처리

**Google Cloud 로드 밸런서 유형:**

**HTTP(S) Load Balancer:**

-   Layer 7 로드 밸런서
-   HTTP/HTTPS 트래픽에 최적화
-   자동 SSL 인증서 관리 (Google Managed SSL)
-   URL 기반 라우팅, 헤더 조작 등 웹 기능 제공
-   CDN 통합, DDoS 보호 등 웹 애플리케이션 보안 기능

**SSL Proxy Load Balancer:**

-   Layer 4 로드 밸런서
-   SSL/TLS 연결 종료 지원
-   HTTP 특화 기능 없음
-   웹 애플리케이션보다는 SSL 기반 비-HTTP 프로토콜에 적합

**각 선택지 분석:**

-   **A**: 웹 애플리케이션에 최적화된 완전한 솔루션
-   **B**: 내부 전용으로 공개 웹앱에 부적합
-   **C**: SSL 종료는 가능하지만 HTTP 최적화 부족
-   **D**: SSL 종료 기능 제한적, 웹앱에 부적합

**Google 권장 사례:**

-   웹 애플리케이션: HTTP(S) Load Balancer 사용
-   SSL 인증서: Google Managed SSL 활용
-   백엔드: Instance Groups과 Health Check 구성
-   보안: Cloud CDN과 Cloud Armor 통합

**HTTP(S) Load Balancer 장점:**

-   자동 SSL 인증서 갱신
-   전역 로드 밸런싱 (Anycast IP)
-   HTTP/2 및 gRPC 지원
-   URL 기반 트래픽 라우팅
-   Cloud CDN 통합으로 성능 향상
-   DDoS 보호 및 웹 방화벽 기능

**정답: A**

공개 웹 애플리케이션에서 SSL 종료와 함께 HTTPS 트래픽을 처리하려면 HTTP(S) 로드 밸런서를 구성해야 합니다. 이는 웹 애플리케이션에 최적화된 Layer 7 로드 밸런서로, SSL 종료 기능과 함께 URL 기반 라우팅, 자동 SSL 인증서 관리, CDN 통합 등 웹 애플리케이션에 필요한 모든 기능을 제공합니다. Google Cloud에서 HTTPS 웹 애플리케이션에 권장하는 표준 솔루션입니다.

"로드 밸런서가 클라이언트 SSL 세션을 종료한다"는 것은 암호화된 HTTPS 연결을 로드 밸런서에서 복호화하여 처리한다는 의미입니다.

SSL Termination (종료)

```text
[클라이언트] --HTTPS--> [로드밸런서] --HTTP--> [백엔드]
                     ↑
                SSL 복호화 처리
```

### 네트워크 계층 구조

```text
Layer 7 - Application  (HTTP, HTTPS, FTP)
Layer 6 - Presentation (SSL/TLS, 암호화)
Layer 5 - Session      (세션 관리)
Layer 4 - Transport    (TCP, UDP) ← Layer 4 로드밸런서
Layer 3 - Network      (IP)
Layer 2 - Data Link    (Ethernet)
Layer 1 - Physical     (케이블, 전파)
```

### Layer 4 vs Layer 7 비교

-   Layer 4 (Transport Layer):
    -   확인 정보: IP 주소, 포트 번호, TCP/UDP
    -   처리 방식: 패킷 기반 전달
    -   장점: 빠른 속도, 낮은 지연시간
    -   단점: HTTP 내용 기반 라우팅 불가
    -   예시: TCP Load Balancer, SSL Proxy Load Balancer
-   Layer 7 (Application Layer):
    -   확인 정보: HTTP 헤더, URL, 쿠키, 콘텐츠
    -   처리 방식: HTTP 요청 분석 후 처리
    -   장점: 지능적 라우팅, 콘텐츠 기반 분산
    -   단점: 높은 CPU 사용량, 상대적 지연
    -   예시: HTTP(S) Load Balancer, Application Load Balancer

:::

## dump 36

You have a web application deployed as a Managed Instance Group. You have a new version of the application to gradually deploy. Your web application is currently receiving live web traffic. You want to ensure that the available capacity does not decrease during the deployment. What should you do?

A. Perform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1.

B. Perform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0.

C. Create a new Managed Instance Group with an updated instance template. Add the group to the backend service for the load balancer. When all instances in the new Managed Instance Group are healthy, delete the old Managed Instance Group.

D. Create a new instance template with the new application version. Update the existing Managed Instance Group with the new instance template. Delete the instances in the Managed Instance Group to allow the Managed Instance Group to recreate the instance using the new instance template.

:::details 풀이

### **영문 문제 해석**

Managed Instance Group으로 배포된 웹 애플리케이션이 있습니다. 점진적으로 배포할 새 버전의 애플리케이션이 있습니다. 웹 애플리케이션은 현재 라이브 웹 트래픽을 받고 있습니다. 배포 중에 사용 가능한 용량이 감소하지 않도록 하려고 합니다.

이는 무중단 배포(Zero-downtime deployment)를 위한 롤링 업데이트 전략 문제입니다. 서비스 중단 없이 새 버전을 배포하면서 용량을 유지하는 것이 핵심입니다.

### **선택지 해석**

**A. Perform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1.**

-   maxSurge=0: 추가 인스턴스 생성 없음
-   maxUnavailable=1: 동시에 1개 인스턴스만 사용 불가
-   기존 인스턴스를 하나씩 종료 후 새로 생성
-   총 용량이 일시적으로 감소함 (예: 5개 → 4개 → 5개)

**B. Perform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0.**

-   maxSurge=1: 최대 1개 추가 인스턴스 생성 가능
-   maxUnavailable=0: 기존 인스턴스 종료 없이 진행
-   새 인스턴스를 먼저 생성하고 준비 완료 후 기존 인스턴스 교체
-   용량이 증가하거나 최소한 유지됨 (예: 5개 → 6개 → 5개)

**C. Create a new Managed Instance Group with an updated instance template. Add the group to the backend service for the load balancer. When all instances in the new Managed Instance Group are healthy, delete the old Managed Instance Group.**

-   Blue-Green 배포 방식
-   완전히 새로운 MIG 생성 후 로드 밸런서에 추가
-   모든 새 인스턴스가 준비된 후 기존 MIG 제거
-   배포 중 용량이 두 배로 증가하여 비용 증가

**D. Create a new instance template with the new application version. Update the existing Managed Instance Group with the new instance template. Delete the instances in the Managed Instance Group to allow the Managed Instance Group to recreate the instance using the new instance template.**

-   강제로 모든 인스턴스를 삭제하는 방식
-   동시에 모든 인스턴스가 재생성되어 서비스 중단 발생
-   용량이 일시적으로 0이 되는 위험

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. 점진적 배포 (Gradual deployment)
2. 라이브 트래픽 처리 중
3. 사용 가능한 용량 감소 방지
4. 무중단 서비스

**maxSurge와 maxUnavailable 매개변수:**

**maxSurge:**

-   롤링 업데이트 중 원래 크기를 초과하여 생성할 수 있는 추가 인스턴스 수
-   예: 원래 5개, maxSurge=1 → 최대 6개까지 가능

**maxUnavailable:**

-   롤링 업데이트 중 동시에 사용할 수 없는 인스턴스 수
-   예: 원래 5개, maxUnavailable=1 → 최소 4개는 유지

**용량 변화 시뮬레이션:**

**선택지 A (maxSurge=0, maxUnavailable=1):**

    초기: [v1] [v1] [v1] [v1] [v1] (5개)
    1단계: [v1] [v1] [v1] [v1] [교체중] (4개 사용 가능)
    2단계: [v1] [v1] [v1] [v1] [v2] (5개 사용 가능)
    ...
    → 용량이 일시적으로 감소

**선택지 B (maxSurge=1, maxUnavailable=0):**

    초기: [v1] [v1] [v1] [v1] [v1] (5개)
    1단계: [v1] [v1] [v1] [v1] [v1] [v2추가] (6개 사용 가능)
    2단계: [v2] [v1] [v1] [v1] [v1] (5개 사용 가능)
    ...
    → 용량이 유지되거나 증가

**각 선택지 분석:**

-   **A**: 용량 감소 발생으로 요구사항 위배
-   **B**: 용량 유지하며 점진적 배포 달성
-   **C**: 용량은 유지되지만 비용 증가, 복잡성 증가
-   **D**: 서비스 중단 위험으로 요구사항 완전 위배

**Google Cloud 권장 사례:**

-   무중단 배포를 위해 maxSurge > 0, maxUnavailable = 0 권장
-   추가 리소스 비용을 감수하고 안정성 확보
-   점진적 배포로 문제 발생 시 빠른 롤백 가능

**롤링 업데이트 명령 예시:**

    gcloud compute instance-groups managed rolling-action start-update my-mig \
        --version=template=new-template \
        --max-surge=1 \
        --max-unavailable=0

**정답: B**

라이브 트래픽을 처리하는 웹 애플리케이션의 무중단 배포를 위해서는 maxSurge=1, maxUnavailable=0으로 롤링 업데이트를 수행해야 합니다. 이렇게 하면 새 인스턴스를 먼저 생성하여 준비 상태가 된 후에 기존 인스턴스를 교체하므로, 배포 과정에서 사용 가능한 용량이 감소하지 않습니다. 일시적으로 추가 인스턴스 비용이 발생하지만 서비스 연속성과 사용자 경험을 보장할 수 있는 가장 안전한 배포 방법입니다.

:::

## dump 37

You need to grant access for three users so that they can view and edit table data on a Cloud Spanner instance. What should you do?

A. Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to the role.

B. Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to a new group. Add the group to the role.

C. Run gcloud iam roles describe roles/spanner.viewer –project my-project. Add the users to the role.

D. Run gcloud iam roles describe roles/spanner.viewer –project my-project. Add the users to a new group. Add the group to the role.

:::details 풀이

### **영문 문제 해석**

세 명의 사용자가 Cloud Spanner 인스턴스의 테이블 데이터를 보고 편집할 수 있도록 액세스 권한을 부여해야 합니다.

이는 Cloud Spanner에서 데이터 읽기와 쓰기 권한을 제공하는 적절한 IAM 역할 선택과 권한 부여 방식을 묻는 문제입니다.

### **선택지 해석**

**A. Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to the role.**

-   `roles/spanner.databaseUser` 역할 확인 후 사용자들을 직접 역할에 추가
-   개별 사용자를 직접 역할에 바인딩하는 방식
-   Google 권장 사례에 어긋남

**B. Run gcloud iam roles describe roles/spanner.databaseUser. Add the users to a new group. Add the group to the role.**

-   `roles/spanner.databaseUser` 역할 확인
-   새로운 그룹 생성 후 사용자들을 그룹에 추가
-   그룹을 역할에 바인딩하는 방식
-   Google 권장 사례에 부합

**C. Run gcloud iam roles describe roles/spanner.viewer –project my-project. Add the users to the role.**

-   `roles/spanner.viewer` 역할 사용
-   읽기 전용 권한만 제공
-   편집 권한이 없어 요구사항 미충족

**D. Run gcloud iam roles describe roles/spanner.viewer –project my-project. Add the users to a new group. Add the group to the role.**

-   `roles/spanner.viewer` 역할 + 그룹 방식
-   읽기 전용 권한만 제공
-   편집 권한이 없어 요구사항 미충족

### **문제 분석 및 정답 도출**

**핵심 요구사항:**

1. Cloud Spanner 테이블 데이터 조회 (읽기)
2. Cloud Spanner 테이블 데이터 편집 (쓰기)
3. 세 명의 사용자에게 권한 부여

**Cloud Spanner IAM 역할 비교:**

**`roles/spanner.databaseUser`:**

-   데이터베이스 연결 및 쿼리 실행 권한
-   SELECT, INSERT, UPDATE, DELETE 작업 가능
-   읽기와 쓰기 모든 권한 포함
-   요구사항 완전 충족

**`roles/spanner.viewer`:**

-   Cloud Spanner 리소스 메타데이터 조회만 가능
-   실제 테이블 데이터 접근 불가
-   읽기 전용이며 편집 권한 없음
-   요구사항 미충족

**IAM 권한 부여 모범 사례:**

**그룹 기반 권한 관리 (권장):**

-   사용자들을 그룹에 추가
-   그룹에 역할 부여
-   관리 용이성과 확장성 제공
-   권한 추적 및 감사 용이

**개별 사용자 직접 권한 부여 (비권장):**

-   사용자별로 개별 역할 바인딩
-   관리 복잡성 증가
-   확장성 부족

**실제 구현 과정:**

    # 1. 역할 권한 확인
    gcloud iam roles describe roles/spanner.databaseUser

    # 2. 그룹 생성
    gcloud identity groups create spanner-db-users@company.com

    # 3. 사용자들을 그룹에 추가
    gcloud identity groups memberships add \
        --group-email="spanner-db-users@company.com" \
        --member-email="user1@company.com"

    # 4. 그룹을 역할에 바인딩
    gcloud projects add-iam-policy-binding PROJECT_ID \
        --member="group:spanner-db-users@company.com" \
        --role="roles/spanner.databaseUser"

**각 선택지 분석:**

-   **A**: 올바른 역할이지만 개별 사용자 관리로 비효율적
-   **B**: 올바른 역할 + 그룹 기반 관리로 최적
-   **C**: 편집 권한 없는 역할로 요구사항 미충족
-   **D**: 편집 권한 없는 역할 + 그룹 방식이지만 여전히 요구사항 미충족

**Google Cloud 권장 사례:**

-   그룹을 통한 권한 관리
-   최소 권한 원칙 (필요한 권한만 부여)
-   역할 기반 접근 제어 (RBAC)
-   정기적인 권한 검토 및 감사

**정답: B**

세 명의 사용자가 Cloud Spanner 테이블 데이터를 보고 편집하려면 `roles/spanner.databaseUser` 역할이 필요합니다. 이 역할은 데이터베이스 연결과 읽기/쓰기 작업을 모두 허용합니다. Google 권장 사례에 따라 사용자들을 새로운 그룹에 추가하고 그룹을 역할에 바인딩하는 것이 관리 효율성과 확장성 측면에서 최적의 방법입니다.

:::

## dump 38

You need to create a new billing account and then link it with an existing Google Cloud Platform project. What should you do?

A. Verify that you are Project Billing Manager for the GCP project. Update the existing project to link it to the existing billing account.

B. Verify that you are Project Billing Manager for the GCP project. Create a new billing account and link the new billing account to the existing project.

C. Verify that you are Billing Administrator for the billing account. Create a new project and link the new project to the existing billing account.

D. Verify that you are Billing Administrator for the billing account. Update the existing project to link it to the existing billing account.

:::details 풀이

### 영문 문제 해석

새로운 청구 계정(billing account)을 생성하고 기존 Google Cloud Platform 프로젝트와 연결해야 합니다. 어떻게 해야 할까요?

문제의 핵심은 두 가지 작업입니다:

1. 새로운 청구 계정 생성
2. 기존 GCP 프로젝트와 연결

### 선택지 해석

**A. Project Billing Manager 권한 확인 후, 기존 프로젝트를 기존 청구 계정에 연결하도록 업데이트**

-   Project Billing Manager 역할 확인
-   기존 청구 계정 사용 (새 청구 계정 생성하지 않음)
-   문제 요구사항과 불일치

**B. Project Billing Manager 권한 확인 후, 새 청구 계정 생성하고 기존 프로젝트에 연결**

-   Project Billing Manager 역할 확인
-   새 청구 계정 생성 및 기존 프로젝트 연결
-   문제 요구사항과 일치하지만 권한이 부족

**C. Billing Administrator 권한 확인 후, 새 프로젝트 생성하고 기존 청구 계정에 연결**

-   Billing Administrator 역할 확인
-   새 프로젝트 생성 (기존 프로젝트 사용하지 않음)
-   문제 요구사항과 불일치

**D. Billing Administrator 권한 확인 후, 기존 프로젝트를 기존 청구 계정에 연결하도록 업데이트**

-   Billing Administrator 역할 확인
-   기존 청구 계정 사용 (새 청구 계정 생성하지 않음)
-   문제 요구사항과 불일치

### 문제 분석 및 정답 도출

**필요한 권한 분석:**

-   새 청구 계정 생성: Billing Administrator 권한 필요
-   프로젝트와 청구 계정 연결: Project Billing Manager 또는 Billing Administrator 권한 필요

**작업 요구사항:**

1. 새 청구 계정 생성 (기존 계정 사용 아님)
2. 기존 프로젝트 사용 (새 프로젝트 생성 아님)

새 청구 계정을 생성하려면 Billing Administrator 권한이 반드시 필요합니다. Project Billing Manager는 청구 계정 생성 권한이 없고, 기존 청구 계정을 프로젝트에 연결하는 권한만 가지고 있습니다.

선택지를 다시 검토하면, B가 작업 요구사항은 만족하지만 권한이 부족하고, 나머지 선택지들은 작업 요구사항 자체를 만족하지 않습니다.

하지만 실제 GCP에서 새 청구 계정 생성 후 기존 프로젝트 연결을 위해서는:

1. Billing Administrator 권한으로 새 청구 계정 생성
2. 해당 청구 계정을 기존 프로젝트에 연결

**정답: B**

비록 Project Billing Manager 권한으로는 청구 계정 생성이 불가능하지만, 문제에서 요구하는 작업 시퀀스(새 청구 계정 생성 + 기존 프로젝트 연결)와 가장 일치하는 선택지입니다.

:::

## dump 39

You have sensitive data stored in three Cloud Storage buckets and have enabled data access logging. You want to verify activities for a particular user for these buckets, using the fewest possible steps. You need to verify the addition of metadata labels and which files have been viewed from those buckets. What should you do?

A. Using the GCP Console, filter the Activity log to view the information.

B. Using the GCP Console, filter the Stackdriver log to view the information.

C. View the bucket in the Storage section of the GCP Console.

D. Create a trace in Stackdriver to view the information.

:::details 풀이

### 영문 문제 해석

세 개의 Cloud Storage 버킷에 민감한 데이터를 저장하고 있으며 데이터 액세스 로깅을 활성화했습니다. 최소한의 단계로 특정 사용자의 이러한 버킷에 대한 활동을 확인하려고 합니다. 메타데이터 레이블 추가와 해당 버킷에서 어떤 파일들이 조회되었는지 확인해야 합니다. 어떻게 해야 할까요?

문제의 핵심:

-   Cloud Storage 버킷 활동 추적
-   데이터 액세스 로깅이 이미 활성화됨
-   특정 사용자의 활동 확인
-   메타데이터 레이블 추가 확인
-   파일 조회 활동 확인
-   최소한의 단계로 수행

### 선택지 해석

**A. GCP Console을 사용하여 Activity log를 필터링하여 정보 확인**

-   GCP Console의 Activity 탭에서 통합된 감사 로그 제공
-   사용자별, 리소스별 간단한 필터링 옵션
-   Cloud Storage의 Admin Activity와 Data Access 로그 모두 확인 가능
-   최소한의 클릭으로 원하는 정보 확인

**B. GCP Console을 사용하여 Stackdriver log를 필터링하여 정보 확인**

-   Cloud Logging(구 Stackdriver) 인터페이스
-   복잡한 로그 쿼리 작성 필요
-   더 많은 단계와 기술적 지식 요구

**C. GCP Console의 Storage 섹션에서 버킷 확인**

-   버킷의 현재 상태와 객체 목록만 확인
-   과거 활동 이력이나 사용자별 액세스 로그 확인 불가
-   메타데이터 변경 이력 추적 불가능

**D. Stackdriver에서 trace 생성하여 정보 확인**

-   Trace는 분산 추적 도구로 애플리케이션 성능 분석용
-   Storage 버킷 액세스 패턴이나 관리 활동 추적에는 부적합

### 문제 분석 및 정답 도출

**Cloud Storage 로깅 구조:**

-   **Admin Activity logs**: 버킷 생성/삭제, 메타데이터 변경, 권한 수정 등
-   **Data Access logs**: 객체 읽기/쓰기, 다운로드/업로드 등 (별도 활성화 필요)

**문제 요구사항 분석:**

1. 메타데이터 레이블 추가 → Admin Activity logs에 기록
2. 파일 조회 → Data Access logs에 기록 (이미 활성화됨)
3. 특정 사용자 활동만 필터링
4. 최소한의 단계

**Activity log의 장점:**

-   직관적인 웹 UI 인터페이스
-   드롭다운 메뉴를 통한 간단한 필터링
-   사용자명, 서비스, 리소스별 필터 옵션
-   Admin Activity와 Data Access 로그 통합 표시
-   복잡한 쿼리 작성 불필요

Cloud Logging 인터페이스 대비 Activity log는 비기술적 사용자도 쉽게 사용할 수 있으며, "최소한의 단계"라는 요구사항에 가장 적합합니다.

**정답: A**

Activity log는 Cloud Storage의 관리 활동과 데이터 액세스 활동을 모두 추적할 수 있으며, 사용자 친화적인 필터링 옵션을 통해 특정 사용자의 메타데이터 변경과 파일 조회 활동을 효율적으로 확인할 수 있습니다.

:::

## dump 40

You need to monitor resources that are distributed over different projects in Google Cloud Platform. You want to consolidate reporting under the same Stackdriver Monitoring dashboard. What should you do?

A. Use Shared VPC to connect all projects, and link Stackdriver to one of the projects.

B. For each project, create a Stackdriver account. In each project, create a service account for that project and grant it the role of Stackdriver Account Editor in all other projects.

C. Configure a single Stackdriver account, and link all projects to the same account.

D. Configure a single Stackdriver account for one of the projects. In Stackdriver, create a Group and add the other project names as criteria for that Group.

:::details 풀이

### 영문 문제 해석

Google Cloud Platform의 여러 프로젝트에 분산된 리소스를 모니터링해야 합니다. 동일한 Stackdriver Monitoring 대시보드 하에서 보고를 통합하려고 합니다. 어떻게 해야 할까요?

문제의 핵심:

-   여러 프로젝트에 분산된 리소스 모니터링
-   단일 Stackdriver Monitoring 대시보드에서 통합 보고
-   중앙 집중식 모니터링 구성

### 선택지 해석

**A. Shared VPC를 사용하여 모든 프로젝트를 연결하고, Stackdriver를 프로젝트 중 하나에 연결**

-   Shared VPC는 네트워킹 솔루션으로 모니터링과는 별개
-   네트워크 연결이 모니터링 데이터 통합을 보장하지 않음
-   Stackdriver 계정 연결 방식이 명확하지 않음

**B. 각 프로젝트마다 Stackdriver 계정 생성하고, 각 프로젝트에서 서비스 계정을 만들어 다른 모든 프로젝트에서 Stackdriver Account Editor 역할 부여**

-   여러 개의 분리된 Stackdriver 계정 생성
-   복잡한 권한 관리 구조
-   통합 대시보드 구성이 아닌 분산된 모니터링

**C. 단일 Stackdriver 계정을 구성하고 모든 프로젝트를 동일한 계정에 연결**

-   하나의 Stackdriver 계정으로 통합 관리
-   모든 프로젝트의 메트릭을 중앙에서 수집
-   단일 대시보드에서 모든 리소스 모니터링 가능

**D. 프로젝트 중 하나에 단일 Stackdriver 계정 구성하고, Stackdriver에서 Group을 생성하여 다른 프로젝트 이름을 해당 Group의 기준으로 추가**

-   하나의 프로젝트에만 Stackdriver 연결
-   Group은 리소스 분류 도구이지 프로젝트 연결 방법이 아님
-   다른 프로젝트의 메트릭 수집 불가능

### 문제 분석 및 정답 도출

**Stackdriver Monitoring(현재 Cloud Monitoring) 구조:**

-   Monitoring은 프로젝트별로 메트릭을 수집
-   여러 프로젝트의 데이터를 통합하려면 모든 프로젝트가 동일한 Monitoring 워크스페이스에 연결되어야 함
-   워크스페이스는 여러 프로젝트의 모니터링 데이터를 통합하는 컨테이너

**멀티 프로젝트 모니터링 요구사항:**

1. 단일 Monitoring 워크스페이스 생성
2. 모든 대상 프로젝트를 해당 워크스페이스에 연결
3. 통합 대시보드에서 모든 프로젝트 리소스 확인

**각 선택지 분석:**

-   A: 네트워킹 솔루션으로 모니터링 문제 해결 시도 (부적합)
-   B: 분산된 계정 구조로 통합 불가능
-   C: 올바른 멀티 프로젝트 모니터링 구성 방법
-   D: Group 기능을 잘못 이해한 접근법

**올바른 구성 방법:**

1. 호스팅 프로젝트에서 Monitoring 워크스페이스 생성
2. 모니터링하려는 모든 프로젝트를 해당 워크스페이스에 추가
3. 필요한 권한 설정 (Monitoring Viewer/Editor)
4. 통합 대시보드에서 모든 프로젝트 리소스 모니터링

**정답: C**

단일 Stackdriver(Cloud Monitoring) 워크스페이스를 구성하고 모든 관련 프로젝트를 동일한 워크스페이스에 연결하는 것이 멀티 프로젝트 환경에서 통합 모니터링을 구현하는 표준 방법입니다.

:::

## dump 41

You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use?

A. Deployment Manager.

B. Cloud Composer.

C. Managed Instance Group.

D. Unmanaged Instance Group.

:::details 풀이

### 영문 문제 해석

Compute Engine에서 VM을 동적으로 프로비저닝하는 방법이 필요합니다. 정확한 사양은 전용 구성 파일에 있을 것입니다. Google의 권장 사례를 따르려고 합니다. 어떤 방법을 사용해야 할까요?

문제의 핵심:

-   VM의 동적 프로비저닝
-   전용 구성 파일에 사양 정의
-   Google 권장 사례 준수
-   Infrastructure as Code 접근법

### 선택지 해석

**A. Deployment Manager**

-   Google Cloud의 Infrastructure as Code (IaC) 도구
-   YAML 또는 Python 템플릿을 사용한 구성 파일 기반 배포
-   선언적 방식으로 리소스 정의 및 관리
-   VM, 네트워크, 스토리지 등 모든 GCP 리소스 지원
-   버전 관리 및 반복 가능한 배포

**B. Cloud Composer**

-   Apache Airflow 기반의 워크플로우 오케스트레이션 서비스
-   복잡한 데이터 파이프라인 및 워크플로우 관리용
-   VM 프로비저닝보다는 작업 스케줄링 및 의존성 관리에 특화
-   단순 인프라 배포에는 과도하게 복잡함

**C. Managed Instance Group (MIG)**

-   동일한 템플릿 기반의 VM 인스턴스 그룹 관리
-   자동 스케일링, 자동 복구, 로드 밸런싱 지원
-   인스턴스 템플릿 사용하지만 구성 파일 기반이 아님
-   주로 웹 애플리케이션이나 서비스의 고가용성 구현용

**D. Unmanaged Instance Group**

-   서로 다른 구성의 VM들을 수동으로 그룹화
-   자동화 기능 제한적
-   로드 밸런서 백엔드로만 사용 가능
-   동적 프로비저닝에는 부적합

### 문제 분석 및 정답 도출

**문제 요구사항 분석:**

1. **동적 프로비저닝**: 필요에 따라 자동으로 리소스 생성/삭제
2. **전용 구성 파일**: Infrastructure as Code 방식
3. **Google 권장 사례**: 공식 도구 및 모범 사례 활용

**Deployment Manager의 장점:**

-   **구성 파일 기반**: YAML/Python 템플릿으로 VM 사양 정의
-   **선언적 접근법**: 원하는 상태를 선언하면 자동으로 달성
-   **버전 관리**: 구성 파일을 Git 등으로 관리 가능
-   **반복 가능성**: 동일한 구성을 여러 환경에 배포
-   **의존성 관리**: 리소스 간 의존성 자동 처리
-   **Google 공식 도구**: GCP의 표준 IaC 솔루션

**구성 파일 예시:**
resources: - name: vm-instance
type: compute.v1.instance
properties:
zone: us-central1-a
machineType: n1-standard-1
disks: - boot: true
autoDelete: true
initializeParams:
sourceImage: projects/debian-cloud/global/images/family/debian-11

**다른 선택지들의 한계:**

-   **Cloud Composer**: 워크플로우 관리용으로 과도한 복잡성
-   **MIG**: 템플릿 기반이지만 구성 파일 중심 접근법 아님
-   **Unmanaged Instance Group**: 수동 관리로 동적 프로비저닝 불가

**정답: A**

Deployment Manager는 구성 파일 기반의 선언적 인프라 관리를 제공하는 Google의 공식 Infrastructure as Code 도구로, VM의 동적 프로비저닝과 구성 파일 기반 관리라는 요구사항을 모두 만족시키는 Google 권장 솔루션입니다.

dedicated configuration file: 전용 설정 파일

:::

## dump 42

You created an instance of SQL Server 2017 on Compute Engine to test features in the new version. You want to connect to this instance using the fewest number of steps. What should you do?

A. Install a RDP client on your desktop. Verify that a firewall rule for port 3389 exists.

B. Install a RDP client in your desktop. Set a Windows username and password in the GCP Console. Use the credentials to log in to the instance.

C. Set a Windows password in the GCP Console. Verify that a firewall rule for port 22 exists. Click the RDP button in the GCP Console and supply the credentials to log in.

D. Set a Windows username and password in the GCP Console. Verify that a firewall rule for port 3389 exists. Click the RDP button in the GCP Console, and supply the credentials to log in.

:::details 풀이

### 영문 문제 해석

테스트를 위해 Compute Engine에서 SQL Server 2017 인스턴스를 생성했습니다. 최소한의 단계로 이 인스턴스에 연결하려고 합니다. 어떻게 해야 할까요?

문제의 핵심:

-   Windows SQL Server 인스턴스에 연결
-   최소한의 단계로 연결
-   효율적인 접근 방법

### 선택지 해석

**A. 데스크톱에 RDP 클라이언트 설치하고, 포트 3389에 대한 방화벽 규칙 존재 확인**

-   RDP 클라이언트 설치
-   방화벽 규칙만 확인
-   **Windows 인증 정보 설정 누락** (연결 불가능)

**B. 데스크톱에 RDP 클라이언트 설치하고, GCP Console에서 Windows 사용자명과 비밀번호 설정 후 해당 인증 정보로 인스턴스 로그인**

-   RDP 클라이언트 설치
-   Windows 인증 정보 설정
-   직접 연결 (2가지 주요 작업)

**C. GCP Console에서 Windows 비밀번호 설정하고, 포트 22에 대한 방화벽 규칙 확인 후, GCP Console의 RDP 버튼 클릭하여 인증 정보 제공**

-   **포트 22는 SSH용** (Windows는 포트 3389 사용)
-   잘못된 포트로 인한 연결 실패

**D. GCP Console에서 Windows 사용자명과 비밀번호 설정하고, 포트 3389에 대한 방화벽 규칙 확인 후, GCP Console의 RDP 버튼 클릭하여 인증 정보 제공**

-   Windows 인증 정보 설정
-   방화벽 규칙 확인 (추가 단계)
-   Console RDP 버튼 클릭
-   인증 정보 다시 입력 (4가지 작업)

### 문제 분석 및 정답 도출

**핵심 요소:**

1. **Windows 인스턴스는 기본적으로 RDP가 허용됨** (`default-allow-rdp` 규칙)
2. **방화벽 확인은 실제로 불필요한 단계**
3. **"fewest steps"는 실제 수행해야 할 작업 수를 의미**

**선택지 B의 효율성:**

-   **2가지 주요 작업**: RDP 클라이언트 설치 + 인증 정보 설정
-   **불필요한 단계 제거**: 방화벽 확인 생략 (이미 기본 허용)
-   **직접적인 연결**: 중간 단계 없이 바로 인스턴스 접근

**선택지 D의 단점:**

-   **추가 확인 단계**: 방화벽 규칙 확인 (실제로는 불필요)
-   **Console 경유**: RDP 버튼 클릭이라는 중간 단계
-   **인증 정보 중복 입력**: 설정 후 다시 입력

**실제 단계 비교:**

-   **B**: 클라이언트 설치 → 인증 정보 설정 → 연결
-   **D**: 인증 정보 설정 → 방화벽 확인 → Console 접근 → 인증 정보 입력

**정답: B**

Windows 인스턴스는 기본적으로 RDP가 허용되므로 방화벽 확인이 불필요하며, RDP 클라이언트를 직접 사용하는 것이 Console을 경유하는 것보다 더 적은 단계로 연결할 수 있습니다.

### RDP란?

**RDP (Remote Desktop Protocol)**는 원격 데스크톱 연결을 위한 프로토콜입니다.

-   Remote Desktop Protocol의 줄임말
-   Microsoft에서 개발한 원격 데스크톱 접속 프로토콜
    네트워크를 통해 다른 컴퓨터의 데스크톱에 원격으로 접속할 수 있게 해주는 기술

### 주요 특징

-   포트: 기본적으로 3389번 포트 사용
-   운영체제: 주로 Windows 시스템에서 사용
-   GUI 접근: 원격지 컴퓨터의 전체 데스크톱 화면을 로컬에서 조작 가능

### Linux의 SSH vs Windows의 RDP

-   SSH: 리눅스 원격 접속, 주로 명령줄 인터페이스, 포트 22
-   RDP: 윈도우 원격 접속, 그래픽 데스크톱 인터페이스, 포트 3389

### RDP 클라이언트 예시

-   Windows: 내장 "원격 데스크톱 연결" 프로그램
-   Mac: Microsoft Remote Desktop
-   Linux: Remmina, xfreerdp 등
    웹 브라우저: GCP Console의 내장 RDP 클라이언트

### GCP에서 RDP

-   Windows VM 인스턴스에 접속할 때 사용
-   SQL Server, IIS 등 Windows 기반 서비스 관리
-   그래픽 인터페이스를 통한 Windows 관리 작업

:::

## dump 43

ou are the organization and billing administrator for your company. The engineering team has the Project Creator role on the organization. You do not want the engineering team to be able to link projects to the billing account. Only the finance team should be able to link a project to a billing account, but they should not be able to make any other changes to projects. What should you do?

A. Assign the finance team only the Billing Account User role on the billing account.

B. Assign the engineering team only the Billing Account User role on the billing account.

C. Assign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.

D. Assign the engineering team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.

:::details 풀이

### 영문 문제 해석

당신은 회사의 조직 및 청구 관리자입니다. 엔지니어링 팀은 조직에서 Project Creator 역할을 가지고 있습니다. 엔지니어링 팀이 프로젝트를 청구 계정에 연결할 수 있게 하고 싶지 않습니다. 오직 재무팀만 프로젝트를 청구 계정에 연결할 수 있어야 하지만, 프로젝트에 다른 변경사항은 만들 수 없어야 합니다. 어떻게 해야 할까요?

문제의 핵심:

-   엔지니어링 팀: 프로젝트 생성은 가능하지만 청구 계정 연결은 불가능
-   재무팀: 프로젝트를 청구 계정에 연결만 가능, 다른 프로젝트 변경은 불가능
-   역할 분리를 통한 청구 권한 제어

### 선택지 해석

**A. 재무팀에게만 청구 계정에서 Billing Account User 역할 할당**

-   Billing Account User: 청구 계정을 프로젝트에 연결할 수 있는 권한
-   재무팀만 청구 계정 사용 가능
-   엔지니어링 팀은 청구 계정에 접근 불가능하여 연결 불가

**B. 엔지니어링 팀에게만 청구 계정에서 Billing Account User 역할 할당**

-   엔지니어링 팀이 청구 계정 연결 가능
-   요구사항과 반대

**C. 재무팀에게 청구 계정에서 Billing Account User 역할과 조직에서 Project Billing Manager 역할 할당**

-   추가적인 Project Billing Manager 권한 부여
-   "프로젝트에 다른 변경사항을 만들 수 없어야 한다"는 요구사항 위배 가능성

**D. 엔지니어링 팀에게 청구 계정에서 Billing Account User 역할과 조직에서 Project Billing Manager 역할 할당**

-   엔지니어링 팀이 청구 연결 가능
-   요구사항과 완전히 반대

### 문제 분석 및 정답 도출

**핵심 이해:**
문제에서 "당신은 조직 및 청구 관리자"라고 명시되어 있습니다. 이는 문제 해결자가 이미 **조직 레벨에서 모든 프로젝트에 대한 관리 권한**을 가지고 있음을 의미합니다.

**Project Creator 역할의 권한:**

-   프로젝트 생성 가능
-   **생성한 프로젝트에 대한 Owner 권한 자동 취득**
-   Owner 권한에는 Project Billing Manager 권한이 포함됨

**청구 계정 연결 조건 재분석:**
프로젝트를 청구 계정에 연결하려면:

1. **청구 계정에 대한 접근 권한** (Billing Account User)
2. **프로젝트에 대한 청구 설정 권한** (Project Billing Manager 또는 상위)

**현재 권한 상황:**

-   **엔지니어링 팀**: Project Creator → 프로젝트 생성 시 Owner 권한 취득 (청구 설정 가능)
-   **재무팀**: 현재 권한 없음

**A 선택지가 올바른 이유:**

-   재무팀에게만 **Billing Account User** 부여
-   엔지니어링 팀은 청구 계정에 접근할 수 없어서 연결 불가능
-   재무팀은 청구 계정 접근 가능하지만, 개별 프로젝트 권한은 별도로 부여받아야 함
-   **조직 관리자(문제 해결자)가 필요에 따라 재무팀에게 특정 프로젝트의 청구 권한만 선별적으로 부여 가능**

**"다른 변경사항을 만들 수 없어야 한다"는 요구사항:**

-   Billing Account User 자체로는 프로젝트 변경 권한이 없음
-   조직 관리자가 필요한 경우에만 최소 권한으로 부여 가능

**정답: A**

Billing Account User 역할만 부여하면 재무팀은 청구 계정에 접근할 수 있지만, 개별 프로젝트 권한은 조직 관리자가 필요에 따라 최소한으로 부여할 수 있어 권한 분리 원칙을 지킬 수 있습니다.

### Billing 관련 역할들

1. Billing Account Administrator

-   청구 계정에 대한 모든 권한
-   청구 계정 생성, 삭제, 수정
-   다른 사용자에게 청구 권한 부여
-   결제 수단 관리

2. Billing Account User

-   청구 계정을 프로젝트에 연결할 수 있는 권한
-   청구 계정 자체는 수정 불가
-   청구 정보 조회 가능

3. Project Billing Manager

-   특정 프로젝트의 청구 설정 관리
-   프로젝트의 청구 계정 연결/해제
-   청구 정보 조회
-   프로젝트의 다른 설정은 변경 불가

4. Billing Account Viewer

-   청구 계정 정보 조회만 가능
-   변경 권한 없음

:::

## dump 44

You are creating a Google Kubernetes Engine (GKE) cluster with a cluster autoscaler feature enabled. You need to make sure that each node of the cluster will run a monitoring pod that sends container metrics to a third-party monitoring solution. What should you do?

A. Deploy the monitoring pod in a StatefulSet object.

B. Deploy the monitoring pod in a DaemonSet object.

C. Reference the monitoring pod in a Deployment object.

D. Reference the monitoring pod in a cluster initializer at the GKE cluster creation time.

:::details 풀이

### 영문 문제 해석

클러스터 자동 스케일링 기능이 활성화된 Google Kubernetes Engine (GKE) 클러스터를 생성하고 있습니다. 클러스터의 각 노드에서 컨테이너 메트릭을 타사 모니터링 솔루션으로 전송하는 모니터링 파드가 실행되도록 해야 합니다. 어떻게 해야 할까요?

문제의 핵심:

-   GKE 클러스터의 **각 노드**에서 모니터링 파드 실행
-   클러스터 자동 스케일링 환경 (노드 수가 동적으로 변경)
-   컨테이너 메트릭을 타사 모니터링 솔루션으로 전송
-   모든 노드에서 일관된 모니터링 보장

### 선택지 해석

**A. StatefulSet 객체에서 모니터링 파드 배포**

-   StatefulSet: 상태를 가진 애플리케이션용 (데이터베이스 등)
-   고정된 네트워크 식별자와 영구 스토리지 제공
-   특정 노드 배치 보장 없음
-   각 노드마다 하나씩 실행 보장 없음

**B. DaemonSet 객체에서 모니터링 파드 배포**

-   DaemonSet: **모든 노드에서 파드의 사본을 실행**
-   새 노드 추가 시 자동으로 파드 배포
-   노드 제거 시 해당 파드 자동 제거
-   노드별 시스템 서비스에 최적화

**C. Deployment 객체에서 모니터링 파드 참조**

-   Deployment: 일반적인 애플리케이션 배포용
-   지정된 수의 파드 복제본 유지
-   특정 노드 배치 제어 없음
-   각 노드마다 하나씩 실행 보장 없음

**D. GKE 클러스터 생성 시 클러스터 이니셜라이저에서 모니터링 파드 참조**

-   클러스터 이니셜라이저는 클러스터 생성 시점의 초기 설정
-   동적 노드 추가/제거에 대응하지 않음
-   자동 스케일링 환경에서 새 노드에 파드 배포 보장 없음

### 문제 분석 및 정답 도출

**요구사항 분석:**

1. **각 노드에서 실행**: 모든 노드에 모니터링 파드 필요
2. **클러스터 자동 스케일링**: 노드 수가 동적으로 변경
3. **컨테이너 메트릭 수집**: 노드 레벨 모니터링 에이전트 역할
4. **타사 모니터링**: 외부 시스템으로 메트릭 전송

**DaemonSet의 특징:**

-   **노드별 파드 보장**: 모든 노드(또는 선택된 노드)에서 정확히 하나의 파드 실행
-   **자동 스케일링 대응**: 새 노드 추가 시 자동으로 파드 배포
-   **노드 제거 대응**: 노드 삭제 시 해당 파드도 자동 정리
-   **시스템 서비스 최적화**: 로그 수집, 모니터링, 네트워크 에이전트 등에 적합

**모니터링 에이전트의 일반적인 패턴:**

-   Prometheus Node Exporter
-   Fluentd/Fluent Bit 로그 수집기
-   DataDog Agent
-   New Relic Infrastructure Agent

이러한 모든 도구들은 **DaemonSet**으로 배포되어 각 노드에서 시스템 메트릭을 수집합니다.

**다른 선택지들의 한계:**

-   **StatefulSet**: 상태 유지가 목적, 노드별 배포 보장 없음
-   **Deployment**: 복제본 수 기반, 특정 노드 배치 제어 없음
-   **클러스터 이니셜라이저**: 정적 설정, 동적 노드 변경 대응 불가

**정답: B**

DaemonSet은 클러스터의 모든 노드(또는 선택된 노드)에서 정확히 하나의 파드가 실행되도록 보장하며, 클러스터 자동 스케일링으로 인한 노드 추가/제거에 자동으로 대응하여 각 노드에서 일관된 모니터링을 제공합니다.

:::

### GKE 노드, 팟 개념

**GKE (Google Kubernetes Engine)**에서 노드와 파드는 Kubernetes의 핵심 개념입니다.

-   노드 (Node)
    -   물리적 또는 가상 머신: GKE에서는 Google Compute Engine VM
    -   클러스터의 워커: 실제로 컨테이너가 실행되는 서버
    -   여러 파드를 호스팅: 하나의 노드에 여러 파드 실행 가능
    -   자동 스케일링: 부하에 따라 노드 개수 자동 증감

```text
클러스터 = 여러 노드의 집합
노드 1: VM (4 CPU, 16GB RAM)
노드 2: VM (4 CPU, 16GB RAM)
노드 3: VM (4 CPU, 16GB RAM)
```

-   파드 (Pod)

    -   가장 작은 배포 단위: 하나 이상의 컨테이너를 포함
    -   공유 리소스: 같은 파드 내 컨테이너들은 네트워크와 스토리지 공유
    -   일시적: 파드는 생성/삭제가 자유로움
    -   노드 위에서 실행: 스케줄러가 적절한 노드에 배치

-   파드 = 하나 또는 여러 컨테이너의 묶음
-   파드 A: [웹서버 컨테이너]
-   파드 B: [웹서버 컨테이너 + 로그수집 컨테이너]

### 관계 구조

```
클러스터
├── 노드 1 (VM)
│ ├── 파드 A (웹서버)
│ └── 파드 B (데이터베이스)
├── 노드 2 (VM)
│ ├── 파드 C (API서버)
│ └── 파드 D (모니터링)
└── 노드 3 (VM)
└── 파드 E (백그라운드 작업)

노드 (하드웨어/VM 인스턴스)
├── 운영체제 (Ubuntu/CentOS)
├── Container Runtime (Docker/containerd)
└── 실행 중인 소프트웨어들:
    ├── Pod A (nginx 프로세스)
    ├── Pod B (mysql 프로세스)
    ├── Pod C (api-server 프로세스)
    └── Pod D (monitoring 프로세스)
```

이렇게 노드는 하드웨어, 파드는 애플리케이션이라고 생각하면 됩니다.

GKE에서 객체(Object)란 **"Kubernetes에서 관리하는 모든 것"**을 객체라고 합니다.

1. Pod

-   가장 기본 단위: 하나 이상의 컨테이너 묶음
-   공유 리소스: 네트워크, 스토리지 공유
-   일시적: 직접 생성하기보다는 다른 객체를 통해 관리

2. Deployment

-   가장 일반적: 상태가 없는 애플리케이션 배포
-   복제본 관리: 지정된 수의 파드 복제본 유지
-   롤링 업데이트: 무중단 배포 지원
-   용도: 웹 서버, API 서버, 마이크로서비스

3. StatefulSet

-   상태 유지: 고유한 네트워크 식별자와 영구 스토리지
-   순서 보장: 파드 생성/삭제 순서 제어
-   용도: 데이터베이스, 분산 시스템 (MySQL, MongoDB, Elasticsearch)

4. DaemonSet

-   노드별 배포: 모든 노드(또는 선택된 노드)에 파드 하나씩
-   시스템 서비스: 노드 레벨 작업 수행
-   용도: 로그 수집, 모니터링 에이전트, 네트워크 플러그인

5. Job

-   일회성 작업: 완료 후 종료되는 작업
-   성공 보장: 지정된 수의 파드가 성공적으로 완료될 때까지 재시도
-   용도: 배치 처리, 데이터 마이그레이션

6. CronJob

-   스케줄된 작업: 정해진 시간에 반복 실행
-   cron 문법: Unix cron과 동일한 스케줄링
-   용도: 백업, 정기 데이터 처리, 리포트 생성

7. Service

-   로드 밸런싱: 파드들에 대한 안정적인 엔드포인트 제공
-   서비스 디스커버리: 파드 IP가 변경되어도 일관된 접근
    -   ClusterIP: 클러스터 내부 통신
    -   NodePort: 노드 포트를 통한 외부 접근
    -   LoadBalancer: 외부 로드 밸런서 생성

8. Ingress

-   HTTP/HTTPS 라우팅: 외부에서 클러스터 서비스로 접근
-   도메인 기반 라우팅: 여러 서비스를 하나의 IP로 통합
-   SSL/TLS 지원: 인증서 관리

9. ConfigMap

-   설정 데이터: 애플리케이션 설정을 코드와 분리
-   비민감 정보: 환경 변수, 설정 파일
-   용도: 데이터베이스 URL, API 엔드포인트

10. Secret

-   민감한 데이터: 암호화되어 저장
-   보안 정보: 비밀번호, API 키, 인증서
-   타입들: Opaque, TLS, Docker Registry

11. PersistentVolume (PV)

-   저장소 리소스: 클러스터 레벨의 스토리지
-   독립적: 파드 생명주기와 무관

12. PersistentVolumeClaim (PVC)

-   저장소 요청: 파드가 PV를 사용하기 위한 요청
-   동적 프로비저닝: 자동으로 PV 생성 가능

13. ServiceAccount

-   파드 인증: 파드가 Kubernetes API에 접근할 때 사용
-   권한 제어: RBAC와 연동

14. Role / ClusterRole

-   권한 정의: 어떤 리소스에 어떤 작업을 할 수 있는지 정의
-   범위: Role(네임스페이스), ClusterRole(클러스터 전체)

15. RoleBinding / ClusterRoleBinding

-   권한 할당: 사용자/그룹/ServiceAccount에 Role 연결

```text
Kubernetes 객체들

1. 파드 관련
   ├── Pod (실제 실행 단위)
   └── 컨트롤러들 (파드의 성격/역할 정의)
       ├── Deployment (지속 서비스)
       ├── Job (일회성 작업)
       ├── DaemonSet (노드별 시스템 서비스)
       └── StatefulSet (상태 보존)

2. 인프라 지원 객체들
   ├── Service (네트워킹)
   ├── ConfigMap (설정)
   ├── Secret (보안)
   └── PersistentVolume (저장소)
```

## dump 45

You create a new Google Kubernetes Engine (GKE) cluster and want to make sure that it always runs a supported and stable version of Kubernetes. What should you do?

A. Enable the Node Auto-Repair feature for your GKE cluster.

B. Enable the Node Auto-Upgrades feature for your GKE cluster.

C. Select the latest available cluster version for your GKE cluster.

D. Select 'Container-Optimized OS (cos)' as a node image for your GKE cluster.

:::details 풀이

### 영문 문제 해석

새로운 Google Kubernetes Engine (GKE) 클러스터를 생성하고 항상 지원되고 안정적인 Kubernetes 버전이 실행되도록 하려고 합니다. 어떻게 해야 할까요?

문제의 핵심:

-   새 GKE 클러스터 생성
-   항상 지원되고 안정적인 Kubernetes 버전 유지
-   지속적인 버전 관리와 업데이트

### 선택지 해석

**A. GKE 클러스터에서 Node Auto-Repair 기능 활성화**

-   Node Auto-Repair: 비정상 상태의 노드를 자동으로 감지하고 수리/교체
-   노드 하드웨어/소프트웨어 문제 해결
-   Kubernetes 버전 업데이트와는 직접적 관련 없음
-   클러스터 안정성 향상이지만 버전 관리 기능 아님

**B. GKE 클러스터에서 Node Auto-Upgrades 기능 활성화**

-   Node Auto-Upgrades: 노드의 Kubernetes 버전을 자동으로 업그레이드
-   Google이 권장하는 안정적인 버전으로 자동 업데이트
-   보안 패치 및 버그 수정 자동 적용
-   지원되는 버전 범위 내에서 자동 유지

**C. GKE 클러스터에서 사용 가능한 최신 클러스터 버전 선택**

-   최신 버전 선택은 일회성 설정
-   시간이 지나면 더 새로운 버전이 출시되어 상대적으로 구버전이 됨
-   지속적인 업데이트를 보장하지 않음
-   최신 버전이 항상 가장 안정적이지는 않음

**D. 노드 이미지로 'Container-Optimized OS (COS)' 선택**

-   COS: Google에서 개발한 컨테이너 최적화 운영체제
-   보안 및 성능 최적화
-   자동 업데이트 지원
-   Kubernetes 버전 관리가 아닌 OS 레벨 최적화

### 문제 분석 및 정답 도출

**요구사항 분석:**

-   **"항상 지원되고 안정적인 버전"**: 지속적인 버전 관리 필요
-   **자동화**: 수동 개입 없이 버전 유지
-   **안정성**: 검증된 버전으로의 업그레이드

**GKE 버전 관리 구조:**

-   **Control Plane**: Google이 자동으로 관리 및 업그레이드
-   **Node Pool**: 사용자가 관리하거나 자동 업그레이드 설정 가능
-   **Release Channel**: Rapid, Regular, Stable 채널로 업데이트 정책 관리

**각 선택지의 효과:**

-   **A (Auto-Repair)**: 노드 장애 복구, 버전 업데이트 아님
-   **B (Auto-Upgrades)**: Kubernetes 버전 자동 업데이트 ✓
-   **C (최신 버전)**: 초기 설정만, 지속성 없음
-   **D (COS)**: OS 최적화, Kubernetes 버전과 별개

**Node Auto-Upgrades의 동작:**

1. Google이 새로운 안정적 버전 릴리스
2. 자동으로 노드 업그레이드 스케줄링
3. 무중단 롤링 업데이트 수행
4. 지원되지 않는 구버전 방지

**Best Practice:**

-   Control Plane은 Google이 자동 관리
-   Node Auto-Upgrades 활성화로 노드도 자동 업데이트
-   Release Channel 설정으로 업데이트 정책 조절

**정답: B**

Node Auto-Upgrades 기능을 활성화하면 GKE 클러스터의 노드들이 Google에서 검증한 안정적이고 지원되는 Kubernetes 버전으로 자동 업그레이드되어, 수동 개입 없이 지속적으로 최신 상태를 유지할 수 있습니다.

:::

### 쿠버네티스 클러스터 / 노드 구분

쿠버네티스 클러스터는 노드들의 집합을 의미한다.

```text
Kubernetes 클러스터
├── 마스터 노드들 (Control Plane)
│   ├── 마스터 노드 1
│   ├── 마스터 노드 2 (고가용성)
│   └── 마스터 노드 3
└── 워커 노드들 (Node Pool)
    ├── 워커 노드 1
    ├── 워커 노드 2
    ├── 워커 노드 3
    └── 워커 노드 N개...
```

-   마스터 노드 (Control Plane)
-   역할: 클러스터 관리 및 제어

    -   API 서버 실행
    -   스케줄링 결정
    -   클러스터 상태 저장
    -   전체 클러스터 조율

-   워커 노드
-   역할: 실제 애플리케이션(파드) 실행

    -   파드 실행 환경 제공
    -   컨테이너 런타임 실행
    -   네트워킹 처리
    -   마스터 노드와 통신

-   물리적 관점

    -   클러스터 = 여러 대의 서버/VM들이 네트워크로 연결된 집합

-   예시

    -   서버 1 (마스터): 클러스터 관리
    -   서버 2 (워커): 웹 애플리케이션 파드 실행
    -   서버 3 (워커): 데이터베이스 파드 실행
    -   서버 4 (워커): API 서버 파드 실행

-   GKE 클러스터 생성 시

```text
✅ Google이 자동 생성:

-   마스터 노드들 (사용자에게 안 보임)
-   네트워킹 설정
-   보안 설정

✅ 사용자가 설정:

-   워커 노드 개수 및 사양
-   노드 풀 구성
-   클러스터 이름 및 위치
```

### GKE 버전 관리 구조

```text
GKE 클러스터
├── Control Plane (구글 자동 관리)
│   └── Kubernetes 마스터 구성 요소들
├── Node Pool (사용자 관리 선택)
│   ├── Node Pool A (웹서버용)
│   ├── Node Pool B (데이터베이스용)
│   └── Node Pool C (배치작업용)
└── Release Channel (업데이트 정책)
    ├── Rapid (빠른 업데이트)
    ├── Regular (균형적 업데이트)
    └── Stable (안정적 업데이트)
```

1. Control Plane (컨트롤 플레인)

-   위치: Google Cloud의 관리 영역 (사용자에게 안 보임)
-   역할: 클러스터의 "두뇌" 역할
-   Control Plane 구성 요소:

```text
┌─────────────────────────────────┐
│  API Server (kubectl 받는 곳)    │
│  etcd (모든 데이터 저장소)        │
│  Scheduler (파드 배치 결정)      │
│  Controller Manager (상태 관리)  │
└─────────────────────────────────┘
        ↓ (명령 전달)
   Worker Nodes들
```

-   버전 관리: Google이 100% 자동 관리
    -   사용자 개입 불가능
    -   보안 패치 즉시 적용
    -   24시간 무중단 업데이트

2. Node Pool (노드 풀)
    - 정의: 같은 설정을 가진 워커 노드들의 그룹

```
클러스터
├── Node Pool "웹서버"
│ ├── 노드 1 (n1-standard-2)
│ ├── 노드 2 (n1-standard-2)
│ └── 노드 3 (n1-standard-2)
├── Node Pool "데이터베이스"
│ ├── 노드 A (n1-highmem-4)
│ └── 노드 B (n1-highmem-4)
└── Node Pool "GPU 작업"
└── 노드 X (n1-standard-1 + GPU)
```

-   각 노드 풀별로 다른 설정 가능
    -   머신 타입 (CPU/메모리)
    -   디스크 크기
    -   Kubernetes 버전 ← 이게 중요!
    -   자동 업그레이드 설정

3. Release Channel (릴리즈 채널)
    - 클러스터 전체의 업데이트 정책을 결정:

```text
Release Channel 선택
↓
┌─────────────────────────┐
│ Rapid Channel │ → 최신 기능 빠르게
│ (주 1-2회 업데이트) │ (개발/테스트용)
├─────────────────────────┤
│ Regular Channel │ → 안정성과 혁신 균형
│ (월 1-2회 업데이트) │ (일반 운영용)
├─────────────────────────┤
│ Stable Channel │ → 검증된 버전만
│ (분기 1회 업데이트) │ (중요 운영용)
└─────────────────────────┘
```

-   세 요소의 관계

```text
Release Channel (정책)
↓
Control Plane ← 자동 적용 (Google 관리)
↓
Node Pool ← 선택적 적용 (Auto-Upgrade ON/OFF)
```

### 사용자 관리 포인트

-   사용자가 제어할 수 있는 것

    -   Release Channel 선택 (클러스터 생성 시)
    -   Node Pool의 Auto-Upgrade 설정 (풀별로)
    -   수동 업그레이드 실행 (필요시)

-   Google이 자동 관리하는 것
    -   Control Plane 업데이트 (항상 자동)
    -   보안 패치 (긴급시 강제 적용)

```text
Release Channel의 역할:
"이 클러스터에서는 이런 버전들만 사용할 수 있어요"

Kubernetes 전체 릴리즈: v1.29.0, v1.28.5, v1.28.4, v1.27.8...
                    ↓ (Release Channel 필터링)
Rapid Channel 리스트:   [v1.29.0, v1.28.5, v1.28.4]    ← 최신 버전들
Regular Channel 리스트: [v1.28.5, v1.28.4, v1.27.8]    ← 검증된 버전들
Stable Channel 리스트:  [v1.28.4, v1.27.8, v1.27.7]    ← 안정화된 버전들
```

Auto-upgrade on off 여부에 따라 릴리즈 채널의 버전을 적용할지 여부를 결정하게 된다.

버전이 노드풀이 적용되면 각 노드 VM OS레벨의 변화가 있을 수도 있고, 노드의 파드 내부 정책들이 변경될 수도 있다. 오브젝트의 변화가 있을 수도 있는 등 GKE 버전 변경에 따른 세부 수정사항은 다양하다.

## dump 46
