---
title: Google Cloud Platform - GCP 덤프 정리
sidebarDepth: 1
---

## dump 1

Every employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do?

A. Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance.
B. Ask each member of the team to generate a new SSH key pair and to send you their public key. Use a configuration management tool to deploy those keys on each instance.
C. Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the ג€compute.osAdminLoginג€ role to the Google group corresponding to this team.
D. Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance.

:::details 풀이

문제 해석) 회사의 모든 직원이 Google 계정을 가지고 있습니다. 운영팀은 Compute Engine에서 많은 수의 인스턴스를 관리해야 합니다. 이 팀의 각 구성원은 서버에 대한 관리자 액세스 권한만 필요합니다. 보안팀은 자격 증명 배포가 운영상 효율적이어야 하며 누가 특정 인스턴스에 액세스했는지 확인할 수 있어야 한다고 요구합니다. 어떻게 해야 할까요?

서버에 대한 관리자 액세스 권한이라 함은, Compute Engine 서버 내에서 시스템 관리 작업에 대한 권한을 의미한다. 리소스 관리 권한이 아닌 것을 기억해야 한다.

오답 정리

-   A(오답): 새로운 SSH 키 쌍을 생성하고, 개인 키를 팀의 각 구성원에게 제공하며, 각 인스턴스의 메타데이터에 공개 키를 구성
    -   동일한 개인 키를 공유하면 누가 접근했는지 추적 불가
-   B(오답): 각 팀 구성원이 새로운 SSH 키 쌍을 생성하고 공개 키를 보내도록 하여, 구성 관리 도구로 각 인스턴스에 배포
    -   수동 배포 과정 비효율 / 확장성 저하
-   **C(정답)**: 각 팀 구성원이 새로운 SSH 키 쌍을 생성하고 Google 계정에 공개 키를 추가하도록 하고, 해당 팀에 대응하는 Google 그룹에 'compute.osAdminLogin' 역할을 부여
    -   OS Login을 통해 Google 계정과 SSH 키가 연동됨
    -   각 사용자를 개별적으로 추적 가능
    -   그룹 기반 권한 관리로 효율적
    -   Google의 감사 로그를 통해 접근 기록 추적 가능
-   D(오답): 새로운 SSH 키 쌍을 생성하고 개인 키를 각 구성원에게 제공하며, 프로젝트 전체 공개 SSH 키로 구성
    -   A와 동일하게 개별 사용자 추적 불가능

:::

## dump 2

```
You need to create a custom VPC with a single subnet. The subnet's range must be as large as possible. Which range should you use?

A. 0.0.0.0/0
B. 10.0.0.0/8
C. 172.16.0.0/12
D. 192.168.0.0/16
```

:::details 풀이

사용자 정의 VPC를 단일 서브넷으로 생성해야 합니다. 서브넷의 범위는 가능한 한 커야 합니다. 어떤 범위를 사용해야 할까요?

CIDR이란 IP 주소를 효율적으로 할당하고 라우팅하기 위한 방법이다. (`IP주소/프리픽스길이`)

A. 0.0.0.0/0

의미: 전체 IPv4 주소 공간 (모든 IP 주소)
범위: 0.0.0.0 ~ 255.255.255.255
주소 개수: 약 43억 개

B. 10.0.0.0/8 ⭐

의미: RFC 1918 사설 IP 대역 중 Class A
범위: 10.0.0.0 ~ 10.255.255.255
주소 개수: 약 1,677만 개

C. 172.16.0.0/12

의미: RFC 1918 사설 IP 대역 중 Class B
범위: 172.16.0.0 ~ 172.31.255.255
주소 개수: 약 104만 개

D. 192.168.0.0/16

의미: RFC 1918 사설 IP 대역 중 Class C
범위: 192.168.0.0 ~ 192.168.255.255
주소 개수: 약 6만 5천 개

CIDR 표기에서 192.168.1.100/24의 앞부분은 IP주소, 뒷부분은 서브넷마스크 길이이다. 192, 168과 같은 각 부분은 옥텟이라고 불린다. 각 옥텟마다 0~255까지 할당 가능한데, 이는 비트 기준 00000000 ~ 11111111로 값이 변할 수 있다는 것을 가리킨다.

서브넷 마스크 길이는 1의 갯수를 의미하고, /24면 비트 1이 24개가 고정이라는 것을 의미한다.

즉 192.168.1이 고정 주소이고, 나머지 100 주소값이 변할 수 있음을 의미한다.

:::

## dump 3

You want to select and configure a cost-effective solution for relational data on Google Cloud Platform. You are working with a small set of operational data in one geographic location. You need to support point-in-time recovery. What should you do?

-   A. Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected.
-   B. Select Cloud SQL (MySQL). Select the create failover replicas option.
-   C. Select Cloud Spanner. Set up your instance with 2 nodes.
-   D. Select Cloud Spanner. Set up your instance as multi-regional.

:::details 풀이

#### 영문 문제 해석

**문제**: "Google Cloud Platform에서 관계형 데이터를 위한 비용 효율적인 솔루션을 선택하고 구성하려고 합니다. 한 지리적 위치에서 소규모 운영 데이터로 작업하고 있습니다. 특정 시점 복구(point-in-time recovery)를 지원해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   관계형 데이터베이스
-   **비용 효율적인** 솔루션
-   **소규모** 운영 데이터
-   **단일 지리적 위치**
-   **Point-in-time Recovery (PITR)** 지원 필수

#### 선택지 해석

**A. Cloud SQL (MySQL)을 선택하고, enable binary logging 옵션이 선택되었는지 확인**

-   Binary logging: MySQL의 변경 사항을 로그로 기록
-   PITR을 위해 필수적인 기능

**B. Cloud SQL (MySQL)을 선택하고, create failover replicas 옵션을 선택**

-   Failover replicas: 고가용성을 위한 복제본 생성
-   주로 장애 대응용, PITR과는 다른 개념

**C. Cloud Spanner를 선택하고, 2개 노드로 인스턴스 설정**

-   Cloud Spanner: 글로벌 분산 데이터베이스
-   최소 2개 노드 필요 (비용이 높음)

**D. Cloud Spanner를 선택하고, 인스턴스를 멀티 리전으로 설정**

-   Multi-regional: 여러 지역에 분산
-   가장 비용이 높은 옵션

#### 문제 분석 및 정답 도출

#### 서비스 비교

**Cloud SQL vs Cloud Spanner**

-   **Cloud SQL**:
-   소규모~중간 규모 애플리케이션
-   비용 효율적
-   단일 리전에 적합
-   PITR 지원 (binary logging 필요)

-   **Cloud Spanner**:
-   대규모, 글로벌 애플리케이션
-   비용이 높음 (최소 2노드부터)
-   자동 PITR 지원하지만 과도한 스펙

#### Point-in-Time Recovery

**Cloud SQL에서 PITR 활성화 방법**:

1. **Binary logging 활성화** ← 핵심!
2. Automated backup 설정
3. 이 둘이 결합되어 PITR 기능 제공

**Failover replicas**는 고가용성을 위한 것으로 PITR과는 별개입니다.

#### 비용 고려사항

-   **소규모 데이터 + 단일 위치** → Cloud SQL이 적합
-   Cloud Spanner는 요구사항 대비 과도하게 비싼 솔루션

#### 정답: **A. Cloud SQL (MySQL)을 선택하고, enable binary logging 옵션이 선택되었는지 확인**

**선택 이유**:

1. **비용 효율성**: 소규모 데이터에는 Cloud SQL이 적합
2. **PITR 지원**: Binary logging이 PITR의 핵심 요구사항
3. **단일 위치**: 멀티 리전 불필요
4. **요구사항 충족**: 모든 조건을 가장 경제적으로 만족

:::

## dump 4

You want to configure autohealing for network load balancing for a group of Compute Engine instances that run in multiple zones, using the fewest possible steps. You need to configure re-creation of VMs if they are unresponsive after 3 attempts of 10 seconds each. What should you do?

-   A. Create an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy (HTTP)
-   B. Create an HTTP load balancer with a backend configuration that references an existing instance group. Define a balancing mode and set the maximum RPS to 10.
-   C. Create a managed instance group. Set the Autohealing health check to healthy (HTTP)
-   D. Create a managed instance group. Verify that the autoscaling setting is on.

:::details 풀이

#### 영문 문제 해석

**문제**: "여러 영역에서 실행되는 Compute Engine 인스턴스 그룹에 대해 네트워크 로드 밸런싱을 위한 자동 복구(autohealing)를 가능한 한 최소한의 단계로 구성하려고 합니다. 각각 10초씩 3회 시도 후 응답하지 않으면 VM을 다시 생성하도록 구성해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   **Autohealing** (자동 복구) 구성
-   **Network load balancing**
-   **Multiple zones** (여러 영역)
-   **최소 단계**로 구성
-   **3회 시도 × 10초 = 30초** 후 VM 재생성
-   응답하지 않는 VM 처리

#### 선택지 해석

**A. 기존 인스턴스 그룹을 참조하는 백엔드 구성으로 HTTP 로드 밸런서 생성. 헬스 체크를 healthy (HTTP)로 설정**

-   HTTP 로드 밸런서: L7 로드 밸런서 (문제에서 요구하는 것은 네트워크 로드 밸런싱)
    -   L7 - OSI 7계층을 의미, HTTP 로드밸런서는 7계층인데, 문제에서 요구하는건 네트워크 계층 (4계층) 로드밸런서
-   기존 인스턴스 그룹 사용 (autohealing 기능 없음)
    -   문제에서의 existing instance group이 unmanaged instance group을 의미한다.

**B. 기존 인스턴스 그룹을 참조하는 백엔드 구성으로 HTTP 로드 밸런서 생성. 밸런싱 모드 정의하고 최대 RPS를 10으로 설정**

-   HTTP 로드 밸런서 (네트워크 로드 밸런싱 아님)
-   RPS 설정은 autohealing과 무관

**C. 관리형 인스턴스 그룹(Managed Instance Group) 생성. Autohealing 헬스 체크를 healthy (HTTP)로 설정**

-   MIG: autohealing 기능 내장
-   헬스 체크로 자동 복구 가능

**D. 관리형 인스턴스 그룹 생성. 오토스케일링 설정이 켜져 있는지 확인**

-   MIG 생성은 맞지만
-   Autoscaling ≠ Autohealing (다른 기능)

#### 문제 분석 및 정답 도출

#### Autohealing vs Load Balancing 구분

**Autohealing (자동 복구)**:

-   비정상 인스턴스를 **자동으로 재생성**
-   Managed Instance Group의 기능
-   Health check로 인스턴스 상태 모니터링

**Load Balancing**:

-   트래픽을 여러 인스턴스에 **분산**
-   비정상 인스턴스로 트래픽 전송 중단
-   인스턴스를 재생성하지는 않음

#### Network Load Balancing

문제에서 "network load balancing"을 언급했지만, 핵심 요구사항은 **autohealing**입니다.

-   Network LB 자체는 autohealing 기능 없음
-   MIG + Health Check = autohealing 구현

##### Managed Instance Group (MIG)의 장점

1. **Autohealing 내장**: 헬스 체크 기반 자동 복구
2. **Multi-zone 지원**: 여러 영역에 인스턴스 분산
3. **최소 단계**: 한 번의 설정으로 모든 기능 활성화
4. **Health Check 커스터마이징**: 3회 × 10초 설정 가능

#### 헬스 체크 설정

MIG에서 autohealing 설정 시:

Check interval: 10초
Timeout: 10초
Unhealthy threshold: 3회
총 대기 시간: 30초 후 VM 재생성

#### 정답: **C. 관리형 인스턴스 그룹 생성. Autohealing 헬스 체크를 healthy (HTTP)로 설정**

**선택 이유**:

1. **Autohealing 직접 제공**: MIG는 autohealing 기능을 내장
2. **최소 단계**: 하나의 구성으로 모든 요구사항 충족
3. **Multi-zone 지원**: 자동으로 여러 영역에 분산
4. **헬스 체크 커스터마이징**: 3회 × 10초 설정 가능
5. **요구사항 완벽 일치**: VM 재생성 기능 제공

오토힐링 헬스체크 타입을 HTTP 지정한다는 것은 HTTP 요청을 보낸 뒤 응답 코드를 보고 상태를 판단한다는 것을 의미한다.

-   TCP: 포트 연결만 확인
-   HTTPS: SSL/TLS 포함한 HTTP 확인
-   HTTP/2: HTTP/2 프로토콜로 확인

:::

## dump 5

You are using multiple configurations for gcloud. You want to review the configured Kubernetes Engine cluster of an inactive configuration using the fewest possible steps. What should you do?

-   A. Use gcloud config configurations describe to review the output.
-   B. Use gcloud config configurations activate and gcloud config list to review the output.
-   C. Use kubectl config get-contexts to review the output.
-   D. Use kubectl config use-context and kubectl config view to review the output.

:::details 풀이

### 영문 문제 해석

**문제**: "gcloud에 대해 여러 구성을 사용하고 있습니다. 가장 적은 단계로 비활성 구성의 구성된 Kubernetes Engine 클러스터를 검토하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Multiple gcloud configurations (여러 gcloud 구성)
-   Inactive configuration (비활성 구성)
-   Kubernetes Engine cluster 검토
-   Fewest possible steps (최소 단계)

### 선택지 해석

**A. gcloud config configurations describe를 사용하여 출력 검토**

-   특정 구성의 모든 속성을 표시
-   구성 활성화 없이 바로 확인 가능

**B. gcloud config configurations activate와 gcloud config list를 사용하여 출력 검토**

-   구성 활성화 후 현재 활성 구성 확인
-   2단계 필요

**C. kubectl config get-contexts를 사용하여 출력 검토**

-   kubectl의 컨텍스트 목록 확인
-   gcloud 구성과는 별개의 kubectl 구성

**D. kubectl config use-context와 kubectl config view를 사용하여 출력 검토**

-   kubectl 컨텍스트 변경 후 클러스터 구성 정보 확인
-   2단계 필요

### 문제 분석 및 정답 도출

#### gcloud configuration vs kubectl context

**gcloud configuration**

-   Google Cloud CLI의 구성 프로필
-   프로젝트, 리전, 계정, 클러스터 이름 등 설정

**kubectl context**

-   Kubernetes CLI의 클러스터 접속 정보
-   클러스터 API 서버, 인증 정보, 네임스페이스 등

#### "configured Kubernetes Engine cluster를 review"의 의미

**실제 Kubernetes 클러스터 구성 확인**

-   클러스터 API 서버 주소
-   인증 정보 및 방식
-   네임스페이스 설정
-   사용자 권한 정보

#### 각 선택지 분석

**A. gcloud config configurations describe**

-   출력: 프로젝트, 리전, 클러스터 이름
-   한계: 실제 Kubernetes 구성 정보 없음

**B. gcloud config configurations activate + list**

-   출력: gcloud 레벨 정보
-   한계: Kubernetes 클러스터 구성 세부사항 없음

**C. kubectl config get-contexts**

-   출력: 컨텍스트 목록
-   한계: 실제 클러스터 구성 정보 없음

**D. kubectl config use-context + view**

-   출력: 클러스터 API 서버, 인증 정보, 사용자 설정 등
-   장점: 실제 Kubernetes 클러스터 구성 정보 제공

##### 정답: D. kubectl config use-context와 kubectl config view를 사용하여 출력 검토

**선택 이유**:

1. 실제 Kubernetes 클러스터 구성 정보 확인 가능
2. API 서버 주소, 인증 방식, 사용자 권한 등 상세 정보 제공
3. kubectl config view가 클러스터 구성 검토에 적합한 명령어
4. gcloud configuration은 메타 정보만 제공하여 부족

:::

## dump 6

Your company uses Cloud Storage to store application backup files for disaster recovery purposes. You want to follow Google's recommended practices. Which storage option should you use?

A. Multi-Regional Storage
B. Regional Storage
C. Nearline Storage
D. Coldline Storage

:::details 풀이

### 영문 문제 해석

**문제**: "회사에서 재해 복구 목적으로 애플리케이션 백업 파일을 저장하기 위해 Cloud Storage를 사용합니다. Google의 권장 사례를 따르고자 합니다. 어떤 스토리지 옵션을 사용해야 할까요?"

**핵심 요구사항**:

-   Application backup files (애플리케이션 백업 파일)
-   Disaster recovery purposes (재해 복구 목적)
-   Google's recommended practices (Google 권장 사례)

### 선택지 해석

**A. Multi-Regional Storage**

-   여러 지역에 데이터 복제
-   높은 가용성과 내구성
-   가장 높은 비용

**B. Regional Storage**

-   단일 리전 내 여러 존에 데이터 복제
-   높은 성능, 중간 비용
-   지역적 재해에 취약

**C. Nearline Storage**

-   월 1회 미만 접근하는 데이터용
-   중간 비용, 검색 비용 있음
-   30일 최소 보관 기간

**D. Coldline Storage**

-   분기 1회 미만 접근하는 데이터용
-   낮은 비용, 높은 검색 비용
-   90일 최소 보관 기간

### 문제 분석 및 정답 도출

#### 재해 복구용 백업의 실제 특성

**접근 패턴**

-   평상시: 거의 접근하지 않음 (분기 1회 미만)
-   재해 발생시에만 접근
-   장기간 보관이 주 목적

**백업 데이터의 생명주기**

-   생성 후 장기간 보관
-   법적 요구사항으로 수년간 보존
-   실제 사용 빈도는 매우 낮음

**비용 최적화**

-   저장 비용이 가장 중요한 요소
-   재해시 검색 비용은 일회성으로 허용 가능

#### Google의 백업 권장 사례

**Disaster Recovery 백업의 특징**

-   매우 낮은 접근 빈도 (분기 1회 미만)
-   장기 보관 (90일 이상)
-   비용 효율성 최우선
-   재해시에만 빠른 검색 필요

**Coldline Storage의 장점**

-   백업 데이터의 접근 패턴과 정확히 일치
-   가장 낮은 저장 비용
-   90일 최소 보관 기간이 백업 정책과 맞음
-   재해 복구시 허용 가능한 검색 시간

#### 다른 옵션들이 부적합한 이유

**Multi-Regional/Regional Storage**

-   백업 데이터에는 과도한 비용
-   자주 접근하는 데이터용

**Nearline Storage**

-   월 1회 접근을 가정하지만 백업은 그보다 훨씬 적음
-   Coldline보다 불필요하게 높은 비용

**Archive Storage**도 있는데, 이는 가격 측면에서는 이점이 있지만 재해복구시 데이터 접근 속도가 너무 느려서 부적절하다.

#### 정답: D. Coldline Storage

**선택 이유**:

1. 재해 복구 백업의 매우 낮은 접근 빈도와 일치 (분기 1회 미만)
2. 장기 보관에 최적화된 가장 낮은 저장 비용
3. 90일 최소 보관 기간이 백업 정책에 적합
4. Google이 재해 복구 백업에 권장하는 스토리지 클래스
5. 재해시에만 발생하는 검색 비용은 비용 대비 허용 가능

:::

## dump 7

Several employees at your company have been creating projects with Cloud Platform and paying for it with their personal credit cards, which the company reimburses. The company wants to centralize all these projects under a single, new billing account. What should you do?

A. Contact cloud-billing@google.com with your bank account details and request a corporate billing account for your company.
B. Create a ticket with Google Support and wait for their call to share your credit card details over the phone.
C. In the Google Platform Console, go to the Resource Manage and move all projects to the root Organizarion.
D. In the Google Cloud Platform Console, create a new billing account and set up a payment method.

:::details 풀이

### 영문 문제 해석

**문제**: "회사의 여러 직원들이 Cloud Platform으로 프로젝트를 만들고 개인 신용카드로 결제하여 회사에서 보상받고 있습니다. 회사는 모든 프로젝트를 단일한 새로운 청구 계정으로 중앙화하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Multiple projects with personal credit cards (개인 신용카드로 결제하는 여러 프로젝트)
-   Company reimbursement (회사 보상)
-   Centralize under single billing account (단일 청구 계정으로 중앙화)
-   New billing account needed (새로운 청구 계정 필요)

### 선택지 해석

**A. cloud-billing@google.com에 은행 계좌 세부정보를 제공하여 회사용 기업 청구 계정을 요청**

-   이메일로 은행 정보 전달
-   기업 청구 계정 요청 방식

**B. Google Support에 티켓을 생성하고 전화로 신용카드 세부정보를 공유하기 위해 연락을 기다림**

-   Support 티켓 생성
-   전화를 통한 신용카드 정보 공유

**C. Google Platform Console에서 Resource Manager로 이동하여 모든 프로젝트를 루트 Organization으로 이동**

-   조직 구조 변경
-   프로젝트 이동 (청구 계정 변경 아님)

**D. Google Cloud Platform Console에서 새로운 청구 계정을 생성하고 결제 방법을 설정**

-   콘솔에서 직접 청구 계정 생성
-   결제 방법 직접 설정

### 문제 분석 및 정답 도출

#### 청구 계정 중앙화 과정

**필요한 단계**

1. 새로운 기업 청구 계정 생성
2. 결제 방법 설정 (기업 카드/계좌)
3. 기존 프로젝트들을 새 청구 계정으로 이동

#### 각 선택지 분석

**A. 이메일로 기업 청구 계정 요청**

-   cloud-billing@google.com은 실제 Google 이메일이 아님
-   은행 정보를 이메일로 전달하는 것은 보안상 부적절
-   공식적인 프로세스가 아님

**B. Support 티켓으로 신용카드 정보 공유**

-   Google은 전화로 결제 정보를 요구하지 않음
-   피싱 시도와 유사한 위험한 방법
-   공식적인 청구 계정 생성 방법이 아님

**C. Resource Manager로 프로젝트 이동**

-   Organization 구조 변경은 청구와 별개
-   프로젝트를 Organization으로 이동해도 청구 계정은 변경되지 않음
-   청구 중앙화와 직접적 관련 없음

**D. Console에서 새 청구 계정 생성**

-   Google Cloud Console의 표준 프로세스
-   직접적이고 안전한 방법
-   기업 결제 방법 설정 가능
-   생성 후 프로젝트들을 새 청구 계정으로 연결 가능

#### Google Cloud 청구 계정 생성 프로세스

**표준 절차**

1. Google Cloud Console → Billing 섹션
2. Create billing account 선택
3. 계정 정보 및 결제 방법 설정
4. 기존 프로젝트들을 새 청구 계정으로 이동

**보안 및 공식성**

-   모든 과정이 Google Cloud Console 내에서 진행
-   안전한 결제 정보 입력
-   즉시 적용 가능

#### 정답: D. Google Cloud Platform Console에서 새로운 청구 계정을 생성하고 결제 방법을 설정

**선택 이유**:

1. Google Cloud의 표준적이고 공식적인 프로세스
2. 안전하고 직접적인 청구 계정 생성 방법
3. 기업 결제 방법을 즉시 설정 가능
4. 생성 후 기존 프로젝트들을 새 청구 계정으로 쉽게 이동 가능
5. 추가적인 외부 연락이나 대기 시간 불필요

:::

## dump 8

You have an application that looks for its licensing server on the IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server. What should you do?

A. Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.
B. Reserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.
C. Use the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.
D. Start the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address.

:::details 풀이

### 영문 문제 해석

**문제**: "라이선스 서버를 IP 10.0.3.21에서 찾는 애플리케이션이 있습니다. Compute Engine에 라이선스 서버를 배포해야 합니다. 애플리케이션의 구성을 변경하고 싶지 않으며 애플리케이션이 라이선스 서버에 도달할 수 있기를 원합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Application looks for licensing server on IP 10.0.3.21 (애플리케이션이 특정 IP에서 라이선스 서버 검색)
-   Deploy licensing server on Compute Engine (Compute Engine에 라이선스 서버 배포)
-   Do not change application configuration (애플리케이션 구성 변경 금지)
-   Application must reach licensing server (애플리케이션이 라이선스 서버에 접근 가능해야 함)

### 선택지 해석

**A. gcloud를 사용하여 IP 10.0.3.21을 정적 내부 IP 주소로 예약하고 라이선스 서버에 할당**

-   Static internal IP address 예약
-   내부 네트워크에서 사용
-   Private IP 범위 (10.x.x.x)

**B. gcloud를 사용하여 IP 10.0.3.21을 정적 공용 IP 주소로 예약하고 라이선스 서버에 할당**

-   Static public IP address 예약
-   인터넷에서 접근 가능한 공용 IP
-   10.x.x.x는 사설 IP 대역

**C. IP 10.0.3.21을 커스텀 임시 IP 주소로 사용하여 라이선스 서버에 할당**

-   Custom ephemeral IP 사용
-   임시 IP로 특정 IP 지정

**D. 자동 임시 IP 주소로 라이선스 서버를 시작한 후 정적 내부 IP 주소로 승격**

-   임시 IP로 시작
-   나중에 정적 IP로 변경
-   원하는 특정 IP (10.0.3.21) 보장 안됨

### 문제 분석 및 정답 도출

#### IP 주소 10.0.3.21 분석

**IP 주소 특성**

-   10.0.3.21은 RFC 1918 사설 IP 대역 (10.0.0.0/8)
-   내부 네트워크에서만 사용 가능
-   공용 인터넷에서 라우팅 불가

#### 각 선택지 분석

**A. Static internal IP 예약**

-   10.x.x.x는 사설 IP 대역으로 internal IP가 적합
-   정적 예약으로 IP 주소 고정 보장
-   애플리케이션이 동일한 VPC 내에서 접근 가능
-   구성 변경 없이 기존 IP 주소 사용

**B. Static public IP 예약**

-   10.x.x.x는 사설 IP 대역으로 public IP로 사용 불가
-   RFC 1918 사설 주소는 공용 IP로 할당 불가능
-   Google Cloud에서 허용하지 않음

**C. Custom ephemeral IP**

-   Ephemeral IP는 일시적이며 재시작 시 변경될 수 있음
-   특정 IP 주소 보장이 어려움
-   라이선스 서버의 지속적 접근성에 부적합

**D. 자동 ephemeral에서 static으로 승격**

-   처음에 할당받는 IP가 10.0.3.21일 보장 없음
-   다른 IP가 할당되면 애플리케이션 접근 불가
-   승격 과정에서도 원하는 IP 보장 안됨

#### Static Internal IP 예약 과정

**Google Cloud 명령어**

```bash
gcloud compute addresses create LICENSE-SERVER-IP --region=us-central1 --subnet=default --addresses=10.0.3.21
gcloud compute instances create licensing-server --private-network-ip=10.0.3.21
```

#### 네트워크 요구사항 충족

**애플리케이션과 라이선스 서버 연결**

-   동일한 VPC 네트워크 내 배치
-   방화벽 규칙으로 적절한 포트 허용
-   내부 DNS 또는 직접 IP 접근

#### 정답: A. gcloud를 사용하여 IP 10.0.3.21을 정적 내부 IP 주소로 예약하고 라이선스 서버에 할당

**선택 이유**:

1. 10.0.3.21은 사설 IP 대역으로 internal IP가 적합
2. 정적 예약으로 IP 주소 고정 보장
3. 애플리케이션 구성 변경 없이 기존 하드코딩된 IP 사용 가능
4. 동일한 VPC 내에서 애플리케이션이 라이선스 서버에 접근 가능
5. Google Cloud의 표준적인 정적 IP 관리 방식

RFC 1918 사설 IP 대역이란 인터넷에서 라우팅되지 않는 내부 네트워크 전용 IP 주소 범위를 말한다. 사설 IP 대역은 총 3개이다.

1. `10.0.0.0/8` - `10.0.0.0` ~ `10.255.255.255`까지 할당 가능
    - 대규모 기업 네트워크
2. `172.16.0.0/12` - `172.16.0.0` ~ `172.31.255.255`까지 할당 가능
    - 중간 규모 네트워크
3. `192.168.0.0/16` - `192.168.255.255`
    - 소규모 네트워크 / 가정용 라우터

위 IP주소로는 라우팅이 불가능하고, 공용 인터넷에서 해당 주소들로 패킷을 전송하지 않는다.

자동 ephemeral IP주소는 VM 인스턴스 생성 시 자동으로 부여되는 주소이고, VM 재시작시 해당 주소가 변경될 수 있다. 별도 IP 예약비용이 없는게 특징이다. 임시 작업 및 개발환경에서 사용 가능하다.

:::

## dump 9

You are deploying an application to App Engine. You want the number of instances to scale based on request rate. You need at least 3 unoccupied instances at all times. Which scaling type should you use?

A. Manual Scaling with 3 instances.
B. Basic Scaling with min_instances set to 3.
C. Basic Scaling with max_instances set to 3.
D. Automatic Scaling with min_idle_instances set to 3.

:::details 풀이

### 영문 문제 해석

**문제**: "App Engine에 애플리케이션을 배포하고 있습니다. 요청 비율에 따라 인스턴스 수가 확장되기를 원합니다. 항상 최소 3개의 사용되지 않는 인스턴스가 필요합니다. 어떤 스케일링 타입을 사용해야 할까요?"

**핵심 요구사항**:

-   Deploy to App Engine (App Engine에 배포)
-   Scale based on request rate (요청 비율 기반 스케일링)
-   At least 3 unoccupied instances at all times (항상 최소 3개의 비어있는 인스턴스)

### 선택지 해석

**A. 3개 인스턴스로 Manual Scaling 사용**

-   Manual Scaling: 수동으로 인스턴스 수 고정
-   요청량에 관계없이 항상 3개 인스턴스 유지
-   자동 스케일링 불가

**B. min_instances를 3으로 설정한 Basic Scaling 사용**

-   Basic Scaling: 요청이 있을 때만 인스턴스 시작
-   min_instances: 최소 인스턴스 수 설정
-   유휴(idle) 시간 후 인스턴스 종료

**C. max_instances를 3으로 설정한 Basic Scaling 사용**

-   Basic Scaling 사용
-   max_instances: 최대 인스턴스 수 제한
-   3개를 초과할 수 없음

**D. min_idle_instances를 3으로 설정한 Automatic Scaling 사용**

-   Automatic Scaling: 요청량에 따른 자동 스케일링
-   min_idle_instances: 항상 유지할 유휴 인스턴스 수
-   요청 처리 중인 인스턴스 외에 추가로 유휴 인스턴스 유지

### 문제 분석 및 정답 도출

#### App Engine 스케일링 타입별 특징

**Manual Scaling**

-   고정된 인스턴스 수
-   자동 스케일링 없음
-   항상 동일한 수의 인스턴스 실행
-   요청량 변화에 대응 불가

**Basic Scaling**

-   요청이 있을 때만 인스턴스 시작
-   idle_timeout 후 인스턴스 종료
-   min_instances: 최소 실행 인스턴스 수
-   max_instances: 최대 실행 인스턴스 수
-   단순한 on-demand 스케일링

**Automatic Scaling**

-   요청량에 따른 지능형 자동 스케일링
-   다양한 메트릭 기반 스케일링
-   min_idle_instances: 항상 대기 상태로 유지할 인스턴스 수
-   빠른 응답을 위한 유휴 인스턴스 pool 유지

#### 요구사항 분석

**"Scale based on request rate" (요청 비율 기반 스케일링)**

-   Manual Scaling: ❌ 고정된 인스턴스로 스케일링 불가
-   Basic Scaling: ✅ 요청에 따른 기본적 스케일링
-   Automatic Scaling: ✅ 고급 요청 기반 스케일링

**"At least 3 unoccupied instances" (최소 3개 비어있는 인스턴스)**

-   Manual Scaling: ❌ 모든 인스턴스가 요청 처리에 사용됨
-   Basic Scaling min_instances: ❌ 최소 실행 인스턴스이지 유휴 인스턴스가 아님
-   Basic Scaling max_instances: ❌ 최대 제한일 뿐, 유휴 인스턴스 보장 안함
-   Automatic Scaling min_idle_instances: ✅ 요청 처리와 별도로 유휴 인스턴스 보장

#### 각 선택지 상세 분석

**A. Manual Scaling with 3 instances**

-   항상 3개 인스턴스만 실행
-   요청량 증가 시 스케일링 불가
-   3개 모두 요청 처리에 사용되어 "unoccupied" 보장 안됨

**B. Basic Scaling with min_instances=3**

-   최소 3개 인스턴스 실행
-   요청이 많으면 추가 인스턴스 시작 가능
-   하지만 3개가 모두 요청 처리 중일 수 있어 "unoccupied" 보장 안됨

**C. Basic Scaling with max_instances=3**

-   최대 3개까지만 실행
-   요청 증가 시 스케일링 제한
-   유휴 인스턴스 보장 없음

**D. Automatic Scaling with min_idle_instances=3**

-   요청량에 따른 자동 스케일링
-   요청 처리 인스턴스와 별도로 항상 3개의 유휴 인스턴스 유지
-   두 요구사항 모두 충족

#### min_idle_instances의 동작 원리

**실제 시나리오**

```
현재 처리 중인 요청: 10개 → 10개 인스턴스 사용 중
min_idle_instances: 3 설정
결과: 총 13개 인스턈스 실행 (10개 사용 중 + 3개 대기 중)

새로운 요청 급증 시:
- 3개 유휴 인스턴스가 즉시 요청 처리 시작
- 필요 시 추가 인스턴스 자동 생성
- 항상 3개 유휴 인스턴스 pool 유지
```

#### 정답: D. min_idle_instances를 3으로 설정한 Automatic Scaling 사용

**선택 이유**:

1. 요청 비율에 따른 자동 스케일링 제공
2. min_idle_instances=3으로 항상 3개의 비어있는 인스턴스 보장
3. 트래픽 급증 시 즉시 대응 가능한 유휴 인스턴스 pool 유지
4. 두 가지 요구사항을 모두 완벽히 충족
5. App Engine의 고급 스케일링 기능 활용

:::

## dump 10

You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps. What should you do?

-   A. Use gcloud iam roles copy and specify the production project as the destination project.
-   B. Use gcloud iam roles copy and specify your organization as the destination organization.
-   C. In the Google Cloud Platform Console, use the 'create role from role' functionality.
-   D. In the Google Cloud Platform Console, use the 'create role' functionality and select all applicable permissions.

:::details 풀이

### 영문 문제 해석

**문제**: "적절한 IAM 역할이 정의된 개발 프로젝트가 있습니다. 프로덕션 프로젝트를 생성하고 있으며 가장 적은 단계를 사용하여 새 프로젝트에 동일한 IAM 역할을 적용하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Development project with IAM roles defined (IAM 역할이 정의된 개발 프로젝트)
-   Creating production project (프로덕션 프로젝트 생성)
-   Same IAM roles on new project (새 프로젝트에 동일한 IAM 역할)
-   Fewest possible steps (최소 단계)

### 선택지 해석

**A. gcloud iam roles copy를 사용하고 프로덕션 프로젝트를 대상 프로젝트로 지정**

-   명령줄을 통한 역할 복사
-   개발 프로젝트에서 프로덕션 프로젝트로 직접 복사
-   프로젝트 간 역할 복사

**B. gcloud iam roles copy를 사용하고 조직을 대상 조직으로 지정**

-   명령줄을 통한 역할 복사
-   조직 레벨로 역할 복사
-   조직 전체에서 사용 가능한 역할 생성

**C. Google Cloud Platform Console에서 'create role from role' 기능 사용**

-   웹 콘솔을 통한 역할 생성
-   기존 역할을 기반으로 새 역할 생성
-   GUI 기반 접근 방식

**D. Google Cloud Platform Console에서 'create role' 기능을 사용하고 모든 해당 권한 선택**

-   웹 콘솔에서 처음부터 역할 생성
-   모든 권한을 수동으로 선택
-   가장 많은 단계 필요

### 문제 분석 및 정답 도출

#### IAM 역할 복사 방법 비교

**gcloud CLI 방법**

-   명령어 한 줄로 즉시 실행 가능
-   스크립트 자동화 가능
-   빠르고 정확한 복사

**Console GUI 방법**

-   웹 인터페이스를 통한 수동 작업
-   클릭과 선택 작업 필요
-   시간이 더 소요됨

#### gcloud iam roles copy 명령어 분석

**기본 문법**

```bash
gcloud iam roles copy --source=SOURCE_ROLE_ID --destination=DEST_ROLE_ID --dest-project=DEST_PROJECT_ID
```

**프로젝트 간 복사 vs 조직 레벨 복사**

**A. 프로덕션 프로젝트로 복사**

```bash
gcloud iam roles copy --source=projects/dev-project/roles/customRole --destination=customRole --dest-project=prod-project
```

-   개발 프로젝트의 커스텀 역할을 프로덕션 프로젝트로 직접 복사
-   프로덕션 프로젝트에서만 사용 가능
-   정확히 요구사항에 맞음

**B. 조직으로 복사**

```bash
gcloud iam roles copy --source=projects/dev-project/roles/customRole --destination=customRole --dest-organization=123456789
```

-   조직 레벨로 역할 복사
-   조직 내 모든 프로젝트에서 사용 가능
-   과도한 범위 (프로덕션 프로젝트만 필요한데 조직 전체에 적용)

#### Console 방법 분석

**C. 'create role from role' 기능**

-   기존 역할을 선택하여 복사본 생성
-   GUI에서 몇 번의 클릭 필요
-   gcloud보다 더 많은 단계

**D. 'create role' 기능**

-   처음부터 새로운 역할 생성
-   모든 권한을 개별적으로 선택해야 함
-   가장 많은 시간과 단계 소요
-   실수 가능성 높음

#### 최소 단계 비교

**단계 수 비교**

1. **A (gcloud copy to project)**: 1단계 - 명령어 한 줄 실행
2. **B (gcloud copy to org)**: 1단계 - 하지만 불필요한 범위
3. **C (console from role)**: 3-4단계 - 웹 접속, 역할 선택, 복사, 설정
4. **D (console create)**: 5+ 단계 - 웹 접속, 역할 생성, 권한 개별 선택

#### 범위 적절성

**요구사항**: "새 프로젝트에 동일한 IAM 역할"

-   A: ✅ 프로덕션 프로젝트에만 적용 (정확한 범위)
-   B: ❌ 조직 전체에 적용 (과도한 범위)
-   C: ✅ 프로젝트별 적용 가능
-   D: ✅ 프로젝트별 적용 가능

#### 정답: A. gcloud iam roles copy를 사용하고 프로덕션 프로젝트를 대상 프로젝트로 지정

**선택 이유**:

1. 최소 단계: 명령어 한 줄로 완료
2. 정확한 범위: 프로덕션 프로젝트에만 역할 복사
3. 완전한 복사: 모든 권한이 정확히 복사됨
4. 자동화 가능: 스크립트로 반복 실행 가능
5. 실수 방지: 수동 권한 선택으로 인한 누락 방지

:::

## dump 11

You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use?

-   A. Deployment Manager
-   B. Cloud Composer
-   C. Managed Instance Group
-   D. Unmanaged Instance Group

:::details 풀이

### 영문 문제 해석

**문제**: "Compute Engine에서 VM을 동적으로 프로비저닝하는 방법이 필요합니다. 정확한 사양은 전용 구성 파일에 있을 것입니다. Google의 권장 사례를 따르고 싶습니다. 어떤 방법을 사용해야 할까요?"

**핵심 요구사항**:

-   Dynamic provisioning of VMs (VM의 동적 프로비저닝)
    -   VM 생성, VM 수 및 사양, 배포 및 관리 등의 과정이 자동으로 이루어지는 것을 의미한다.
    -   정적 프로비저닝은 관리자가 VM을 하나씩 직접 생성하고, 구성과 관리가 고정적으로 이루어진다.
-   Exact specifications in dedicated configuration file (전용 구성 파일에 정확한 사양)
-   Follow Google's recommended practices (Google 권장 사례 준수)
-   On Compute Engine (Compute Engine 사용)

### 선택지 해석

**A. Deployment Manager**

-   Google Cloud의 Infrastructure as Code (IaC) 서비스
-   YAML/Jinja2/Python 템플릿을 사용한 리소스 정의
-   구성 파일 기반 인프라 배포 및 관리
-   선언적 구성 관리

**B. Cloud Composer**

-   Apache Airflow 기반 워크플로우 오케스트레이션 서비스
-   복잡한 데이터 파이프라인 및 워크플로우 관리
-   DAG(Directed Acyclic Graph) 기반 작업 스케줄링
-   데이터 처리 워크플로우용

**C. Managed Instance Group (MIG)**

-   동일한 VM 인스턴스들의 관리형 그룹
-   Instance Template 기반 자동 스케일링
-   자동 복구 및 업데이트 기능
-   동적 스케일링 지원

**D. Unmanaged Instance Group**

-   기존 VM 인스턴스들의 단순한 그룹핑
-   수동 관리 필요
-   자동 스케일링 및 관리 기능 없음
-   정적인 구성

### 문제 분석 및 정답 도출

#### 요구사항 세부 분석

**"Dynamic provisioning" (동적 프로비저닝)**

-   필요에 따라 자동으로 리소스 생성/삭제
-   런타임에 인프라 변경 가능
-   선언적 구성을 통한 자동 배포

**"Dedicated configuration file" (전용 구성 파일)**

-   Infrastructure as Code 접근 방식
-   코드로 인프라 정의 및 버전 관리
-   재사용 가능한 템플릿 기반 배포

**"Google's recommended practices" (Google 권장 사례)**

-   IaC 사용 권장
-   자동화된 배포 및 관리
-   버전 관리 및 재현 가능한 배포

#### 각 선택지 상세 분석

**A. Deployment Manager**

-   ✅ 구성 파일 기반 (YAML/Jinja2/Python 템플릿)
-   ✅ 동적 프로비저닝 지원
-   ✅ Google의 공식 IaC 도구
-   ✅ 선언적 구성으로 VM 사양 정의 가능
-   ✅ 버전 관리 및 롤백 지원

**B. Cloud Composer**

-   ❌ 워크플로우 오케스트레이션 도구 (VM 프로비저닝용 아님)
-   ❌ 주로 데이터 파이프라인 관리용
-   ❌ Infrastructure as Code 도구가 아님
-   ❌ VM 사양 정의에 부적합

**C. Managed Instance Group**

-   ✅ 동적 스케일링 지원
-   ✅ Instance Template 기반 구성
-   ❌ 단일 템플릿 기반으로 제한적
-   ❌ 복잡한 인프라 구성에는 한계
-   ❌ 전체적인 IaC 솔루션이 아님

**D. Unmanaged Instance Group**

-   ❌ 수동 관리 필요
-   ❌ 동적 프로비저닝 지원 안함
-   ❌ 구성 파일 기반 관리 불가
-   ❌ 정적인 그룹핑만 제공

#### Deployment Manager의 장점

**Infrastructure as Code**

```yaml
# deployment-manager-template.yaml
resources:
    - name: web-server-template
      type: compute.v1.instanceTemplate
      properties:
          properties:
              machineType: n1-standard-1
              disks:
                  - boot: true
                    initializeParams:
                        sourceImage: projects/debian-cloud/global/images/family/debian-10
              networkInterfaces:
                  - network: global/networks/default
                    accessConfigs:
                        - type: ONE_TO_ONE_NAT

    - name: web-server-group
      type: compute.v1.instanceGroupManager
      properties:
          baseInstanceName: web-server
          instanceTemplate: $(ref.web-server-template.selfLink)
          targetSize: 3
```

**동적 관리**

-   구성 파일 변경 후 재배포로 인프라 업데이트
-   자동 롤백 및 버전 관리
-   의존성 관리 및 순서 보장

**Google 권장 사례**

-   공식 IaC 도구로 권장
-   다른 Google Cloud 서비스와 완전 통합
-   엔터프라이즈급 기능 제공

#### 다른 도구들과의 비교

**Terraform vs Deployment Manager**

-   Terraform: 멀티 클라우드 지원
-   Deployment Manager: Google Cloud 특화, 더 깊은 통합

**Instance Groups vs Deployment Manager**

-   Instance Groups: VM 그룹 관리에 특화
-   Deployment Manager: 전체 인프라 스택 관리

#### 정답: A. Deployment Manager

**선택 이유**:

1. 구성 파일 기반 Infrastructure as Code 지원
2. 동적 프로비저닝 및 리소스 관리 제공
3. Google Cloud의 공식 IaC 도구로 권장 사례 준수
4. YAML/Jinja2/Python 템플릿으로 복잡한 VM 사양 정의 가능
5. 버전 관리, 롤백, 의존성 관리 등 엔터프라이즈 기능 제공
6. Google Cloud 서비스와의 완전한 통합

IaC는 인프라를 코드로 관리하는 방법론을 말한다. 기존에는 GUI, 명령어 입력, 문서화 등으로 인프라를 관리했지만 IaC는 코드 작성을 통해 자동으로 배포 / 버전 관리 등이 이루어진다.

코드로 관리되기 때문에 변경사항이 코드로 추적 가능하고, 어떻게 구성하는 지에 대한 서술이 아닌 무엇을 구성하는지 작성함으로써 선언적인 코드 작성이 가능하다. 또한 같은 코드로 인프라 구성 시 동일한 결과를 나타낸다는 멱등성이 성립된다.

구글 클라우드에서는 Deployment Manager가 공식 IaC 도구이며, Cloud Foundation Toolkit이 모범 사례로 사용되는 템플릿이다.

:::

## dump 12

You have a Dockerfile that you need to deploy on Kubernetes Engine. What should you do?

A. Use kubectl app deploy `<dockerfilename>`.
B. Use gcloud app deploy `<dockerfilename>`.
C. Create a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.
D. Create a docker image from the Dockerfile and upload it to Cloud Storage. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.

:::details 풀이

### 영문 문제 해석

**문제**: "Kubernetes Engine에 배포해야 하는 Dockerfile이 있습니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Dockerfile exists (Dockerfile 존재)
-   Deploy on Kubernetes Engine (Kubernetes Engine에 배포)
-   Proper deployment process (적절한 배포 프로세스)

### 선택지 해석

**A. kubectl app deploy `<dockerfilename>` 사용**

-   kubectl 명령어 사용
-   app deploy 하위 명령어로 Dockerfile 직접 배포
-   직접적인 Dockerfile 배포 시도

**B. gcloud app deploy `<dockerfilename>` 사용**

-   gcloud 명령어 사용
-   App Engine 배포 명령어
-   Kubernetes Engine이 아닌 App Engine용 명령어

**C. Dockerfile에서 Docker 이미지 생성하여 Container Registry에 업로드. Deployment YAML 파일을 생성하여 해당 이미지를 가리키도록 설정. kubectl을 사용하여 파일로 배포 생성**

-   Docker 이미지 빌드 및 Container Registry 업로드
-   Kubernetes Deployment YAML 파일 생성
-   kubectl을 통한 배포

**D. Dockerfile에서 Docker 이미지 생성하여 Cloud Storage에 업로드. Deployment YAML 파일을 생성하여 해당 이미지를 가리키도록 설정. kubectl을 사용하여 파일로 배포 생성**

-   Docker 이미지 빌드 및 Cloud Storage 업로드
-   Kubernetes Deployment YAML 파일 생성
-   kubectl을 통한 배포

### 문제 분석 및 정답 도출

#### Kubernetes 배포 프로세스 이해

**Kubernetes의 기본 동작 원리**

-   Kubernetes는 컨테이너 이미지를 실행
-   Dockerfile 자체를 직접 실행하지 않음
-   컨테이너 레지스트리에서 이미지를 Pull하여 실행

**표준 배포 단계**

1. Dockerfile → Docker Image 빌드
2. Docker Image → Container Registry 업로드
3. Deployment YAML 작성 (이미지 참조)
4. kubectl로 Deployment 생성

#### 각 선택지 상세 분석

**A. kubectl app deploy `<dockerfilename>`**

-   kubectl에 'app deploy' 하위 명령어는 존재하지 않음
-   kubectl은 Kubernetes 리소스를 관리하는 도구
-   Dockerfile을 직접 처리하는 기능 없음
-   잘못된 명령어

**B. gcloud app deploy `<dockerfilename>`**

-   App Engine 배포 명령어
-   Kubernetes Engine이 아닌 App Engine용
-   서비스가 다름 (App Engine ≠ Kubernetes Engine)
-   요구사항과 불일치

**C. Container Registry 사용**

-   ✅ 올바른 Docker 빌드 프로세스
-   ✅ Container Registry는 Docker 이미지 저장소
-   ✅ Kubernetes가 Container Registry에서 이미지 Pull 가능
-   ✅ 표준 Kubernetes 배포 프로세스

**D. Cloud Storage 사용**

-   ✅ 올바른 Docker 빌드 프로세스
-   ❌ Cloud Storage는 파일 저장소 (Docker 이미지 레지스트리 아님)
-   ❌ Kubernetes가 Cloud Storage에서 직접 이미지 Pull 불가
-   ❌ 잘못된 저장소 선택

#### Container Registry vs Cloud Storage

**Container Registry**

-   Docker 이미지 전용 레지스트리 서비스
-   Docker pull/push 명령어 지원
-   Kubernetes와 완전 통합
-   이미지 버전 관리 및 보안 스캔
-   표준 Docker Registry API 지원

**Cloud Storage**

-   일반적인 파일 저장 서비스
-   Docker 이미지 형태로 저장 불가
-   Kubernetes가 직접 접근 불가
-   Docker Registry 프로토콜 미지원

#### 올바른 배포 프로세스 (선택지 C)

**1. Docker 이미지 빌드**

```bash
# Dockerfile에서 이미지 생성
docker build -t my-app:v1.0 .
```

**2. Container Registry에 업로드**

```bash
# 태그 지정
docker tag my-app:v1.0 gcr.io/my-project/my-app:v1.0

# 업로드
docker push gcr.io/my-project/my-app:v1.0
```

**3. Deployment YAML 작성**

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
    name: my-app-deployment
spec:
    replicas: 3
    selector:
        matchLabels:
            app: my-app
    template:
        metadata:
            labels:
                app: my-app
        spec:
            containers:
                - name: my-app
                  image: gcr.io/my-project/my-app:v1.0
                  ports:
                      - containerPort: 8080
```

**4. kubectl로 배포**

```bash
kubectl apply -f deployment.yaml
```

#### Google Cloud의 통합 서비스

**Container Registry와 GKE 통합**

-   같은 프로젝트 내에서 자동 인증
-   IAM 권한으로 접근 제어
-   네트워크 최적화된 이미지 전송
-   보안 및 취약점 스캔

#### 정답: C. Dockerfile에서 Docker 이미지를 생성하여 Container Registry에 업로드하고, Deployment YAML 파일을 생성하여 해당 이미지를 가리키도록 설정한 후, kubectl을 사용하여 해당 파일로 배포를 생성

**선택 이유**:

1. 표준 Kubernetes 배포 프로세스 준수
2. Container Registry는 Docker 이미지 전용 레지스트리 서비스
3. Kubernetes가 Container Registry에서 이미지를 정상적으로 Pull 가능
4. Google Cloud의 권장 사례 및 모범 사례
5. 보안, 버전 관리, 통합 측면에서 최적
6. 실제 운영 환경에서 사용하는 표준 방법

컨테이너 레지스트리는 Docker 이미지 저장소이다. 주소 형식은 `gcr.io/[PROJECT-ID]/[IMAGE-NAME]:[TAG]`이다.

```bash
# 1. 개발자가 코드 작성
vim app.py

# 2. Dockerfile 작성
vim Dockerfile

# 3. 이미지 빌드
docker build -t gcr.io/my-project/my-app:v1.0 .

# 4. 테스트
docker run gcr.io/my-project/my-app:v1.0

# 5. 업로드
docker push gcr.io/my-project/my-app:v1.0

# 6. Kubernetes에서 사용
kubectl run my-app --image=gcr.io/my-project/my-app:v1.0
```

:::

## dump 13

Your development team needs a new Jenkins server for their project. You need to deploy the server using the fewest steps possible. What should you do?

-   A. Download and deploy the Jenkins Java WAR to App Engine Standard.
-   B. Create a new Compute Engine instance and install Jenkins through the command line interface.
-   C. Create a Kubernetes cluster on Compute Engine and create a deployment with the Jenkins Docker image.
-   D. Use GCP Marketplace to launch the Jenkins solution.

:::details 풀이

### 영문 문제 해석

**문제**: "개발팀이 프로젝트를 위해 새로운 Jenkins 서버가 필요합니다. 가능한 한 최소한의 단계로 서버를 배포해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   New Jenkins server for development team (개발팀을 위한 새 Jenkins 서버)
-   Deploy using fewest steps possible (최소한의 단계로 배포)
-   Quick and simple deployment (빠르고 간단한 배포)

### 선택지 해석

**A. Jenkins Java WAR를 App Engine Standard에 다운로드하고 배포**

-   App Engine Standard 환경 사용
-   Jenkins WAR 파일을 직접 배포
-   서버리스 환경에서 Jenkins 실행

**B. 새로운 Compute Engine 인스턴스를 생성하고 명령줄 인터페이스를 통해 Jenkins 설치**

-   VM 인스턴스 수동 생성
-   SSH 접속 후 수동 설치
-   명령줄 기반 설치 과정

**C. Compute Engine에서 Kubernetes 클러스터를 생성하고 Jenkins Docker 이미지로 배포 생성**

-   GKE 클러스터 생성
-   Kubernetes 배포 설정
-   Jenkins Docker 컨테이너 실행

**D. GCP Marketplace를 사용하여 Jenkins 솔루션 실행**

-   Google Cloud Marketplace 활용
-   사전 구성된 Jenkins 솔루션 사용
-   클릭 몇 번으로 배포 완료

### 문제 분석 및 정답 도출

#### 각 선택지 단계 수 분석

**A. App Engine Standard + Jenkins WAR**
단계:

1. Jenkins WAR 파일 다운로드
    - 젠킨스 WAR는 자바 웹앱을 패키징한 압축파일이다.
2. app.yaml 구성 파일 작성
3. gcloud app deploy 실행
    - 문제점: App Engine Standard는 Jenkins 같은 지속적인 백그라운드 서비스에 부적합
    - 앱엔진 스탠다드 환경은 요청이 올때만 어플리케이션을 실행한다. (백그라운드 스레드를 제한한다.)
    - Git, 빌드 도구 등 외부 연결도 제한된다.
    - 앱엔진 스탠다드는 읽기 전용 파일시스템을 갖는다.

**B. Compute Engine + 수동 설치**
단계:

1. VM 인스턴스 생성
2. SSH 접속
3. Java 설치
4. Jenkins 다운로드
5. Jenkins 설치 및 구성
6. 방화벽 규칙 설정
7. 초기 설정 및 플러그인 설치
   총 7+단계 (가장 많음)

**C. GKE + Jenkins Docker**
단계:

1. GKE 클러스터 생성
2. kubectl 구성
3. Jenkins Deployment YAML 작성
4. Service YAML 작성
5. kubectl apply로 배포
6. Persistent Volume 설정
   총 6단계

**D. GCP Marketplace**
단계:

1. Google Cloud Console → Marketplace 접속
2. Jenkins 검색
3. 구성 옵션 선택 (VM 크기, 네트워크 등)
4. Deploy 버튼 클릭
   총 4단계 (가장 적음)

#### GCP Marketplace의 장점

**사전 구성된 솔루션**

-   Jenkins가 이미 설치 및 구성됨
-   최적화된 설정 적용
-   필요한 플러그인 포함
-   보안 설정 자동 적용

**자동 인프라 프로비저닝**

-   VM 인스턴스 자동 생성
-   방화벽 규칙 자동 설정
-   네트워크 구성 자동 적용
-   디스크 및 스토리지 자동 구성

**즉시 사용 가능**

-   배포 완료 후 바로 Jenkins 접속 가능
-   관리자 계정 자동 생성
-   SSL/TLS 인증서 자동 설정
-   모니터링 및 백업 옵션 제공

##### 다른 선택지들의 한계

**A. App Engine Standard 문제점**

-   Jenkins는 지속적으로 실행되어야 하는 서비스
-   App Engine Standard는 요청 기반 실행 모델
-   Jenkins의 빌드 작업, 스케줄링 기능과 호환성 문제
-   파일 시스템 접근 제한

**B. 수동 설치의 단점**

-   시간 소모적 (수 시간 소요 가능)
-   설정 실수 가능성
-   보안 설정 누락 위험
-   초기 구성의 복잡성

**C. Kubernetes의 복잡성**

-   클러스터 관리 오버헤드
-   YAML 구성 파일 작성 필요
-   Persistent Volume 설정 복잡
-   Jenkins에 특화된 설정 필요

#### GCP Marketplace Jenkins 솔루션 특징

**포함된 구성요소**

-   Jenkins LTS (Long Term Support) 버전
-   필수 플러그인 사전 설치
-   NGINX 리버스 프록시
-   Let's Encrypt SSL 인증서
-   자동 백업 스크립트

**배포 옵션**

-   VM 크기 선택 (n1-standard-1~8)
-   디스크 크기 설정
-   네트워크 및 서브넷 선택
-   방화벽 규칙 자동 생성

**사후 관리**

-   Google Cloud Console에서 상태 모니터링
-   자동 업데이트 옵션
-   백업 및 복원 기능
-   로그 및 메트릭 수집

#### 실제 배포 과정 (선택지 D)

**1단계: Marketplace 접속**

```
Google Cloud Console → Navigation Menu → Marketplace
```

**2단계: Jenkins 검색 및 선택**

```
검색창에 "Jenkins" 입력 → "Jenkins Certified by Bitnami" 선택
```

**3단계: 구성 설정**

```
- Deployment name: jenkins-server
- Zone: us-central1-a
- Machine type: n1-standard-2
- Boot disk size: 20GB
- Network: default
```

**4단계: 배포 실행**

```
"Deploy" 버튼 클릭 → 5-10분 후 배포 완료
```

#### 정답: D. GCP Marketplace를 사용하여 Jenkins 솔루션 실행

**선택 이유**:

1. 최소 단계: 단 4단계로 완료 (다른 방법 대비 가장 적음)
2. 사전 구성: Jenkins가 이미 최적화된 상태로 설치
3. 자동 인프라: VM, 네트워크, 보안 설정 자동 생성
4. 즉시 사용: 배포 완료 후 바로 Jenkins 접속 가능
5. 관리 편의성: Google Cloud 통합 모니터링 및 관리
6. 검증된 솔루션: Bitnami에서 제공하는 안정적인 이미지
7. 시간 절약: 수동 설치 대비 95% 시간 단축

젠킨스는 CI/CD를 자동화해주는 도구이다. CI 단계에서 통합된 코드를 자동으로 빌드하고 실행해주고, CD 단계에서 테스트 통과 코드를 자동으로 프로덕션에 배포해준다.

:::

## dump 14

You need to update a deployment in Deployment Manager without any resource downtime in the deployment. Which command should you use?

A. gcloud deployment-manager deployments create --config `<deployment-config-path>`
B. gcloud deployment-manager deployments update --config `<deployment-config-path>`
C. gcloud deployment-manager resources create --config `<deployment-config-path>`
D. gcloud deployment-manager resources update --config `<deployment-config-path>`

:::details 풀이

### 영문 문제 해석

**문제**: "배포에서 리소스 다운타임 없이 Deployment Manager의 배포를 업데이트해야 합니다. 어떤 명령어를 사용해야 할까요?"

**핵심 요구사항**:

-   Update a deployment in Deployment Manager (Deployment Manager에서 배포 업데이트)
-   Without any resource downtime (리소스 다운타임 없이)
-   Existing deployment modification (기존 배포 수정)

### 선택지 해석

**A. gcloud deployment-manager deployments create --config `<deployment-config-path>`**

-   새로운 배포를 생성하는 명령어
-   create 동작으로 기존 배포와 별개
-   신규 배포 생성용

**B. gcloud deployment-manager deployments update --config `<deployment-config-path>`**

-   기존 배포를 업데이트하는 명령어
-   update 동작으로 기존 배포 수정
-   배포 수준에서의 업데이트

**C. gcloud deployment-manager resources create --config `<deployment-config-path>`**

-   새로운 리소스를 생성하는 명령어
-   개별 리소스 생성용
-   배포 전체 업데이트와는 다른 개념

**D. gcloud deployment-manager resources update --config `<deployment-config-path>`**

-   개별 리소스를 업데이트하는 명령어
-   특정 리소스만 수정
-   배포 전체가 아닌 리소스 단위 업데이트

### 문제 분석 및 정답 도출

#### Deployment Manager 구조 이해

**Deployment (배포)**

-   여러 리소스들의 집합
-   YAML/Jinja2/Python 템플릿으로 정의
-   하나의 논리적 단위로 관리

**Resources (리소스)**

-   배포 내의 개별 구성 요소
-   VM, 네트워크, 로드밸런서 등
-   배포에 속하는 개별 클라우드 리소스

#### 명령어 체계 분석

**deployments vs resources**

```bash
# 배포 레벨 명령어 (전체 관리)
gcloud deployment-manager deployments [create|update|delete]

# 리소스 레벨 명령어 (개별 관리)
gcloud deployment-manager resources [create|update|delete]
```

**create vs update**

```bash
# 생성 (신규)
gcloud deployment-manager deployments create my-deployment --config config.yaml

# 업데이트 (기존 수정)
gcloud deployment-manager deployments update my-deployment --config config.yaml
```

#### 각 선택지 상세 분석

**A. deployments create**

-   새로운 배포 생성
-   기존 배포가 있으면 충돌 오류 발생
-   업데이트가 아닌 신규 생성
-   요구사항과 불일치

**B. deployments update**

-   기존 배포의 구성 수정
-   다운타임 최소화 전략 사용
-   Rolling update 또는 Blue-Green 배포 지원
-   전체 배포 단위에서 일관된 업데이트

**C. resources create**

-   개별 리소스 신규 생성
-   배포와 독립적으로 리소스만 추가
-   전체 배포 업데이트와는 다른 개념
-   요구사항과 불일치

**D. resources update**

-   개별 리소스만 수정
-   배포 전체의 일관성 보장 어려움
-   복잡한 의존성 관리 필요
-   전체적인 배포 업데이트에 부적합

#### Deployment Manager의 업데이트 메커니즘

**Smart Update 전략**

```
1. 구성 파일 변경사항 분석
2. 필요한 리소스만 선별적 업데이트
3. 의존성 순서에 따른 순차 업데이트
4. 롤백 지점 자동 생성
```

**다운타임 최소화 방법**

```
- Rolling Update: 점진적 교체
- Blue-Green Deployment: 새 버전 준비 후 전환
- 헬스체크 기반 업데이트
- 자동 롤백 기능
```

#### 실제 업데이트 과정

**기존 배포 확인**

```bash
gcloud deployment-manager deployments list
gcloud deployment-manager deployments describe my-deployment
```

**구성 파일 수정**

```yaml
# config.yaml 수정
resources:
    - name: web-server
      type: compute.v1.instance
      properties:
          machineType: n1-standard-2 # n1-standard-1에서 업그레이드
          zone: us-central1-a
```

**배포 업데이트 실행**

```bash
gcloud deployment-manager deployments update my-deployment --config config.yaml
```

**업데이트 모니터링**

```bash
gcloud deployment-manager operations list
gcloud deployment-manager operations describe OPERATION_ID
```

#### 다운타임 없는 업데이트의 원리

**Managed Instance Group 예시**

```yaml
# 기존: 3개 인스턴스
resources:
- name: web-servers
  type: compute.v1.instanceGroupManager
  properties:
    targetSize: 3
    instanceTemplate: old-template

# 업데이트: 새 템플릿으로 변경
resources:
- name: web-servers
  type: compute.v1.instanceGroupManager
  properties:
    targetSize: 3
    instanceTemplate: new-template  # 새 템플릿
```

**업데이트 과정**

```
1. 새 템플릿으로 1개 인스턴스 생성
2. 헬스체크 통과 확인
3. 기존 인스턴스 1개 삭제
4. 과정 반복 (Rolling Update)
5. 모든 인스턴스가 새 버전으로 교체 완료
```

#### deployments update의 장점

**일관된 상태 관리**

-   전체 배포의 일관성 보장
-   의존성 자동 해결
-   원자적 업데이트 (All or Nothing)

**자동 롤백**

-   업데이트 실패 시 자동 롤백
-   이전 상태로 안전하게 복원
-   데이터 무결성 보장

**모니터링 및 로깅**

-   업데이트 과정 전체 추적
-   상세한 로그 및 메트릭
-   문제 발생 시 신속한 진단

#### 정답: B. gcloud deployment-manager deployments update --config `<deployment-config-path>`

**선택 이유**:

1. 기존 배포를 업데이트하는 올바른 명령어
2. 다운타임 최소화를 위한 스마트 업데이트 전략 사용
3. 전체 배포의 일관성과 의존성을 자동으로 관리
4. Rolling Update 및 Blue-Green 배포 등 무중단 업데이트 지원
5. 자동 롤백 및 오류 복구 기능 제공
6. Google Cloud의 권장 사례 및 표준 방법

다운타임(Downtime)은 시스템이나 서비스가 정상적으로 작동하지 않는 시간을 의미한다. 다운타임 없이 배포하기 위해 롤링 업데이트를 지원하는 서비스가 필요하다.

:::

## dump 15

You need to run an important query in BigQuery but expect it to return a lot of records. You want to find out how much it will cost to run the query. You are using on-demand pricing. What should you do?

A. Arrange to switch to Flat-Rate pricing for this query, then move back to on-demand.
B. Use the command line to run a dry run query to estimate the number of bytes read. Then convert that bytes estimate to dollars using the Pricing Calculator.
C. Use the command line to run a dry run query to estimate the number of bytes returned. Then convert that bytes estimate to dollars using the Pricing Calculator.
D. Run a select count (\*) to get an idea of how many records your query will look through. Then convert that number of rows to dollars using the Pricing Calculator.

:::details 풀이

### 영문 문제 해석

**문제**: "BigQuery에서 중요한 쿼리를 실행해야 하지만 많은 레코드를 반환할 것으로 예상됩니다. 쿼리 실행 비용이 얼마나 될지 알고 싶습니다. On-demand 요금제를 사용하고 있습니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Important query in BigQuery (BigQuery에서 중요한 쿼리)
-   Expect to return a lot of records (많은 레코드 반환 예상)
-   Find out cost to run the query (쿼리 실행 비용 확인)
-   Using on-demand pricing (On-demand 요금제 사용)

### 선택지 해석

**A. 이 쿼리를 위해 Flat-Rate 요금제로 전환한 후 다시 On-demand로 복구**

-   요금제 변경을 통한 비용 관리
-   일시적으로 다른 요금제 사용
-   쿼리 완료 후 원래 요금제로 복귀

**B. 명령줄을 사용하여 dry run 쿼리로 읽을 바이트 수를 추정한 후 Pricing Calculator로 바이트 추정치를 달러로 변환**

-   Dry run으로 데이터 스캔량 미리 확인
-   읽을 바이트(bytes read) 기반 비용 계산
-   실제 쿼리 실행 전 비용 예측

**C. 명령줄을 사용하여 dry run 쿼리로 반환될 바이트 수를 추정한 후 Pricing Calculator로 바이트 추정치를 달러로 변환**

-   Dry run으로 결과 크기 미리 확인
-   반환될 바이트(bytes returned) 기반 비용 계산
-   결과 데이터 크기 기반 예측

**D. select count(\*)를 실행하여 쿼리가 살펴볼 레코드 수를 파악한 후 Pricing Calculator로 행 수를 달러로 변환**

-   레코드 수 기반 비용 예측
-   count(\*) 쿼리로 행 수 확인
-   행 개수를 비용으로 변환

### 문제 분석 및 정답 도출

#### BigQuery On-Demand 요금제 이해

**과금 기준**

-   BigQuery On-demand 요금제는 **스캔한 데이터의 바이트 수**를 기준으로 과금
-   쿼리가 읽은 데이터량 (bytes read/processed)이 비용 결정 요소
-   반환되는 결과의 크기나 행 개수는 과금과 무관

**요금 구조 (2024년 기준)**

```
$6.25 per TB (테라바이트당 $6.25)
첫 1TB/월은 무료
```

#### 각 선택지 상세 분석

**A. Flat-Rate로 전환**

-   Flat-Rate는 고정 비용 요금제 ($2,000-40,000/월)
-   일회성 쿼리를 위한 요금제 변경은 비효율적
-   변경/복구 과정이 복잡하고 시간 소요
-   단일 쿼리 비용 예측 목적에 부적합

**B. Dry run으로 읽을 바이트 수 추정** ✅

-   BigQuery의 dry run 기능 활용
-   실제 데이터를 스캔하지 않고 스캔할 데이터량만 계산
-   On-demand 과금 기준과 정확히 일치
-   실제 비용 예측에 가장 적합

**C. Dry run으로 반환될 바이트 수 추정**

-   반환되는 결과 크기는 BigQuery 과금과 무관
-   On-demand는 스캔한 데이터량으로 과금하므로 잘못된 접근
-   결과 크기와 실제 비용 간의 연관성 없음
-   사용자에게 반환될 쿼리 결과의 크기를 의미한다.

**D. count(\*)로 레코드 수 확인**

-   행 개수는 BigQuery On-demand 과금 기준이 아님
-   스캔한 데이터의 바이트 수가 중요
-   행 수가 많아도 컬럼이 적으면 스캔 데이터량은 적을 수 있음
-   정확한 비용 예측 불가

#### BigQuery Dry Run 사용법

**명령줄에서 dry run 실행**

```bash
# dry run으로 스캔할 데이터량 확인
bq query --use_legacy_sql=false --dry_run \
'SELECT customer_id, order_total, order_date
 FROM `project.dataset.orders`
 WHERE order_date >= "2023-01-01"'

# 출력 예시:
Query successfully validated. Assuming the tables are not modified,
running this query will process 2.5 GB of data.
```

**웹 UI에서 dry run**

```
1. BigQuery Console 접속
2. 쿼리 입력
3. "More" 버튼 → "Query validator" 클릭
4. 스캔할 데이터량 표시: "This query will process 2.5 GB"
```

#### 비용 계산 과정

**1단계: Dry run으로 스캔 데이터량 확인**

```bash
Query will process 2.5 GB of data
```

**2단계: 비용 계산**

```
2.5 GB = 0.0025 TB
비용 = 0.0025 TB × $6.25/TB = $0.015625 (약 1.6센트)
```

**3단계: Pricing Calculator 활용**

```
Google Cloud Pricing Calculator에서:
- BigQuery 선택
- On-demand queries
- Query data: 2.5 GB 입력
- 결과: $0.02 (반올림된 비용)
```

#### Dry Run의 장점

**정확성**

-   실제 쿼리 실행 시 스캔할 데이터량과 동일
-   파티션 제거, 컬럼 프루닝 등 최적화 반영
-   가장 정확한 비용 예측

**안전성**

-   실제 데이터를 스캔하지 않음
-   비용 발생 없이 예측 가능
-   쿼리 문법 검증도 동시에 수행

**빠른 실행**

-   메타데이터만 확인하므로 빠름
-   대용량 테이블도 몇 초 내 결과 확인
-   반복적인 비용 예측 가능

#### 스캔 데이터량에 영향을 주는 요소

**테이블 구조**

```sql
-- 모든 컬럼 스캔 (많은 데이터)
SELECT * FROM large_table

-- 특정 컬럼만 스캔 (적은 데이터)
SELECT id, name FROM large_table
```

**파티션 활용**

```sql
-- 전체 파티션 스캔
SELECT * FROM partitioned_table

-- 특정 파티션만 스캔 (데이터량 대폭 감소)
SELECT * FROM partitioned_table
WHERE _PARTITIONTIME >= '2023-01-01'
```

**클러스터링 활용**

```sql
-- 클러스터 컬럼으로 필터링 시 스캔량 감소
SELECT * FROM clustered_table
WHERE cluster_column = 'specific_value'
```

#### 정답: B. 명령줄을 사용하여 dry run 쿼리로 읽을 바이트 수를 추정한 후 Pricing Calculator로 바이트 추정치를 달러로 변환

**선택 이유**:

1. BigQuery On-demand 과금 기준과 정확히 일치 (스캔한 데이터량 기준)
2. Dry run 기능으로 실제 비용 발생 없이 정확한 데이터량 예측 가능
3. 쿼리 최적화 효과가 반영된 실제 스캔량 확인
4. Pricing Calculator로 정확한 달러 금액 변환 가능
5. Google Cloud 공식 권장 방법
6. 빠르고 안전한 비용 예측 방법

:::

## dump 16

You have a single binary application that you want to run on Google Cloud Platform. You decided to automatically scale the application based on underlying infrastructure CPU usage. Your organizational policies require you to use virtual machines directly. You need to ensure that the application scaling is operationally efficient and completed as quickly as possible. What should you do?

A. Create a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.
B. Create an instance template, and use the template in a managed instance group with autoscaling configured.
C. Create an instance template, and use the template in a managed instance group that scales up and down based on the time of day.
D. Use a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring.

:::details 풀이

### 영문 문제 해석

**문제**: "Google Cloud Platform에서 실행하려는 단일 바이너리 애플리케이션이 있습니다. 기본 인프라의 CPU 사용률을 기반으로 애플리케이션을 자동으로 확장하기로 결정했습니다. 조직 정책에 따라 가상머신을 직접 사용해야 합니다. 애플리케이션 확장이 운영 효율적이고 가능한 한 빠르게 완료되도록 해야 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Single binary application (단일 바이너리 애플리케이션)
-   Automatically scale based on CPU usage (CPU 사용률 기반 자동 스케일링)
-   Organizational policy requires virtual machines directly (조직 정책상 가상머신 직접 사용 필수)
-   Operationally efficient (운영 효율적)
-   Completed as quickly as possible (가능한 한 빠른 완료)

### 선택지 해석

**A. Google Kubernetes Engine 클러스터를 생성하고 horizontal pod autoscaling을 사용하여 애플리케이션 확장**

-   GKE 클러스터 사용
-   Pod 기반 수평 확장
-   Kubernetes 환경에서 컨테이너 실행
-   VM을 직접 사용하지 않음 (GKE가 VM 관리)

**B. 인스턴스 템플릿을 생성하고 자동 스케일링이 구성된 관리형 인스턴스 그룹에서 템플릿 사용**

-   Instance Template 생성
-   Managed Instance Group (MIG) 사용
-   CPU 기반 자동 스케일링 구성
-   VM 직접 사용

**C. 인스턴스 템플릿을 생성하고 시간대에 따라 확장/축소하는 관리형 인스턴스 그룹에서 템플릿 사용**

-   Instance Template 및 MIG 사용
-   시간 기반 스케일링 (CPU 기반 아님)
-   예측 가능한 패턴 기반 확장
-   실제 CPU 사용률과 무관한 스케일링

**D. 써드파티 도구를 사용하여 Stackdriver CPU 사용량 모니터링을 기반으로 애플리케이션 확장/축소 자동화 구축**

-   외부 도구 사용
-   사용자 정의 자동화 스크립트
-   Stackdriver(현재 Cloud Monitoring) 메트릭 기반
-   수동 구현 필요

### 문제 분석 및 정답 도출

#### 조직 정책 요구사항 분석

**"가상머신 직접 사용" 요구사항**

-   VM 인스턴스를 직접 관리해야 함
-   Kubernetes 같은 추상화 계층 사용 불가
-   컨테이너가 아닌 VM에서 바이너리 직접 실행

#### 각 선택지 상세 분석

**A. GKE + Horizontal Pod Autoscaling**

-   ❌ 조직 정책 위반: VM 직접 사용하지 않음
-   ❌ GKE가 VM을 추상화하여 관리
-   ✅ CPU 기반 자동 스케일링 지원
-   ✅ 빠른 스케일링 속도
-   ✅ 운영 효율적

**B. Instance Template + MIG + Autoscaling**

-   ✅ VM 직접 사용 (조직 정책 준수)
-   ✅ CPU 기반 자동 스케일링 지원
-   ✅ Google Cloud 네이티브 솔루션 (운영 효율적)
-   ✅ 빠른 스케일링 (사전 구성된 템플릿 사용)
-   ✅ 모든 요구사항 충족

**C. MIG + 시간 기반 스케일링**

-   ✅ VM 직접 사용
-   ❌ CPU 기반이 아닌 시간 기반 스케일링
-   ❌ 실제 워크로드와 무관한 스케일링
-   ❌ 요구사항 불충족

**D. 써드파티 도구 + 사용자 정의 자동화**

-   ✅ VM 직접 사용 가능
-   ✅ CPU 기반 스케일링 가능
-   ❌ 운영 비효율적 (사용자 정의 구현 필요)
-   ❌ 개발 및 유지보수 부담
-   ❌ 느린 구현 속도

#### Managed Instance Group (MIG)의 장점

**자동 스케일링 기능**

```yaml
# 자동 스케일링 정책 예시
autoscaling:
    minNumReplicas: 1
    maxNumReplicas: 10
    cpuUtilization:
        utilizationTarget: 0.6 # CPU 60% 기준
    coolDownPeriodSec: 60 # 60초 쿨다운
```

**빠른 인스턴스 프로비저닝**

-   Instance Template에 모든 구성 사전 정의
-   부팅 디스크, 네트워크, 메타데이터 등 미리 설정
-   스케일 아웃 시 템플릿 기반으로 빠른 VM 생성

**운영 효율성**

-   Google Cloud 네이티브 솔루션
-   자동화된 헬스 체크 및 복구
-   로드 밸런싱과의 통합
-   모니터링 및 로깅 자동 구성

#### Instance Template 구성 예시

```yaml
# instance-template.yaml
name: app-template
properties:
    machineType: n1-standard-2
    disks:
        - boot: true
          initializeParams:
              sourceImage: projects/debian-cloud/global/images/family/debian-11
    networkInterfaces:
        - network: global/networks/default
    metadata:
        items:
            - key: startup-script
              value: |
                  #!/bin/bash
                  # 바이너리 애플리케이션 다운로드 및 실행
                  gsutil cp gs://my-bucket/my-app ./my-app
                  chmod +x ./my-app
                  ./my-app
    tags:
        items:
            - http-server
```

#### MIG 자동 스케일링 구성

```bash
# MIG 생성
gcloud compute instance-groups managed create app-group \
  --template=app-template \
  --size=2 \
  --zone=us-central1-a

# 자동 스케일링 설정
gcloud compute instance-groups managed set-autoscaling app-group \
  --max-num-replicas=10 \
  --min-num-replicas=1 \
  --target-cpu-utilization=0.6 \
  --cool-down-period=60 \
  --zone=us-central1-a
```

#### 스케일링 속도 비교

**MIG 자동 스케일링**

```
1. CPU 사용률 60% 초과 감지
2. 스케일링 결정 (10-30초)
3. 새 인스턴스 생성 (1-2분)
4. 애플리케이션 시작 (30초-2분)
총 소요 시간: 2-5분
```

**써드파티 도구 방식**

```
1. 모니터링 데이터 수집 (1-5분)
2. 사용자 정의 로직 실행 (30초-2분)
3. API 호출로 VM 생성 (1-2분)
4. 애플리케이션 배포 및 시작 (2-5분)
총 소요 시간: 5-15분
```

#### 운영 효율성 비교

**MIG 방식 (선택지 B)**

```
✅ 설정 한 번으로 자동화 완료
✅ Google Cloud 통합 모니터링
✅ 자동 헬스 체크 및 복구
✅ 롤링 업데이트 지원
✅ 부하 분산 자동 연동
```

**사용자 정의 방식 (선택지 D)**

```
❌ 스크립트 개발 및 테스트 필요
❌ 모니터링 시스템 별도 구축
❌ 오류 처리 로직 구현 필요
❌ 유지보수 부담
❌ 보안 및 권한 관리 복잡
```

#### 조직 정책 준수

**VM 직접 사용 요구사항**

```
MIG 방식:
- 실제 Compute Engine VM 인스턴스 사용
- VM에서 바이너리 애플리케이션 직접 실행
- VM의 CPU, 메모리, 디스크 직접 관리
→ 정책 완벽 준수

GKE 방식:
- VM 위에 Kubernetes 추상화 계층 존재
- 컨테이너로 애플리케이션 실행
- VM 직접 관리하지 않음
→ 정책 위반
```

#### 정답: B. 인스턴스 템플릿을 생성하고 자동 스케일링이 구성된 관리형 인스턴스 그룹에서 템플릿 사용

**선택 이유**:

1. 조직 정책 완벽 준수: VM 직접 사용
2. CPU 기반 자동 스케일링 지원
3. 운영 효율성: Google Cloud 네이티브 솔루션
4. 빠른 스케일링: 사전 구성된 템플릿 기반 신속한 프로비저닝
5. 통합 관리: 모니터링, 로깅, 헬스체크 자동화
6. 확장성: 다중 존 및 리전 지원
7. 비용 효율성: 불필요한 써드파티 도구 비용 없음

MIG는 인스턴스 템플릿 기반으로만 VM을 생성하기 때문에 기존 VM들을 가져올 수 없다. 기존 인스턴스를 그룹화하려면 Unmanaged Instance Group (UIG) 방식을 사용해야 한다.

:::

## dump 17

You are analyzing Google Cloud Platform service costs from three separate projects. You want to use this information to create service cost estimates by service type, daily and monthly, for the next six months using standard query syntax. What should you do?

A. Export your bill to a Cloud Storage bucket, and then import into Cloud Bigtable for analysis.
B. Export your bill to a Cloud Storage bucket, and then import into Google Sheets for analysis.
C. Export your transactions to a local file, and perform analysis with a desktop tool.
D. Export your bill to a BigQuery dataset, and then write time window-based SQL queries for analysis.

:::details 풀이

### 영문 문제 해석

**문제**: "3개의 별도 프로젝트에서 Google Cloud Platform 서비스 비용을 분석하고 있습니다. 이 정보를 사용하여 향후 6개월 동안 서비스 타입별로 일별 및 월별 서비스 비용 추정치를 표준 쿼리 구문으로 생성하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Analyzing GCP service costs from 3 projects (3개 프로젝트의 GCP 서비스 비용 분석)
-   Create service cost estimates by service type (서비스 타입별 비용 추정)
-   Daily and monthly analysis (일별 및 월별 분석)
-   Next six months forecast (향후 6개월 예측)
-   Using standard query syntax (표준 쿼리 구문 사용)

### 선택지 해석

**A. 청구서를 Cloud Storage 버킷으로 내보낸 후 분석을 위해 Cloud Bigtable로 가져오기**

-   Cloud Storage 중간 저장소 사용
-   Cloud Bigtable을 분석 도구로 활용
-   NoSQL 데이터베이스 방식

**B. 청구서를 Cloud Storage 버킷으로 내보낸 후 분석을 위해 Google Sheets로 가져오기**

-   Cloud Storage 중간 저장소 사용
-   Google Sheets 스프레드시트로 분석
-   GUI 기반 분석 도구

**C. 거래 내역을 로컬 파일로 내보내고 데스크톱 도구로 분석 수행**

-   로컬 환경으로 데이터 다운로드
-   데스크톱 분석 도구 사용 (Excel, 기타 도구)
-   오프라인 분석 방식

**D. 청구서를 BigQuery 데이터셋으로 내보낸 후 시간 윈도우 기반 SQL 쿼리로 분석 작성**

-   BigQuery 데이터 웨어하우스 활용
-   SQL 쿼리를 통한 분석
-   시계열 데이터 분석 최적화

### 문제 분석 및 정답 도출

#### 요구사항 세부 분석

**"표준 쿼리 구문 사용" 요구사항**

-   SQL과 같은 표준화된 쿼리 언어 필요
-   복잡한 데이터 조작 및 집계 기능 필요
-   재사용 가능한 쿼리 스크립트 작성

**"서비스 타입별, 일별/월별 분석" 요구사항**

-   다차원 데이터 분석 (서비스별, 시간별)
-   시계열 데이터 처리
-   집계 및 그룹화 기능

**"향후 6개월 예측" 요구사항**

-   대용량 데이터 처리
-   복잡한 통계 분석
-   시계열 예측 모델링

#### 각 선택지 상세 분석

**A. Cloud Bigtable**

-   ❌ NoSQL 데이터베이스로 복잡한 집계 쿼리에 부적합
-   ❌ 표준 SQL 쿼리 지원하지 않음
-   ❌ 비용 분석보다는 실시간 애플리케이션에 특화
-   ❌ 시계열 분석 도구로서 제한적

**B. Google Sheets**

-   ❌ 대용량 데이터 처리 한계 (100만 행 제한)
-   ❌ 복잡한 SQL 쿼리 지원 안함
-   ❌ 3개 프로젝트 데이터 통합 분석 어려움
-   ❌ 자동화 및 스케일링 제한

**C. 로컬 데스크톱 도구**

-   ❌ 클라우드 환경과 분리되어 실시간 업데이트 어려움
-   ❌ 대용량 데이터 다운로드 시간 및 저장 공간 문제
-   ❌ 협업 및 공유 어려움
-   ❌ 자동화 제한적

**D. BigQuery + SQL**

-   ✅ 표준 SQL 쿼리 완벽 지원
-   ✅ 페타바이트 규모 데이터 처리 가능
-   ✅ 시계열 데이터 분석에 최적화
-   ✅ 복잡한 집계 및 분석 함수 제공
-   ✅ 자동화 및 스케줄링 지원

#### BigQuery를 통한 청구 데이터 분석

**청구 데이터 내보내기 설정**

```bash
# BigQuery로 청구 데이터 자동 내보내기 설정
gcloud beta billing accounts get-iam-policy ACCOUNT_ID
gcloud beta billing export create \
  --billing-account=ACCOUNT_ID \
  --dataset=PROJECT_ID:billing_dataset
```

**표준 SQL을 통한 분석 쿼리 예시**

**서비스별 일별 비용 분석**

```sql
-- 서비스별 일별 비용 추이
SELECT
  service.description AS service_name,
  usage_start_time AS usage_date,
  SUM(cost) AS daily_cost,
  project.id AS project_id
FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
GROUP BY service_name, usage_date, project_id
ORDER BY usage_date DESC, daily_cost DESC
```

**월별 비용 집계 및 예측**

```sql
-- 월별 서비스 비용 집계 및 트렌드 분석
WITH monthly_costs AS (
  SELECT
    service.description AS service_name,
    EXTRACT(YEAR FROM usage_start_time) AS year,
    EXTRACT(MONTH FROM usage_start_time) AS month,
    SUM(cost) AS monthly_cost
  FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
  WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)
  GROUP BY service_name, year, month
),
trend_analysis AS (
  SELECT
    service_name,
    AVG(monthly_cost) AS avg_monthly_cost,
    STDDEV(monthly_cost) AS cost_stddev,
    -- 선형 회귀를 통한 트렌드 계산
    CORR(month, monthly_cost) AS cost_trend
  FROM monthly_costs
  GROUP BY service_name
)
SELECT
  service_name,
  avg_monthly_cost,
  -- 향후 6개월 예상 비용
  avg_monthly_cost * (1 + cost_trend * 0.1) AS projected_cost_6m
FROM trend_analysis
ORDER BY avg_monthly_cost DESC
```

**3개 프로젝트 통합 분석**

```sql
-- 프로젝트별 서비스 비용 비교
SELECT
  project.id AS project_name,
  service.description AS service_name,
  DATE_TRUNC(usage_start_time, MONTH) AS month,
  SUM(cost) AS monthly_cost,
  -- 프로젝트 간 비용 비율
  SUM(cost) / SUM(SUM(cost)) OVER (PARTITION BY DATE_TRUNC(usage_start_time, MONTH)) AS cost_ratio
FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
GROUP BY project_name, service_name, month
ORDER BY month DESC, monthly_cost DESC
```

#### BigQuery의 고급 분석 기능

**시계열 예측 함수**

```sql
-- BigQuery ML을 활용한 비용 예측
CREATE MODEL `project.billing_dataset.cost_forecast_model`
OPTIONS (
  model_type='ARIMA_PLUS',
  time_series_timestamp_col='usage_date',
  time_series_data_col='daily_cost',
  time_series_id_col='service_name'
) AS
SELECT
  DATE(usage_start_time) AS usage_date,
  service.description AS service_name,
  SUM(cost) AS daily_cost
FROM `project.billing_dataset.gcp_billing_export_v1_ACCOUNT_ID`
WHERE usage_start_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)
GROUP BY usage_date, service_name
```

**예측 결과 조회**

```sql
-- 향후 6개월 비용 예측
SELECT *
FROM ML.FORECAST(
  MODEL `project.billing_dataset.cost_forecast_model`,
  STRUCT(180 AS horizon, 0.8 AS confidence_level)
)
ORDER BY service_name, forecast_timestamp
```

##### 다른 선택지들의 한계

**Cloud Bigtable 한계**

```
- SQL 지원 안함 → 표준 쿼리 구문 요구사항 불충족
- 스키마리스 NoSQL → 구조화된 청구 데이터 분석에 부적합
- 집계 함수 제한 → 복잡한 비용 분석 어려움
```

**Google Sheets 한계**

```
- 행 수 제한 → 대용량 청구 데이터 처리 불가
- 고급 SQL 함수 없음 → 시계열 분석 제한
- 자동화 어려움 → 정기적 분석 및 예측 어려움
```

**로컬 도구 한계**

```
- 클라우드 통합 부족 → 실시간 데이터 업데이트 어려움
- 협업 제한 → 팀 단위 분석 어려움
- 스케일링 한계 → 대용량 데이터 처리 제약
```

#### BigQuery 사용의 추가 이점

**비용 효율성**

-   쿼리당 과금으로 분석 비용 최적화
-   자동 스케일링으로 리소스 효율성

**통합 및 확장성**

-   다른 Google Cloud 서비스와 완벽 통합
-   실시간 데이터 스트리밍 지원
-   BigQuery ML을 통한 고급 예측 분석

**표준화 및 재사용성**

-   표준 SQL 99 지원
-   쿼리 스케줄링 및 자동화
-   대시보드 및 리포팅 도구 연동

#### 정답: D. 청구서를 BigQuery 데이터셋으로 내보낸 후 시간 윈도우 기반 SQL 쿼리로 분석 작성

**선택 이유**:

1. 표준 SQL 쿼리 구문 완벽 지원으로 요구사항 충족
2. 대용량 시계열 데이터 분석에 최적화된 아키텍처
3. 서비스별, 일별/월별 다차원 분석 기능 제공
4. 복잡한 집계 및 예측 분석 함수 지원
5. 3개 프로젝트 데이터 통합 분석 용이
6. 자동화 및 스케줄링을 통한 정기적 분석 가능
7. BigQuery ML을 활용한 향후 6개월 비용 예측 지원
8. Google Cloud 네이티브 통합으로 실시간 데이터 업데이트

:::

## dump 18

You need to set up a policy so that videos stored in a specific Cloud Storage Regional bucket are moved to Coldline after 90 days, and then deleted after one year from their creation. How should you set up the policy?

A. Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 275 days (365 ג€" 90)
B. Use Cloud Storage Object Lifecycle Management using Age conditions with SetStorageClass and Delete actions. Set the SetStorageClass action to 90 days and the Delete action to 365 days.
C. Use gsutil rewrite and set the Delete action to 275 days (365-90).
D. Use gsutil rewrite and set the Delete action to 365 days.

:::details 풀이

### 영문 문제 해석

**문제**: "특정 Cloud Storage Regional 버킷에 저장된 비디오가 90일 후 Coldline으로 이동되고, 생성 후 1년 후에 삭제되도록 정책을 설정해야 합니다. 정책을 어떻게 설정해야 할까요?"

**핵심 요구사항**:

-   Videos in Cloud Storage Regional bucket (Regional 버킷의 비디오 파일)
-   Move to Coldline after 90 days (90일 후 Coldline으로 이동)
-   Delete after one year from creation (생성 후 1년 후 삭제)
-   Set up policy (정책 설정)

### 선택지 해석

**A. Age 조건으로 Cloud Storage Object Lifecycle Management를 사용하여 SetStorageClass와 Delete 액션 설정. SetStorageClass 액션을 90일로, Delete 액션을 275일(365-90)로 설정**

-   Object Lifecycle Management 사용
-   90일: Regional → Coldline 이동
-   275일: 삭제 (총 365일 - 90일 = 275일)
-   상대적 날짜 계산 방식

**B. Age 조건으로 Cloud Storage Object Lifecycle Management를 사용하여 SetStorageClass와 Delete 액션 설정. SetStorageClass 액션을 90일로, Delete 액션을 365일로 설정**

-   Object Lifecycle Management 사용
-   90일: Regional → Coldline 이동
-   365일: 삭제 (생성일 기준 절대 날짜)
-   절대적 날짜 계산 방식

**C. gsutil rewrite를 사용하고 Delete 액션을 275일(365-90)로 설정**

-   gsutil rewrite 명령어 사용 (수동 도구)
-   275일 후 삭제 설정
-   라이프사이클 정책이 아닌 일회성 작업

**D. gsutil rewrite를 사용하고 Delete 액션을 365일로 설정**

-   gsutil rewrite 명령어 사용 (수동 도구)
-   365일 후 삭제 설정
-   라이프사이클 정책이 아닌 일회성 작업

### 문제 분석 및 정답 도출

#### Cloud Storage Object Lifecycle Management 이해

**라이프사이클 정책의 작동 방식**

-   모든 조건과 액션은 **객체 생성일(Creation Date) 기준**으로 계산
-   Age 조건: 객체가 생성된 후 경과된 일수
-   각 액션은 독립적으로 생성일부터 계산

**Age 조건의 정확한 의미**

```json
{
    "lifecycle": {
        "rule": [
            {
                "condition": { "age": 90 },
                "action": {
                    "type": "SetStorageClass",
                    "storageClass": "COLDLINE"
                }
            },
            {
                "condition": { "age": 365 },
                "action": { "type": "Delete" }
            }
        ]
    }
}
```

#### 각 선택지 상세 분석

**A. SetStorageClass: 90일, Delete: 275일**

```
타임라인:
Day 0: 객체 생성 (Regional)
Day 90: Coldline으로 이동 ✓
Day 275: 삭제 실행 ❌ (너무 이름)

문제점:
- 생성 후 275일에 삭제 = 약 9개월 후 삭제
- 요구사항인 "1년 후 삭제"와 불일치
- 잘못된 계산 방식 적용
```

**B. SetStorageClass: 90일, Delete: 365일**

```
타임라인:
Day 0: 객체 생성 (Regional)
Day 90: Coldline으로 이동 ✓
Day 365: 삭제 실행 ✓ (정확히 1년 후)

결과:
- 90일 후 Coldline 이동: 요구사항 충족
- 365일(1년) 후 삭제: 요구사항 충족
- 올바른 절대 날짜 계산 방식
```

**C. gsutil rewrite + Delete: 275일**

```
문제점:
- gsutil rewrite는 일회성 수동 명령어
- 자동 라이프사이클 정책 아님
- 새로 업로드되는 객체에 자동 적용 안됨
- 275일 계산도 잘못됨
```

**D. gsutil rewrite + Delete: 365일**

```
문제점:
- gsutil rewrite는 라이프사이클 정책이 아님
- Storage Class 변경 기능 누락
- 수동 개입 필요
- 자동화 불가능
```

#### Object Lifecycle Management vs gsutil rewrite

**Object Lifecycle Management (올바른 접근)**

```json
{
    "lifecycle": {
        "rule": [
            {
                "condition": { "age": 90 },
                "action": {
                    "type": "SetStorageClass",
                    "storageClass": "COLDLINE"
                }
            },
            {
                "condition": { "age": 365 },
                "action": { "type": "Delete" }
            }
        ]
    }
}
```

**장점:**

-   자동화된 정책 실행
-   새로운 객체에 자동 적용
-   지속적인 관리 불필요
-   다양한 조건과 액션 지원

**gsutil rewrite (부적절한 도구)**

```bash
# 일회성 명령어 (자동화 안됨)
gsutil rewrite -s COLDLINE gs://bucket-name/*
gsutil lifecycle set lifecycle.json gs://bucket-name
```

**단점:**

-   수동 실행 필요
-   기존 객체에만 적용
-   스토리지 클래스 변경과 삭제를 동시에 처리 어려움
-   지속적인 정책 적용 불가능

#### Age 조건 계산 방식의 오해

**잘못된 이해 (선택지 A의 오류)**

```
생각: "90일 후 Coldline 이동, 그 후 275일 더 지나서 삭제"
Day 0: 생성
Day 90: Coldline 이동
Day 90+275=365: 삭제

실제 Cloud Storage 동작:
❌ 이런 방식으로 작동하지 않음
```

**올바른 이해 (선택지 B)**

```
모든 조건은 생성일 기준:
Day 0: 객체 생성
Day 90: age=90 조건 만족 → Coldline 이동
Day 365: age=365 조건 만족 → 삭제

각 액션은 독립적으로 생성일부터 계산됨
```

#### 실제 라이프사이클 정책 설정

**JSON 구성 파일**

```json
{
    "lifecycle": {
        "rule": [
            {
                "condition": {
                    "age": 90
                },
                "action": {
                    "type": "SetStorageClass",
                    "storageClass": "COLDLINE"
                }
            },
            {
                "condition": {
                    "age": 365
                },
                "action": {
                    "type": "Delete"
                }
            }
        ]
    }
}
```

**정책 적용 명령어**

```bash
# 라이프사이클 정책 적용
gsutil lifecycle set lifecycle.json gs://my-video-bucket

# 정책 확인
gsutil lifecycle get gs://my-video-bucket
```

#### 라이프사이클 정책의 실행 과정

**Cloud Storage의 자동 실행**

```
매일 자동 스캔:
1. 버킷 내 모든 객체의 age 계산
2. 조건 만족하는 객체 식별
3. 해당 액션 자동 실행
4. 로그 기록 및 모니터링

사용자 개입 불필요!
```

#### 비용 최적화 효과

**Regional → Coldline → Delete**

```
비용 변화:
- Regional Storage: $0.020/GB/month
- Coldline Storage: $0.004/GB/month
- 90일 후: 80% 스토리지 비용 절약
- 365일 후: 100% 비용 절약 (삭제)

대용량 비디오 파일의 경우 상당한 비용 절약
```

#### 다른 Storage Class 옵션

**왜 Coldline인가?**

```
Nearline: 월 1회 미만 접근 (30일 최소 보관)
Coldline: 분기 1회 미만 접근 (90일 최소 보관) ✓
Archive: 연 1회 미만 접근 (365일 최소 보관)

90일 후 이동이므로 Coldline이 적절
```

#### 정답: B. Age 조건으로 Cloud Storage Object Lifecycle Management를 사용하여 SetStorageClass와 Delete 액션 설정. SetStorageClass 액션을 90일로, Delete 액션을 365일로 설정

**선택 이유**:

1. Object Lifecycle Management는 자동화된 정책 관리의 표준 방법
2. Age 조건은 모든 액션이 생성일 기준으로 독립적으로 계산됨
3. 90일: Regional → Coldline 이동 요구사항 정확히 충족
4. 365일: 생성 후 정확히 1년 후 삭제 요구사항 충족
5. 새로운 객체에 자동으로 적용되는 지속적 정책
6. 수동 개입 없이 완전 자동화된 라이프사이클 관리
7. 비용 최적화와 스토리지 관리 효율성 제공

`gsutil rewrite` 명령어는 기존 클라우드 스토리지 객체 메타데이터나 암호화 설정을 변경하는 명령어이다.

:::

## dump 19

You have a Linux VM that must connect to Cloud SQL. You created a service account with the appropriate access rights. You want to make sure that the VM uses this service account instead of the default Compute Engine service account. What should you do?

A. When creating the VM via the web console, specify the service account under the 'Identity and API Access' section.
B. Download a JSON Private Key for the service account. On the Project Metadata, add that JSON as the value for the key compute-engine-service- account.
C. Download a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-engine- service-account.
D. Download a JSON Private Key for the service account. After creating the VM, ssh into the VM and save the JSON under ~/.gcloud/compute-engine-service- account.json.

:::details 풀이

### 영문 문제 해석

**문제**: "Cloud SQL에 연결해야 하는 Linux VM이 있습니다. 적절한 액세스 권한을 가진 서비스 계정을 생성했습니다. VM이 기본 Compute Engine 서비스 계정 대신 이 서비스 계정을 사용하도록 하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Linux VM that must connect to Cloud SQL (Cloud SQL에 연결할 Linux VM)
-   Service account with appropriate access rights (적절한 권한을 가진 서비스 계정)
-   VM uses this service account instead of default (기본 대신 특정 서비스 계정 사용)
-   Proper configuration method (올바른 설정 방법)

### 선택지 해석

**A. 웹 콘솔을 통해 VM 생성 시 'Identity and API Access' 섹션에서 서비스 계정 지정**

-   VM 생성 시점에서 서비스 계정 설정
-   Google Cloud Console UI를 통한 설정
-   공식적인 VM 구성 방법

**B. 서비스 계정의 JSON Private Key를 다운로드하여 Project Metadata에 compute-engine-service-account 키 값으로 추가**

-   JSON 키 파일 다운로드 방식
-   프로젝트 수준 메타데이터 설정
-   전체 프로젝트에 영향

**C. 서비스 계정의 JSON Private Key를 다운로드하여 VM의 Custom Metadata에 compute-engine-service-account 키 값으로 추가**

-   JSON 키 파일 다운로드 방식
-   VM별 커스텀 메타데이터 설정
-   특정 VM에만 영향

**D. 서비스 계정의 JSON Private Key를 다운로드하여 VM 생성 후 ssh로 접속하여 ~/.gcloud/compute-engine-service-account.json에 저장**

-   JSON 키 파일 다운로드 방식
-   수동으로 VM 내부에 파일 저장
-   파일 시스템 기반 설정

### 문제 분석 및 정답 도출

#### Google Cloud VM 서비스 계정 설정 방식

**공식적인 방법: VM 생성 시 서비스 계정 할당**

```
VM 생성 과정:
1. Compute Engine → VM instances → Create instance
2. Identity and API Access 섹션
3. Service account 드롭다운에서 원하는 서비스 계정 선택
4. Access scopes 설정
```

**Google Cloud의 권장 아키텍처**

-   VM과 서비스 계정의 바인딩은 VM 메타데이터 수준에서 관리
-   JSON 키 파일 다운로드는 보안상 권장하지 않음
-   네이티브 IAM 통합 방식 사용

#### 각 선택지 상세 분석

**A. VM 생성 시 Identity and API Access에서 설정**

-   ✅ Google Cloud의 공식 권장 방법
-   ✅ 보안이 가장 우수 (키 파일 노출 없음)
-   ✅ VM 메타데이터 서버를 통한 자동 인증
-   ✅ 키 로테이션 자동 처리
-   ✅ IAM과 완전 통합

**B. Project Metadata에 JSON 키 저장**

-   ❌ 비공식적인 방법
-   ❌ JSON 키 파일 노출 위험
-   ❌ 프로젝트 전체에 영향 (과도한 범위)
-   ❌ 키 관리 복잡성
-   ❌ compute-engine-service-account는 실제 메타데이터 키가 아님

**C. VM Custom Metadata에 JSON 키 저장**

-   ❌ 비공식적인 방법
-   ❌ JSON 키 파일 보안 위험
-   ❌ 메타데이터를 통한 키 노출 가능성
-   ❌ 수동 키 관리 필요
-   ❌ compute-engine-service-account는 유효한 메타데이터 키가 아님

**D. VM 내부 파일 시스템에 JSON 키 저장**

-   ❌ 매우 비보안적인 방법
-   ❌ 파일 시스템 접근으로 키 노출 위험
-   ❌ 키 로테이션 수동 처리 필요
-   ❌ ~/.gcloud/ 경로는 올바른 위치가 아님
-   ❌ VM 재시작 시 설정 유지 문제

#### Google Cloud 서비스 계정 인증 메커니즘

**메타데이터 서버를 통한 인증 (권장 방법)**

```bash
# VM 내에서 자동으로 동작하는 방식
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# 애플리케이션에서 자동 인증
gcloud auth application-default print-access-token
```

**JSON 키 파일 방식 (권장하지 않음)**

```bash
# 수동으로 설정해야 하는 방식
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"
gcloud auth activate-service-account --key-file=/path/to/key.json
```

#### VM 생성 시 서비스 계정 설정 과정

**Google Cloud Console에서**

```
1. Compute Engine → VM instances
2. CREATE INSTANCE 클릭
3. Identity and API access 섹션 확장
4. Service account 드롭다운:
   - Compute Engine default service account
   - [사용자 정의 서비스 계정] ← 여기서 선택
5. Access scopes:
   - Allow default access
   - Allow full access to all Cloud APIs
   - Set access for each API (권장)
```

**gcloud 명령어로**

```bash
gcloud compute instances create my-vm \
  --service-account=my-service-account@project.iam.gserviceaccount.com \
  --scopes=https://www.googleapis.com/auth/sqlservice.admin
```

#### Cloud SQL 연결을 위한 권한 설정

**필요한 IAM 역할**

```
서비스 계정에 부여해야 할 권한:
- Cloud SQL Client (roles/cloudsql.client)
- 또는 Cloud SQL Editor (roles/cloudsql.editor)
```

**액세스 스코프 설정**

```
VM의 액세스 스코프:
- https://www.googleapis.com/auth/sqlservice.admin
- 또는 "Allow full access to all Cloud APIs"
```

#### 보안 모범 사례

**권장 사항 (선택지 A)**

```
✅ VM 생성 시 서비스 계정 할당
✅ 최소 권한 원칙 적용
✅ 액세스 스코프 세밀하게 설정
✅ JSON 키 파일 사용 피하기
✅ IAM 조건부 액세스 활용
```

**지양해야 할 방식 (선택지 B, C, D)**

```
❌ JSON 키 파일 다운로드
❌ 키 파일을 메타데이터나 파일시스템에 저장
❌ 수동 키 관리
❌ 과도한 권한 부여
❌ 키 하드코딩
```

#### 실제 구현 예시

**올바른 방법 (선택지 A)**

```bash
# VM 생성 시 서비스 계정 지정
gcloud compute instances create sql-client-vm \
  --service-account=cloudsql-client@my-project.iam.gserviceaccount.com \
  --scopes=https://www.googleapis.com/auth/sqlservice.admin \
  --zone=us-central1-a

# VM 내에서 자동 인증 확인
gcloud auth list
# * cloudsql-client@my-project.iam.gserviceaccount.com

# Cloud SQL 연결 테스트
gcloud sql instances list
```

#### VM 생성 후 서비스 계정 변경

**기존 VM의 서비스 계정 변경**

```bash
# VM 중지 필요
gcloud compute instances stop my-vm --zone=us-central1-a

# 서비스 계정 변경
gcloud compute instances set-service-account my-vm \
  --service-account=new-service-account@project.iam.gserviceaccount.com \
  --scopes=https://www.googleapis.com/auth/sqlservice.admin \
  --zone=us-central1-a

# VM 시작
gcloud compute instances start my-vm --zone=us-central1-a
```

#### Cloud SQL 연결 확인

**연결 테스트**

```bash
# VM 내에서 Cloud SQL 연결 확인
gcloud sql connect my-instance --user=root

# 또는 직접 연결
mysql -h [INSTANCE_IP] -u root -p
```

#### 정답: A. 웹 콘솔을 통해 VM 생성 시 'Identity and API Access' 섹션에서 서비스 계정 지정

**선택 이유**:

1. Google Cloud의 공식 권장 방법으로 가장 안전하고 표준적
2. JSON 키 파일 다운로드 없이 네이티브 IAM 통합 활용
3. 메타데이터 서버를 통한 자동 인증으로 보안성 극대화
4. 키 로테이션 및 관리가 Google에 의해 자동 처리
5. VM과 서비스 계정 간의 바인딩이 플랫폼 수준에서 관리
6. Cloud SQL 연결에 필요한 권한을 안전하게 제공
7. 확장성과 유지보수성이 뛰어난 솔루션

:::

## dump 20

You created an instance of SQL Server 2017 on Compute Engine to test features in the new version. You want to connect to this instance using the fewest number of steps. What should you do?

A. Install a RDP client on your desktop. Verify that a firewall rule for port 3389 exists.
B. Install a RDP client in your desktop. Set a Windows username and password in the GCP Console. Use the credentials to log in to the instance.
C. Set a Windows password in the GCP Console. Verify that a firewall rule for port 22 exists. Click the RDP button in the GCP Console and supply the credentials to log in.
D. Set a Windows username and password in the GCP Console. Verify that a firewall rule for port 3389 exists. Click the RDP button in the GCP Console, and supply the credentials to log in.

:::details 풀이

### 영문 문제 해석

**문제**: "새 버전의 기능을 테스트하기 위해 Compute Engine에 SQL Server 2017 인스턴스를 생성했습니다. 최소한의 단계로 이 인스턴스에 연결하려고 합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   SQL Server 2017 instance on Compute Engine (Compute Engine의 SQL Server 2017 인스턴스)
-   Connect using fewest number of steps (최소한의 단계로 연결)
-   Windows VM connection (Windows VM 연결)

### 선택지 해석

**A. 데스크톱에 RDP 클라이언트를 설치하고 포트 3389에 대한 방화벽 규칙이 존재하는지 확인**

-   RDP 클라이언트 별도 설치 필요
-   방화벽 규칙 수동 확인
-   Windows 자격 증명 설정 과정 누락

**B. 데스크톱에 RDP 클라이언트를 설치하고 GCP Console에서 Windows 사용자명과 비밀번호를 설정한 후 자격 증명을 사용하여 인스턴스에 로그인**

-   RDP 클라이언트 별도 설치 필요
-   GCP Console에서 Windows 자격 증명 설정
-   수동 RDP 연결 과정

**C. GCP Console에서 Windows 비밀번호를 설정하고 포트 22에 대한 방화벽 규칙이 존재하는지 확인한 후 GCP Console의 RDP 버튼을 클릭하여 자격 증명 제공**

-   GCP Console 내장 RDP 사용
-   잘못된 포트 22 확인 (SSH 포트)
-   비밀번호만 설정 (사용자명 누락)

**D. GCP Console에서 Windows 사용자명과 비밀번호를 설정하고 포트 3389에 대한 방화벽 규칙이 존재하는지 확인한 후 GCP Console의 RDP 버튼을 클릭하여 자격 증명 제공**

-   GCP Console 내장 RDP 사용
-   올바른 RDP 포트 3389 확인
-   완전한 Windows 자격 증명 설정

### 문제 분석 및 정답 도출

#### Windows VM 연결 방식 이해

**RDP (Remote Desktop Protocol)**

-   Windows VM 원격 접속의 표준 프로토콜
-   기본 포트: 3389
-   그래픽 사용자 인터페이스 제공
-   SQL Server 관리에 적합

**연결 방법 비교**

1. 외부 RDP 클라이언트 + 수동 연결
2. Google Cloud Console 내장 RDP (브라우저 기반)

#### 각 선택지 상세 분석

**A. 외부 RDP 클라이언트 + 방화벽 확인만**

```
단계:
1. RDP 클라이언트 다운로드 및 설치
2. 방화벽 규칙 3389 확인
3. ❌ Windows 자격 증명 설정 누락
4. ❌ 연결 불가능 (자격 증명 없음)

문제점: 인증 정보 없이 연결 시도
```

**B. 외부 RDP 클라이언트 + 완전한 설정**

```
단계:
1. RDP 클라이언트 다운로드 및 설치
2. GCP Console에서 Windows 사용자명/비밀번호 설정
3. 방화벽 규칙 확인 (기본적으로 존재)
4. 외부 IP 확인
5. RDP 클라이언트에서 수동 연결
6. 자격 증명 입력

총 6단계 (상당히 복잡)
```

**C. GCP Console RDP + 잘못된 포트**

```
단계:
1. Windows 비밀번호만 설정 (사용자명 누락)
2. ❌ 포트 22 확인 (SSH 포트, RDP와 무관)
3. GCP Console RDP 버튼 클릭
4. ❌ 불완전한 자격 증명으로 연결 실패

문제점:
- 포트 22는 SSH용 (Linux), RDP는 3389
- 사용자명 설정 누락
```

**D. GCP Console RDP + 올바른 설정**

```
단계:
1. GCP Console에서 Windows 사용자명/비밀번호 설정
2. 방화벽 규칙 3389 확인 (보통 기본 제공)
3. GCP Console RDP 버튼 클릭
4. 자격 증명 제공하여 연결

총 4단계 (가장 간단)
```

#### Google Cloud Console 내장 RDP의 장점

**브라우저 기반 RDP**

```
✅ 별도 소프트웨어 설치 불필요
✅ 브라우저에서 바로 연결
✅ Google Cloud 인증과 통합
✅ 방화벽 설정 자동 처리
✅ 외부 IP 자동 인식
```

**외부 RDP 클라이언트 대비 장점**

```
외부 클라이언트:
- RDP 소프트웨어 설치 필요
- 외부 IP 수동 확인 필요
- 방화벽 설정 수동 확인
- 연결 정보 수동 입력

GCP Console RDP:
- 클릭 한 번으로 연결
- 모든 정보 자동 처리
- 통합된 사용자 경험
```

#### Windows VM 자격 증명 설정

**GCP Console에서 Windows 자격 증명 설정**

```
1. Compute Engine → VM instances
2. 해당 Windows VM 클릭
3. "Set Windows password" 버튼 클릭
4. 사용자명 입력 (예: administrator)
5. 비밀번호 자동 생성 또는 수동 입력
```

**필수 구성 요소**

```
✅ Username: Windows 사용자 계정명
✅ Password: 해당 계정의 비밀번호
❌ 비밀번호만으로는 불충분
```

#### 방화벽 규칙 확인

**RDP 연결을 위한 방화벽**

```bash
# RDP 방화벽 규칙 확인
gcloud compute firewall-rules list --filter="name:default-allow-rdp"

# 기본 RDP 규칙 (보통 자동 생성됨):
# default-allow-rdp  default  INGRESS  1000  tcp:3389  0.0.0.0/0
```

**포트별 용도**

```
Port 22: SSH (Linux 연결용)
Port 3389: RDP (Windows 연결용) ✓
Port 80/443: HTTP/HTTPS (웹 서비스)
Port 1433: SQL Server (데이터베이스 연결)
```

#### SQL Server 2017 관리 맥락

**SQL Server 관리 도구**

```
SQL Server Management Studio (SSMS):
- Windows GUI 기반 관리 도구
- RDP 연결 후 VM 내에서 실행
- SQL Server 인스턴스 관리에 최적

명령줄 도구:
- sqlcmd: 명령줄 기반
- PowerShell: 스크립트 기반
```

**연결 후 작업 과정**

```
1. RDP로 Windows VM 접속
2. SQL Server Management Studio 실행
3. localhost 또는 VM 내부 IP로 SQL Server 연결
4. 새 기능 테스트 및 검증
```

#### 단계 수 최소화 분석

**각 선택지별 실제 단계 수**

**선택지 A: 3-4단계 (실패)**

```
1. RDP 클라이언트 설치
2. 방화벽 규칙 확인
3. 연결 시도
4. ❌ 실패 (자격 증명 없음)
```

**선택지 B: 6-7단계**

```
1. RDP 클라이언트 설치
2. Windows 사용자명/비밀번호 설정
3. 방화벽 규칙 확인
4. VM 외부 IP 확인
5. RDP 클라이언트 실행 및 설정
6. 자격 증명 입력 후 연결
7. 연결 성공
```

**선택지 C: 3-4단계 (실패)**

```
1. Windows 비밀번호 설정 (사용자명 없음)
2. ❌ 잘못된 포트 22 확인
3. RDP 버튼 클릭
4. ❌ 연결 실패 (불완전한 정보)
```

**선택지 D: 3-4단계 (성공)** ✓

```
1. Windows 사용자명/비밀번호 설정
2. 방화벽 규칙 3389 확인 (선택적)
3. GCP Console RDP 버튼 클릭
4. 자격 증명 제공하여 연결 성공
```

#### 실제 구현 과정

**선택지 D의 실제 실행**

```
1. GCP Console → Compute Engine → VM instances
2. Windows VM 선택 → "Set Windows password" 클릭
   - Username: administrator 입력
   - Password: 자동 생성 또는 직접 입력
3. (선택적) 방화벽 규칙 확인
   - 보통 default-allow-rdp 규칙이 자동 존재
4. VM 목록에서 "RDP" 버튼 클릭
5. 브라우저에서 RDP 세션 열림
6. 설정한 자격 증명 입력
7. Windows 데스크톱 접속 완료
```

#### 보안 고려사항

**안전한 RDP 연결**

```
✅ 강력한 Windows 비밀번호 설정
✅ 방화벽 규칙을 특정 IP 대역으로 제한
✅ VPN 또는 Cloud IAP 사용 권장
✅ 불필요한 경우 RDP 비활성화
```

#### 정답: D. GCP Console에서 Windows 사용자명과 비밀번호를 설정하고 포트 3389에 대한 방화벽 규칙이 존재하는지 확인한 후 GCP Console의 RDP 버튼을 클릭하여 자격 증명 제공

**선택 이유**:

1. 최소 단계 (3-4단계)로 연결 완료 가능
2. 별도 RDP 클라이언트 설치 불필요 (브라우저 기반)
3. 완전한 Windows 자격 증명 설정 (사용자명 + 비밀번호)
4. 올바른 RDP 포트 3389 확인
5. Google Cloud Console 통합 기능 최대 활용
6. 자동화된 연결 과정으로 오류 가능성 최소화
7. SQL Server 관리에 필요한 그래픽 환경 즉시 제공

RDP(Remote Desktop Protocol Client) 클라이언트란 Windows 컴퓨터에 원격으로 접속할 수 있게 해주는 소프트웨어이다.

:::

## dump 21

Your company has a 3-tier solution running on Compute Engine. The configuration of the current infrastructure is shown below. Each tier has a service account that is associated with all instances within it. You need to enable communication on TCP port 8080 between tiers as follows: Instances in tier #1 must communicate with tier #2. Instances in tier #2 must communicate with tier #3. What should you do?

![gcp](../.vuepress/assets/gcp/250911-1.png)

1.  Create an ingress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.2.0/24). Protocols: allow all. 2. Create an ingress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.1.0/24). Protocols: allow all.
1.  Create an ingress firewall rule with the following settings: Targets: all instances with tier #2 service account. Source filter: all instances with tier #1 service account. Protocols: allow TCP: 8080. 2. Create an ingress firewall rule with the following settings: Targets: all instances with tier #3 service account. Source filter: all instances with tier #2 service account. Protocols: allow TCP: 8080.
1.  Create an ingress firewall rule with the following settings: Targets: all instances with tier #2 service account. Source filter: all instances with tier #1 service account. Protocols: allow all. 2. Create an ingress firewall rule with the following settings: Targets: all instances with tier #3 service account. Source filter: all instances with tier #2 service account. Protocols: allow all.
1.  Create an egress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.2.0/24). Protocols: allow TCP: 8080. 2. Create an egress firewall rule with the following settings: Targets: all instances. Source filter: IP ranges (with the range set to 10.0.1.0/24). Protocols: allow TCP: 8080.

:::details 풀이

### 영문 문제 해석

**문제**: "회사에 Compute Engine에서 실행되는 3-tier 솔루션이 있습니다. 현재 인프라의 구성은 아래와 같습니다. 각 tier에는 해당 tier 내의 모든 인스턴스와 연관된 서비스 계정이 있습니다. 다음과 같이 tier 간에 TCP 포트 8080에서 통신을 활성화해야 합니다: Tier #1의 인스턴스는 tier #2와 통신해야 합니다. Tier #2의 인스턴스는 tier #3과 통신해야 합니다. 어떻게 해야 할까요?"

**네트워크 구성**:

-   VPC 내 3개 서브넷
-   Subnet Tier #1: 10.0.1.0/24
-   Subnet Tier #2: 10.0.2.0/24
-   Subnet Tier #3: 10.0.3.0/24
-   각 tier별 전용 서비스 계정

**통신 요구사항**:

-   Tier #1 → Tier #2 (TCP 8080)
-   Tier #2 → Tier #3 (TCP 8080)

### 선택지 해석

**선택지 1**:

1. 모든 인스턴스를 대상으로 하고 소스를 10.0.2.0/24로 하는 ingress 규칙 (모든 프로토콜 허용)
2. 모든 인스턴스를 대상으로 하고 소스를 10.0.1.0/24로 하는 ingress 규칙 (모든 프로토콜 허용)

**선택지 2**:

1. Tier #2 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #1 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)
2. Tier #3 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #2 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)

**선택지 3**:

1. Tier #2 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #1 서비스 계정으로 하는 ingress 규칙 (모든 프로토콜 허용)
2. Tier #3 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #2 서비스 계정으로 하는 ingress 규칙 (모든 프로토콜 허용)

**선택지 4**:

1. 모든 인스턴스를 대상으로 하고 소스를 10.0.2.0/24로 하는 egress 규칙 (TCP 8080 허용)
2. 모든 인스턴스를 대상으로 하고 소스를 10.0.1.0/24로 하는 egress 규칙 (TCP 8080 허용)

### 문제 분석 및 정답 도출

#### 통신 흐름 분석

**요구되는 통신**

```
Tier #1 (10.0.1.0/24) → Tier #2 (10.0.2.0/24) : TCP 8080
Tier #2 (10.0.2.0/24) → Tier #3 (10.0.3.0/24) : TCP 8080
```

**방화벽 규칙 관점**

```
Tier #2가 Tier #1으로부터 ingress 트래픽 수신
Tier #3이 Tier #2로부터 ingress 트래픽 수신
```

#### Google Cloud 방화벽 규칙 이해

**Ingress vs Egress**

-   **Ingress**: 외부→내부로 들어오는 트래픽
-   **Egress**: 내부→외부로 나가는 트래픽

**서비스 계정 기반 타겟팅**

-   서비스 계정으로 인스턴스 그룹 식별 가능
-   IP 범위보다 더 세밀하고 안전한 제어
-   인스턴스 추가/제거 시 자동으로 적용

#### 각 선택지 상세 분석

**선택지 1 분석**

```
문제점:
❌ 너무 광범위한 타겟 (모든 인스턴스)
❌ 모든 프로토콜 허용 (과도한 권한)
❌ 잘못된 IP 범위 매핑
❌ 보안 원칙 위배 (최소 권한 원칙)

규칙 1: 모든 인스턴스가 10.0.2.0/24에서 모든 트래픽 수신
규칙 2: 모든 인스턴스가 10.0.1.0/24에서 모든 트래픽 수신
→ 의도한 방향과 다름
```

**선택지 2 분석** ✅

```
장점:
✅ 정확한 서비스 계정 기반 타겟팅
✅ 특정 포트만 허용 (TCP 8080)
✅ 최소 권한 원칙 준수
✅ 통신 방향 정확

규칙 1: Tier #2 ← Tier #1 (TCP 8080)
규칙 2: Tier #3 ← Tier #2 (TCP 8080)
→ 요구사항과 정확히 일치
```

**선택지 3 분석**

```
문제점:
✅ 올바른 서비스 계정 타겟팅
❌ 모든 프로토콜 허용 (과도한 권한)
❌ 보안 위험 증가
❌ 요구사항은 TCP 8080만 필요

규칙은 올바르지만 불필요한 프로토콜까지 허용
```

**선택지 4 분석**

```
문제점:
❌ Egress 규칙 사용 (잘못된 방향)
❌ Source filter를 IP 범위로 설정 (egress에서는 destination)
❌ 잘못된 개념 적용
❌ egress 규칙의 source filter는 의미 없음
```

#### 올바른 방화벽 규칙 설계

**규칙 1: Tier #1 → Tier #2 통신 허용**

```bash
gcloud compute firewall-rules create tier1-to-tier2 \
  --direction=INGRESS \
  --action=ALLOW \
  --target-service-accounts=tier2-service-account@project.iam.gserviceaccount.com \
  --source-service-accounts=tier1-service-account@project.iam.gserviceaccount.com \
  --rules=tcp:8080
```

**규칙 2: Tier #2 → Tier #3 통신 허용**

```bash
gcloud compute firewall-rules create tier2-to-tier3 \
  --direction=INGRESS \
  --action=ALLOW \
  --target-service-accounts=tier3-service-account@project.iam.gserviceaccount.com \
  --source-service-accounts=tier2-service-account@project.iam.gserviceaccount.com \
  --rules=tcp:8080
```

#### 서비스 계정 vs IP 범위 비교

**서비스 계정 기반 (선택지 2)**

```
장점:
✅ 동적 인스턴스 관리 (인스턴스 추가/제거 자동 적용)
✅ 세밀한 접근 제어
✅ IP 변경에 영향받지 않음
✅ 보안 모범 사례

예시: 인스턴스가 다른 서브넷으로 이동해도 서비스 계정은 유지
```

**IP 범위 기반 (선택지 1)**

```
단점:
❌ 정적 구성 (IP 변경 시 규칙 수정 필요)
❌ 서브넷 전체에 적용 (불필요한 권한)
❌ 관리 복잡성 증가
❌ 스케일링 시 유연성 부족
```

#### 보안 원칙 적용

**최소 권한 원칙**

```
✅ 필요한 포트만 허용 (TCP 8080)
✅ 특정 서비스 계정 간에만 허용
✅ 명시적 허용 정책
❌ 모든 프로토콜 허용 금지
```

**심층 방어**

```
네트워크 레벨: 방화벽 규칙
애플리케이션 레벨: 인증/권한 부여
인스턴스 레벨: OS 방화벽, 서비스 구성
```

#### 실제 적용 시나리오

**3-Tier 애플리케이션 예시**

```
Tier #1: Web Server (Apache/Nginx)
├── 사용자 요청 수신
├── Tier #2로 API 호출 (TCP 8080)
└── 결과를 사용자에게 반환

Tier #2: Application Server (Tomcat/Node.js)
├── Tier #1에서 요청 수신 (TCP 8080)
├── 비즈니스 로직 처리
├── Tier #3으로 데이터 요청 (TCP 8080)
└── 결과를 Tier #1으로 반환

Tier #3: Database Server (MySQL/PostgreSQL)
├── Tier #2에서 요청 수신 (TCP 8080)
├── 데이터 처리 및 응답
└── 결과를 Tier #2로 반환
```

#### 트러블슈팅 관점

**방화벽 규칙 테스트**

```bash
# 연결성 테스트
gcloud compute instances list --filter="service-account:tier1-sa"
gcloud compute instances list --filter="service-account:tier2-sa"

# 포트 연결 테스트
telnet `<tier2-internal-ip>` 8080
nc -zv `<tier3-internal-ip>` 8080
```

**로그 모니터링**

```bash
# 방화벽 로그 확인
gcloud logging read "resource.type=gce_subnetwork AND jsonPayload.rule_details.action=ALLOW"
```

#### 정답: 선택지 2

**규칙 1**: Tier #2 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #1 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)
**규칙 2**: Tier #3 서비스 계정 인스턴스를 대상으로 하고 소스를 Tier #2 서비스 계정으로 하는 ingress 규칙 (TCP 8080 허용)

**선택 이유**:

1. 정확한 통신 방향과 일치 (ingress 규칙)
2. 서비스 계정 기반의 세밀한 접근 제어
3. 최소 권한 원칙 준수 (TCP 8080만 허용)
4. 확장성과 유지보수성 우수
5. 보안 모범 사례 적용
6. 동적 인스턴스 관리 지원
7. 요구사항과 정확히 일치하는 구성

ingress는 트래픽을 받는 규칙, egress는 트래픽을 내보내는 규칙이다.

:::

## dump 22

You significantly changed a complex Deployment Manager template and want to confirm that the dependencies of all defined resources are properly met before committing it to the project. You want the most rapid feedback on your changes. What should you do?

A. Use granular logging statements within a Deployment Manager template authored in Python.
B. Monitor activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console.
C. Execute the Deployment Manager template against a separate project with the same configuration, and monitor for failures.
D. Execute the Deployment Manager template using the C-preview option in the same project, and observe the state of interdependent resources.

:::details 풀이

### 영문 문제 해석

**문제**: "복잡한 Deployment Manager 템플릿을 크게 변경했으며, 프로젝트에 커밋하기 전에 정의된 모든 리소스의 종속성이 제대로 충족되는지 확인하고 싶습니다. 변경 사항에 대한 가장 빠른 피드백을 원합니다. 어떻게 해야 할까요?"

**핵심 요구사항**:

-   Complex Deployment Manager template with significant changes (크게 변경된 복잡한 템플릿)
-   Confirm dependencies are properly met (종속성이 제대로 충족되는지 확인)
-   Before committing to project (프로젝트에 커밋하기 전)
-   Most rapid feedback (가장 빠른 피드백)

### 선택지 해석

**A. Python으로 작성된 Deployment Manager 템플릿 내에서 세분화된 로깅 구문 사용**

-   Python 템플릿에 로깅 코드 추가
-   실행 중 상세한 로그 출력
-   디버깅 목적의 로깅 추가

**B. GCP Console의 Stackdriver Logging 페이지에서 Deployment Manager 실행 활동 모니터링**

-   실제 배포 실행 후 로그 모니터링
-   Stackdriver를 통한 사후 분석
-   실시간 실행 로그 확인

**C. 동일한 구성을 가진 별도 프로젝트에 대해 Deployment Manager 템플릿을 실행하고 실패 모니터링**

-   테스트 프로젝트에서 실제 배포 실행
-   실제 리소스 생성을 통한 검증
-   실패 발생 시 분석

**D. 같은 프로젝트에서 --preview 옵션을 사용하여 Deployment Manager 템플릿을 실행하고 상호 종속적인 리소스의 상태 관찰**

-   Preview 모드로 dry run 실행
-   실제 리소스 생성 없이 검증
-   종속성 및 구성 오류 사전 확인

### 문제 분석 및 정답 도출

#### Deployment Manager Preview Mode 이해

**--preview 옵션의 특징**

```bash
gcloud deployment-manager deployments create my-deployment \
  --config=template.yaml \
  --preview

# 또는 기존 배포 업데이트 시
gcloud deployment-manager deployments update my-deployment \
  --config=new-template.yaml \
  --preview
```

**Preview Mode 동작 방식**

-   실제 리소스를 생성하지 않음
-   템플릿 구문 검증
-   종속성 그래프 생성 및 검증
-   API 호출 시뮬레이션 (실제 생성 없이)
-   리소스 간 종속성 확인

#### 각 선택지 상세 분석

**A. Python 템플릿에 로깅 추가**

```python
# 예시: 로깅 추가
def GenerateConfig(context):
    print("Generating resources...")  # 로깅 추가
    resources = []
    # 템플릿 로직
    return {'resources': resources}
```

**문제점:**

-   ❌ 실제 실행해야 로그 확인 가능
-   ❌ 종속성 검증과 직접적 관련 없음
-   ❌ 빠른 피드백 제공 어려움
-   ❌ 로깅만으로는 종속성 오류 사전 발견 불가

**B. Stackdriver Logging 모니터링**

```
장점: 상세한 실행 로그 확인 가능
단점:
❌ 실제 배포 실행 후에만 확인 가능
❌ 실패 시 리소스 정리 필요
❌ 사후 분석 방식 (사전 검증 아님)
❌ 빠른 피드백 제공 어려움
```

**C. 별도 프로젝트에서 실제 실행**

```
장점: 완전한 실제 환경 테스트
단점:
❌ 별도 프로젝트 설정 필요 (시간 소요)
❌ 실제 리소스 생성으로 인한 비용 발생
❌ 실행 시간이 오래 걸림
❌ 실패 시 리소스 정리 복잡
❌ "가장 빠른 피드백"과 상반됨
```

**D. Preview 옵션 사용** ✅

```
장점:
✅ 실제 리소스 생성 없이 검증
✅ 종속성 그래프 사전 확인
✅ 템플릿 구문 오류 즉시 발견
✅ API 호출 시뮬레이션
✅ 몇 초 내 빠른 피드백
✅ 비용 발생 없음
✅ 안전한 검증 방법
```

#### Preview Mode의 구체적인 검증 내용

**템플릿 구문 검증**

```yaml
# 잘못된 구문 예시
resources:
- name: vm-instance
  type: compute.v1.instance
  properties:
    zone: us-central1-a
    machineType: invalid-machine-type  # 오류

# Preview 실행 시 즉시 오류 발견
ERROR: Invalid machine type: invalid-machine-type
```

**종속성 검증**

```yaml
# 종속성 오류 예시
resources:
- name: vm-instance
  type: compute.v1.instance
  properties:
    networkInterfaces:
    - network: $(ref.my-network.selfLink)  # my-network가 정의되지 않음

# Preview 실행 시 종속성 오류 발견
ERROR: Reference to undefined resource: my-network
```

**리소스 간 관계 확인**

```yaml
# 올바른 종속성 예시
resources:
    - name: my-network
      type: compute.v1.network

    - name: my-subnet
      type: compute.v1.subnetwork
      properties:
          network: $(ref.my-network.selfLink) # 올바른 참조

    - name: vm-instance
      type: compute.v1.instance
      properties:
          networkInterfaces:
              - subnetwork: $(ref.my-subnet.selfLink) # 올바른 참조
```

#### Preview 실행 결과 예시

**성공적인 Preview**

```bash
$ gcloud deployment-manager deployments create test-deployment \
  --config=template.yaml --preview

Create operation operation-1234567890 completed successfully.
NAME                TYPE                     STATE
my-network          compute.v1.network       IN_PREVIEW
my-subnet           compute.v1.subnetwork    IN_PREVIEW
vm-instance         compute.v1.instance      IN_PREVIEW

Dependencies validated successfully.
```

**종속성 오류가 있는 Preview**

```bash
$ gcloud deployment-manager deployments create test-deployment \
  --config=template.yaml --preview

ERROR: Error in Operation [operation-1234567890]:
errors:
- code: INVALID_RESOURCE_REFERENCE
  message: Reference to undefined resource 'missing-network'
```

#### 실제 워크플로우에서의 활용

**개발 프로세스**

```
1. 템플릿 수정
2. Preview 실행 (--preview)
3. 오류 발견 시 템플릿 수정 후 2단계 반복
4. Preview 성공 시 실제 배포 실행
5. 프로덕션에 커밋
```

**빠른 피드백 루프**

```
템플릿 수정 → Preview (10-30초) → 오류 확인 → 수정 → 반복
vs
템플릿 수정 → 실제 배포 (5-30분) → 오류 확인 → 정리 → 수정 → 반복
```

#### Preview vs 실제 배포 비교

**Preview Mode**

```
✅ 실행 시간: 10-30초
✅ 비용: $0
✅ 위험도: 없음
✅ 종속성 검증: 가능
✅ 리소스 정리: 불필요
❌ 실제 환경 테스트: 불가능
```

**실제 배포**

```
❌ 실행 시간: 5-30분
❌ 비용: 리소스 사용료 발생
❌ 위험도: 높음 (실제 리소스 영향)
✅ 종속성 검증: 가능
❌ 리소스 정리: 필요
✅ 실제 환경 테스트: 가능
```

#### 복잡한 템플릿에서 Preview의 중요성

**복잡한 종속성 예시**

```yaml
# 20개 이상의 리소스가 상호 의존하는 경우
resources:
    - name: vpc-network
    - name: subnet-web
    - name: subnet-app
    - name: subnet-db
    - name: firewall-web
    - name: firewall-app
    - name: load-balancer
    - name: instance-template-web
    - name: instance-group-web
    - name: autoscaler-web
# ... 등등

# 이런 복잡한 템플릿일수록 Preview가 중요
```

#### 정답: D. 같은 프로젝트에서 --preview 옵션을 사용하여 Deployment Manager 템플릿을 실행하고 상호 종속적인 리소스의 상태 관찰

**선택 이유**:

1. 가장 빠른 피드백 (10-30초 내 결과 확인)
2. 실제 리소스 생성 없이 안전한 검증
3. 종속성 그래프 및 참조 무결성 완전 검증
4. 템플릿 구문 오류 즉시 발견
5. 비용 발생 없음
6. API 호출 시뮬레이션을 통한 권한 및 할당량 검증
7. 반복적인 테스트에 적합한 빠른 실행 속도
8. 프로덕션 환경에 영향 없는 안전한 테스트 방법

:::
